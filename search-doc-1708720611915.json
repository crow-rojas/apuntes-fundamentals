{"searchDocs":[{"title":"TermodinÃ¡mica","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/Ciencias/termodinamica","content":"TermodinÃ¡mica","keywords":"","version":"Next"},{"title":"ComputaciÃ³n","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/IngenierÃ­a/computacion","content":"ComputaciÃ³n","keywords":"","version":"Next"},{"title":"EconomÃ­a","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/IngenierÃ­a/economia","content":"EconomÃ­a","keywords":"","version":"Next"},{"title":"Electricidad y Magnetismo","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/Ciencias/electromagnetismo","content":"Electricidad y Magnetismo","keywords":"","version":"Next"},{"title":"Ã‰tica","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/IngenierÃ­a/etica","content":"Ã‰tica","keywords":"","version":"Next"},{"title":"DinÃ¡mica","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/Ciencias/dinamica","content":"DinÃ¡mica","keywords":"","version":"Next"},{"title":"QuÃ­mica","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/Ciencias/quimica","content":"QuÃ­mica","keywords":"","version":"Next"},{"title":"Ecuaciones diferenciales","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/ecuaciones_diferenciales","content":"Ecuaciones diferenciales","keywords":"","version":"Next"},{"title":"IntroducciÃ³n","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/intro","content":"","keywords":"","version":"Next"},{"title":"Estructura del Examenâ€‹","type":1,"pageTitle":"IntroducciÃ³n","url":"/apuntes-fundamentals/docs/intro#estructura-del-examen","content":" El examen consta de 110 preguntas, distribuidas entre 3 pruebas. La distribuciÃ³n de preguntas es la siguiente:  TÃ³pico / Curso\tTÃ³pico / Curso\tNÂ° PreguntasMatemÃ¡tica\tMAT1610 CÃ¡lculo I\t2 MatemÃ¡tica\tMAT1620 CÃ¡lculo II\t4 MatemÃ¡tica\tMAT1630 CÃ¡lculo III\t4 MatemÃ¡tica\tMAT1640 Ecuaciones Diferenciales\t4 MatemÃ¡tica\tMAT1203 Ãlgebra Lineal\t4 Probabilidades y EstadÃ­stica\tProbabilidades y EstadÃ­stica\t12 QuÃ­mica\tQuÃ­mica\t12 ComputaciÃ³n\tComputaciÃ³n\t12 Ã‰tica\tÃ‰tica\t8 EconomÃ­a\tEconomÃ­a\t12 DinÃ¡mica\tDinÃ¡mica\t12 Electricidad y Magnetismo\tElectricidad y Magnetismo\t12 TermodinÃ¡mica\tTermodinÃ¡mica\t12 Total\t110  El examen se divide en 3 mÃ³dulos: MatemÃ¡ticas (M1), Ciencias (M2) e IngenierÃ­a (M3). Usualmente, se rinden los mÃ³dulos M1 y M3 el primer dÃ­a, y el mÃ³dulo M2 el segundo dÃ­a.  ","version":"Next","tagName":"h2"},{"title":"Contenidos y detallesâ€‹","type":1,"pageTitle":"IntroducciÃ³n","url":"/apuntes-fundamentals/docs/intro#contenidos-y-detalles","content":" info Estas tablas fueron copiadas directamente de SIDING. Eventualmente serÃ¡n escritas de una manera mÃ¡s elegante y legible. ğŸ—¿  A continuaciÃ³n, se detallan los contenidos especÃ­ficos de cada mÃ³dulo.  ","version":"Next","tagName":"h2"},{"title":"MatemÃ¡ticas (M1)â€‹","type":1,"pageTitle":"IntroducciÃ³n","url":"/apuntes-fundamentals/docs/intro#matemÃ¡ticas-m1","content":" Esta prueba tiene 30 preguntas y tiene una duraciÃ³n de 2 horas. Los contenidos son:  TÃ³pico\tCurso\tContenidos\tIndicadores a evaluar (NÃºmeros corresponden al correlativo del programa de cada curso)MatemÃ¡ticas\tMAT1610 CÃ¡lculo I 1. GeometrÃ­a AnalÃ­tica\t1. Identificar grÃ¡ficos de funciones bÃ¡sicas, exponenciales, logarÃ­tmicas. MatemÃ¡ticas\tMAT1610 CÃ¡lculo I 1. GeometrÃ­a AnalÃ­tica\t4. Calcular derivadas de funciones obtenidas por Ã¡lgebra de funciones elementales. MatemÃ¡ticas\tMAT1610 CÃ¡lculo I 1. GeometrÃ­a AnalÃ­tica\t6. Reconocer grÃ¡fica y analÃ­ticamente propiedades de los grÃ¡ficos de funciones, como crecimiento, concavidad, mÃ¡ximos y mÃ­nimos locales, asÃ­ntotas. MatemÃ¡ticas\tMAT1610 CÃ¡lculo I 1. GeometrÃ­a AnalÃ­tica\t9. Conocer el cÃ¡lculo de primitivas de funciones bÃ¡sicas. MatemÃ¡ticas\tMAT1620 CÃ¡lculo II 1. CÃ¡lculo Integral\t3. Aplicar el concepto de integral definida para calcular Ã¡reas y momentos de regiones del plano. MatemÃ¡ticas\tMAT1620 CÃ¡lculo II 1. CÃ¡lculo Integral\t5. Aplicar los criterios bÃ¡sicos de convergencia de series e integrales impropias. MatemÃ¡ticas\tMAT1620 CÃ¡lculo II 1. CÃ¡lculo Integral\t8. Conocer las ecuaciones paramÃ©tricas, vectoriales y cartesianas de restas y planos en el espacio. MatemÃ¡ticas\tMAT1630 CÃ¡lculo III 1. CÃ¡lculo Diferencial\t2. Aplicar el concepto de integral mÃºltiple para evaluar volÃºmenes y centros de masa. MatemÃ¡ticas\tMAT1630 CÃ¡lculo III 1. CÃ¡lculo Diferencial\t5. Reconocer y explicar el concepto de â€œcurvas de nivelâ€ y calcularlas. MatemÃ¡ticas\tMAT1630 CÃ¡lculo III 1. CÃ¡lculo Diferencial\t6. Calcular derivadas direccionales. MatemÃ¡ticas\tMAT1640 Ecuaciones Diferenciales 1. Ecuaciones Diferenciales\t2. Modelar situaciones sencillas de la realidad y de los fenÃ³menos fÃ­sicos bÃ¡sicos mediante ecuaciones diferenciales. MatemÃ¡ticas\tMAT1640 Ecuaciones Diferenciales 1. Ecuaciones Diferenciales\t3. Reconocer el tipo de una ecuaciÃ³n diferencial (lineal o no, grado, orden), identificar y utilizar los distintos mÃ©todos disponibles para la soluciÃ³n de acuerdo con el caso. MatemÃ¡ticas\tMAT1640 Ecuaciones Diferenciales 1. Ecuaciones Diferenciales\t6. Calcular las soluciones de ecuaciones lineales y sistemas lineales de 2x2 y 3x3 de coeficientes constantes. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t1. Determinar la escalonada reducida de una matriz y utilizarla para estudiar la dependencia lineal de vectores, resolver un sistema Ax=b, resolver la ecuaciÃ³n matricial AX=B, calcular inversas de matrices y bases de subespacios de Rn. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t2. Interpretar geomÃ©tricamente los conceptos de dependencia-independencia lineal, complemento ortogonal, sistemas de ecuaciones lineales. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t4. Explicar las propiedades de las operaciones matriciales y utilizarlas para simplificar y evaluar expresiones matriciales. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t6. Explicar y utilizar las propiedades de las matrices elementales, triangulares, simÃ©tricas, simÃ©tricas positivas definidas, unitarias, ortogonales. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t7. Explicar las propiedades de determinante y utilizarlas para calcular determinantes, resolver sistemas y evaluar inversas. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t9. Determinar la matriz que representa a una TransformaciÃ³n Lineal entre R^n y R^m, y explicar la relaciÃ³n entre cambio de base y la matriz que la representa. MatemÃ¡ticas\tMAT1203 Ãlgebra Lineal 1. Matrices 2. Raices de Ecuaciones 3. AnÃ¡lisis Vectorial\t12. Explicar las propiedades de valores, vectores propios y ecuaciÃ³n caracterÃ­stica de una matriz y aplicar esto a determinar si una matriz es diagonalizable y al cÃ¡lculo de funciones elementales de matrices. Explicar y utilizar la estructura de valores y vectores propios de matrices simÃ©tricas y sus aplicaciones. Probabilidades y EstadÃ­stica\tEYP1113 Probabilidades y EstadÃ­stica 1. Algebra de eventos, axiomas de probabilidad, cÃ¡lculo clÃ¡sico de probabilidad (conteo), probabilidad condicional, independencia, teorema de probabilidades totales y teorema de bayes. 2. Medidas descriptivas teÃ³ricas de una variable aleatoria: media, moda, mediana, percentil, varianza, rango, IQR, coeficiente de variaciÃ³n, coeficiente de asimetrÃ­a y kurtosis. 3. Modelos usuales univariados (discretos y continuo): Binomial, GeomÃ©trica, Binomial Negativa, Poisson, HipegeomÃ©trica, Uniforme, Normal, Log-Normal, Exponencial, Gamma, Beta, Weibull, Logistica, Log-Logistica, t-Student, chi-cuadrado y Fisher. Uso de R. 4. Distribuciones conjuntas, marginales y condicionales. Covarianza, correlaciÃ³n, esperanza condicional y mejor predictor. 5. EstimaciÃ³n y propiedades. MÃ©todos de estimaciÃ³n. Uso de R. 6. Test de hipÃ³tesis e intervalo de confianza. Uso de R. 7. Test de hipÃ³tesis de bondad de ajuste (chi-cuadrado y ks). Uso de R. 8. RegresiÃ³n lineal, coeficiente de determinaciÃ³n, test-t y test-F. Uso de R.\t1. Ajustar distribuciones de probabilidades para datos que se generen en un fenÃ³meno de incertidumbre. Probabilidades y EstadÃ­stica\tEYP1113 Probabilidades y EstadÃ­stica 1. Algebra de eventos, axiomas de probabilidad, cÃ¡lculo clÃ¡sico de probabilidad (conteo), probabilidad condicional, independencia, teorema de probabilidades totales y teorema de bayes. 2. Medidas descriptivas teÃ³ricas de una variable aleatoria: media, moda, mediana, percentil, varianza, rango, IQR, coeficiente de variaciÃ³n, coeficiente de asimetrÃ­a y kurtosis. 3. Modelos usuales univariados (discretos y continuo): Binomial, GeomÃ©trica, Binomial Negativa, Poisson, HipegeomÃ©trica, Uniforme, Normal, Log-Normal, Exponencial, Gamma, Beta, Weibull, Logistica, Log-Logistica, t-Student, chi-cuadrado y Fisher. Uso de R. 4. Distribuciones conjuntas, marginales y condicionales. Covarianza, correlaciÃ³n, esperanza condicional y mejor predictor. 5. EstimaciÃ³n y propiedades. MÃ©todos de estimaciÃ³n. Uso de R. 6. Test de hipÃ³tesis e intervalo de confianza. Uso de R. 7. Test de hipÃ³tesis de bondad de ajuste (chi-cuadrado y ks). Uso de R. 8. RegresiÃ³n lineal, coeficiente de determinaciÃ³n, test-t y test-F. Uso de R.\t2. Describir fenÃ³menos de incertidumbre sobre la base de variables aleatorias y hacer cÃ¡lculos asociados a ese fenÃ³meno en base a esas variables aleatorias. Probabilidades y EstadÃ­stica\tEYP1113 Probabilidades y EstadÃ­stica 1. Algebra de eventos, axiomas de probabilidad, cÃ¡lculo clÃ¡sico de probabilidad (conteo), probabilidad condicional, independencia, teorema de probabilidades totales y teorema de bayes. 2. Medidas descriptivas teÃ³ricas de una variable aleatoria: media, moda, mediana, percentil, varianza, rango, IQR, coeficiente de variaciÃ³n, coeficiente de asimetrÃ­a y kurtosis. 3. Modelos usuales univariados (discretos y continuo): Binomial, GeomÃ©trica, Binomial Negativa, Poisson, HipegeomÃ©trica, Uniforme, Normal, Log-Normal, Exponencial, Gamma, Beta, Weibull, Logistica, Log-Logistica, t-Student, chi-cuadrado y Fisher. Uso de R. 4. Distribuciones conjuntas, marginales y condicionales. Covarianza, correlaciÃ³n, esperanza condicional y mejor predictor. 5. EstimaciÃ³n y propiedades. MÃ©todos de estimaciÃ³n. Uso de R. 6. Test de hipÃ³tesis e intervalo de confianza. Uso de R. 7. Test de hipÃ³tesis de bondad de ajuste (chi-cuadrado y ks). Uso de R. 8. RegresiÃ³n lineal, coeficiente de determinaciÃ³n, test-t y test-F. Uso de R.\t3. Realizar estimaciones de parÃ¡metros de una distribuciÃ³n en base a datos del fenÃ³meno aleatorio y construir intervalos de confianza para esos estimadores, y entender cabalmente lo que esos intervalos de confianza representan. Probabilidades y EstadÃ­stica\tEYP1113 Probabilidades y EstadÃ­stica 1. Algebra de eventos, axiomas de probabilidad, cÃ¡lculo clÃ¡sico de probabilidad (conteo), probabilidad condicional, independencia, teorema de probabilidades totales y teorema de bayes. 2. Medidas descriptivas teÃ³ricas de una variable aleatoria: media, moda, mediana, percentil, varianza, rango, IQR, coeficiente de variaciÃ³n, coeficiente de asimetrÃ­a y kurtosis. 3. Modelos usuales univariados (discretos y continuo): Binomial, GeomÃ©trica, Binomial Negativa, Poisson, HipegeomÃ©trica, Uniforme, Normal, Log-Normal, Exponencial, Gamma, Beta, Weibull, Logistica, Log-Logistica, t-Student, chi-cuadrado y Fisher. Uso de R. 4. Distribuciones conjuntas, marginales y condicionales. Covarianza, correlaciÃ³n, esperanza condicional y mejor predictor. 5. EstimaciÃ³n y propiedades. MÃ©todos de estimaciÃ³n. Uso de R. 6. Test de hipÃ³tesis e intervalo de confianza. Uso de R. 7. Test de hipÃ³tesis de bondad de ajuste (chi-cuadrado y ks). Uso de R. 8. RegresiÃ³n lineal, coeficiente de determinaciÃ³n, test-t y test-F. Uso de R.\t4. Ajustar modelos de regresiÃ³n lineal a fenÃ³menos de incertidumbre, y lograr una adecuada comprensiÃ³n del rango de validez de esos modelos y de los parÃ¡metros de los mismos.   ","version":"Next","tagName":"h3"},{"title":"Ciencias (M2)â€‹","type":1,"pageTitle":"IntroducciÃ³n","url":"/apuntes-fundamentals/docs/intro#ciencias-m2","content":" Esta prueba tiene 48 preguntas y tiene una duraciÃ³n de 2 horas y 50 minutos.  TÃ³pico\tCurso\tContenidos\tIndicadores a evaluar (NÃºmeros corresponden al correlativo del programa de cada curso)DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t2. Establecer las ecuaciones del movimiento y equilibrio de sistemas utilizando la cinemÃ¡tica, la leyes constitutivas, y las condiciones de equilibrio. DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t3. Resolver problemas de equilibrio estÃ¡tico y dinÃ¡mico de sistemas. DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t4. Plantear el equilibrio de sistemas utilizando los principios de energÃ­a y trabajo virtual. DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t5. Manejar el concepto de restricciones cinemÃ¡ticas y fuerzas de vÃ­nculo. DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t6. Transformar fuerzas y desplazamientos entre distintos sistemas coordenados. DinÃ¡mica*\tFIS1514 DinÃ¡mica 1. EstÃ¡tica: 1. Resultantes de sistemas de fuerzas 2. Sistemas de fuerzas concurrentes 3. Equilibrio de cuerpos rÃ­gidos 4.Marcos 5. Centroide del Ã¡rea 6. Momentos del area de inercia 7. FricciÃ³n 2. DinÃ¡mica 1. Movimiento lineal (por ejemplo, fuerza, masa, aceleraciÃ³n, momento) 2. Movimiento angular (por ejemplo, el par, la inercia, la aceleraciÃ³n, el momento) 3. Momentos de inercia 4. Principio de Impulso y cantidad de movimiento aplicados a partÃ­culas y cuerpos rÃ­gidos 5. Trabajo, energÃ­a, potencia y como se aplica a partÃ­culas y cuerpos rÃ­gidos\t7. Conocer planteamientos algorÃ­tmicos y numÃ©ricos para resolver eficientemente problemas de la mecÃ¡nica clÃ¡sica. Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t1. Describir el fenÃ³meno del campo elÃ©ctrico, la conceptualizaciÃ³n de carga elÃ©ctrica asÃ­ como la corriente elÃ©ctrica (Ley de Gauss). Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t2. Identificar los campos vectoriales creados a travÃ©s de arreglos discretos y continuos de cargas elÃ©ctricas. Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t4. Calcular el potencial electroestÃ¡tico de un sistema y explicar su relaciÃ³n con dispositivos reales como el capacitor. Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t5. Explicar el principio de inducciÃ³n magnÃ©tica y su relaciÃ³n con dispositivos reales como la inductancia. Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t6. Describir un circuito de corriente continua mediante las ecuaciones que lo gobiernan y de calcular la corriente y el voltaje en cada uno de sus nodos. Electricidad y Magnetismo\tFIS1533 Electricidad y Magnetismo 1. Carga 2. Corriente 3. EnergÃ­a 4. Voltaje y poder 5. Voltaje y trabajo 6. Fuerza entre cargas 7. Leyes de voltaje y corriente (Kirchhoff, Ohm) 8. Circuitos Equivalentes (series y paralelo) 9. Capacitancia e inductancia 10. Circuitos de corriente alterna 11. Reactancia e impedancia 12. Algebra compleja bÃ¡sica\t7. Describir un circuito de corriente alterna mediante las ecuaciones que lo gobiernan y de predecir su comportamiento inicial y estacionario. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t1.2 Manejar y aplicar la conversiÃ³n de unidades (Sistema Internacional y sus prefijos). QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t3.2 Discutir cÃ³mo los cambios de temperatura afectan el estado de una sustancia (sÃ³lido, lÃ­quido y gas). QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t3.3 Describir los diferentes tipos de enlaces presentes en materiales sÃ³lidos y sus estructuras cristalinas. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t6.1 Describir el significado de equilibrio dinÃ¡mico y diferenciar equilibrios homogÃ©neos y heterogÃ©neos. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t6.2 Entender y relacionar cuociente de reacciÃ³n (Q), concentraciÃ³n de especies, y constante de equilibrio (K). QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t6.3 Explicar el significado de la constante de equilibrio, y su cÃ¡lculo a partir de las concentraciones en el equilibrio. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t7.1 Diferenciar conceptos de electrolitos fuertes y dÃ©biles. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t7.3 Identificar las relaciones entre la concentraciÃ³n de iones y el pH. Discutir las relaciones entre Ka y el grado de ionizaciÃ³n de Ã¡cido. Describir el comportamiento de Ã¡cidos y bases fuertes y dÃ©biles en disoluciÃ³n. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t7.4 Calcular el pH de soluciones de Ã¡cidos, bases y sistemas buffer. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t9.1 Formular semi-reacciones balanceadas en masa y carga. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t9.2 Describir los componentes de una celda electroquÃ­mica. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t9.3 Describir el electrodo estÃ¡ndar de hidrÃ³geno. QuÃ­mica**\tQIM100E QuÃ­mica para IngenierÃ­a\t1. Nomenclatura 2. OxidaciÃ³n-ReducciÃ³n 3. Tabla periÃ³dica 4. Estados de la materia 5. Ãcidos y Bases 6. Ecuaciones (estequiometrÃ­a) 7. Metales y No Metales 8. Equilibrio\t9.4 Identificar las relaciones entre energÃ­a de Gibbs, potencial estÃ¡ndar y la constante de equilibrio K. TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t1. Definir el concepto de temperatura y temperatura absoluta. TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t2. Explicar el equilibrio tÃ©rmico y el principio de expansiÃ³n tÃ©rmica. TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t4. Explicar la primera ley de la termodinÃ¡mica y aplicar la ley a ejemplos con gases ideales TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t5. Describir el concepto de entropÃ­a y de la direcciÃ³n de los procesos. TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t6. Calcular la entropÃ­a, potencial termodinÃ¡mico y eficiencia en distintos ciclos ideales y reales. TermodinÃ¡mica\tFIS1523 TermodinÃ¡mica 1. Leyes termodinÃ¡micas (por ejemplo, primera ley, segunda ley) 2. EnergÃ­a, calor y trabajo 3. Disponibilidad y reversibilidad 4. Ciclos 5. Gases ideales 6. Mezcla de gases 7. Fase cambios 8. Transferencia de calor 9. Propiedades de: entalpÃ­a y entropÃ­a\t7. Calcular varias cantidades termodinÃ¡micas como promedios de propiedades mecÃ¡nicas de sistemas de gran nÃºmero de partÃ­culas.   ","version":"Next","tagName":"h3"},{"title":"IngenierÃ­a (M3)â€‹","type":1,"pageTitle":"IntroducciÃ³n","url":"/apuntes-fundamentals/docs/intro#ingenierÃ­a-m3","content":" Esta prueba tiene 32 preguntas y tiene una duraciÃ³n de 1 hora y 55 minutos.  TÃ³pico\tCurso\tContenidos\tIndicadores a evaluar (NÃºmeros corresponden al correlativo del programa de cada curso)EconomÃ­a\tICS1513 IntroducciÃ³n a la EconomÃ­a\tA. Flujo de caja (por ejemplo, la equivalencia, tasa de retorno) B. Costo (por ejemplo, incremental, promedio, hundido, estimaciÃ³n) C. AnÃ¡lisis (por ejemplo, el punto de equilibrio, de costo-beneficio) D. La incertidumbre (por ejemplo, valor esperado y el riesgo)\t1. Entender y aplicar los conceptos bÃ¡sicos del anÃ¡lisis econÃ³mico y entender los problema centrales que estudia la economÃ­a, tanto a nivel micro como macro. EconomÃ­a\tICS1513 IntroducciÃ³n a la EconomÃ­a\tA. Flujo de caja (por ejemplo, la equivalencia, tasa de retorno) B. Costo (por ejemplo, incremental, promedio, hundido, estimaciÃ³n) C. AnÃ¡lisis (por ejemplo, el punto de equilibrio, de costo-beneficio) D. La incertidumbre (por ejemplo, valor esperado y el riesgo)\t2. Analizar los elementos fundamentales que explican el comportamiento de los agentes, el rol de los mercados y las leyes de oferta y demanda EconomÃ­a\tICS1513 IntroducciÃ³n a la EconomÃ­a\tA. Flujo de caja (por ejemplo, la equivalencia, tasa de retorno) B. Costo (por ejemplo, incremental, promedio, hundido, estimaciÃ³n) C. AnÃ¡lisis (por ejemplo, el punto de equilibrio, de costo-beneficio) D. La incertidumbre (por ejemplo, valor esperado y el riesgo)\t4. Entender y aplicar los conceptos bÃ¡sicos de flujo de caja, tasa de descuento, valor presente y tasa interna de retorno, asociados a un proyecto y ser capaz de calcularlos y aplicarlos en un proyecto. ComputaciÃ³n\tIIC1103 IntroducciÃ³n a la ProgramaciÃ³n 1. TerminologÃ­a (tipos de memoria, CPU, velocidades de transmisiÃ³n, internet) 3. ProgramaciÃ³n\t1. Comprender conceptos bÃ¡sicos relativos a un programa computacional, tales como algoritmos, variables, expresiones, control de flujo, funciones, listas, strings, clases y objetos. ComputaciÃ³n\tIIC1103 IntroducciÃ³n a la ProgramaciÃ³n 1. TerminologÃ­a (tipos de memoria, CPU, velocidades de transmisiÃ³n, internet) 3. ProgramaciÃ³n\t2. Aplicar tÃ©cnicas fundamentales para la resoluciÃ³n de diversos problemas con ayuda del computador. ComputaciÃ³n\tIIC1103 IntroducciÃ³n a la ProgramaciÃ³n 1. TerminologÃ­a (tipos de memoria, CPU, velocidades de transmisiÃ³n, internet) 3. ProgramaciÃ³n\t3. Aplicar el razonamiento algorÃ­tmico para generar la soluciÃ³n a un problema como una secuencia de pasos bien definidos, incluyendo pasos condicionales, repeticiÃ³n de pasos, llamadas a funciones, y recursiÃ³n. ComputaciÃ³n\tIIC1103 IntroducciÃ³n a la ProgramaciÃ³n 2. Hojas de CÃ¡lculo: Manejo Nivel BÃ¡sico Transversal a la formaciÃ³n. Ã‰tica\tFIL188 Ã‰tica para Ingenieros\t1. CÃ³digo de Ã©tica (sociedades profesionales y tÃ©cnicas) 2. Acuerdos y contratos 3. LegislaciÃ³n y Ã©tica 4. Responsabilidad Profesional 5. Cuestiones de protecciÃ³n pÃºblica\t1. Conocer los fundamentos y las principales corrientes Ã©ticas que subyacen a las decisiones morales del hombre. Ã‰tica\tFIL188 Ã‰tica para Ingenieros\t1. CÃ³digo de Ã©tica (sociedades profesionales y tÃ©cnicas) 2. Acuerdos y contratos 3. LegislaciÃ³n y Ã©tica 4. Responsabilidad Profesional 5. Cuestiones de protecciÃ³n pÃºblica\t2. Reflexionar sobre los criterios y principios para orientar la acciÃ³n en el Ã¡mbito cientÃ­fico y tecnolÃ³gico. Ã‰tica\tFIL188 Ã‰tica para Ingenieros\t1. CÃ³digo de Ã©tica (sociedades profesionales y tÃ©cnicas) 2. Acuerdos y contratos 3. LegislaciÃ³n y Ã©tica 4. Responsabilidad Profesional 5. Cuestiones de protecciÃ³n pÃºblica\t3. Aplicar los principios de la Ã©tica a problemas especÃ­ficos de la ciencia y la tecnologÃ­a. Ã‰tica\tFIL188 Ã‰tica para Ingenieros\t1. CÃ³digo de Ã©tica (sociedades profesionales y tÃ©cnicas) 2. Acuerdos y contratos 3. LegislaciÃ³n y Ã©tica 4. Responsabilidad Profesional 5. Cuestiones de protecciÃ³n pÃºblica\t4. Identificar los alcances de la legislaciÃ³n nacional (cÃ³digo de Ã©tica) vigente relacionada con la prÃ¡ctica de la ingenierÃ­a y saber cÃ³mo aplicarla en el ejercicio de la profesiÃ³n. ","version":"Next","tagName":"h3"},{"title":"CÃ¡lculo III","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III","content":"","keywords":"","version":"Next"},{"title":"Integrales mÃºltiplesâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#integrales-mÃºltiples","content":" La integral doble de fff sobre el rectÃ¡ngulo RRR es  âˆ¬Rf(x,y)dA=limâ¡m,nâ†’âˆâˆ‘i=1mâˆ‘j=1nf(xi,yj)Î”A\\iint_R f(x, y) d A=\\lim _{m, n \\rightarrow \\infty} \\sum_{i=1}^m \\sum_{j=1}^n f\\left(x_i, y_j\\right) \\Delta Aâˆ¬Râ€‹f(x,y)dA=m,nâ†’âˆlimâ€‹i=1âˆ‘mâ€‹j=1âˆ‘nâ€‹f(xiâ€‹,yjâ€‹)Î”A  si el lÃ­mite existe.  Integral doble  ","version":"Next","tagName":"h2"},{"title":"Volumenâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#volumen","content":" Si f(x,y)â©¾0f(x, y) \\geqslant 0f(x,y)â©¾0, entonces el volumen VVV del sÃ³lido que estÃ¡ arriba del rectÃ¡ngulo RRR y debajo de la superficie z=f(x,y)z=f(x, y)z=f(x,y) es  V=âˆ¬Rf(x,y)dAV=\\iint_R f(x, y) d AV=âˆ¬Râ€‹f(x,y)dA    ","version":"Next","tagName":"h3"},{"title":"Momentos y centros de masaâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#momentos-y-centros-de-masa","content":" Los momentos de una lÃ¡mina plana que ocupa una regiÃ³n DDD y tiene densidad Î´(x,y)\\delta(x, y)Î´(x,y) es  Mx=âˆ¬Dyâ€‰Î´(x,y)dAMy=âˆ¬Dxâ€‰Î´(x,y)dAM_x=\\iint_D y \\,\\delta(x, y) d A \\qquad\\qquad M_y=\\iint_D x \\,\\delta(x, y) d AMxâ€‹=âˆ¬Dâ€‹yÎ´(x,y)dAMyâ€‹=âˆ¬Dâ€‹xÎ´(x,y)dA  donde MxM_xMxâ€‹ es el momento respecto al eje xxx y MyM_yMyâ€‹ es el momento respecto al eje yyy.  Con los momentos, podemos encontrar el centro de masa (xË‰,yË‰)(\\bar{x}, \\bar{y})(xË‰,yË‰â€‹) de la lÃ¡mina, que es  xË‰=Mym=1mâˆ¬DxÏ(x,y)dAyË‰=Mxm=1mâˆ¬DyÏ(x,y)dA\\bar{x}=\\frac{M_y}{m}=\\frac{1}{m} \\iint_D x \\rho(x, y) d A \\qquad\\qquad \\bar{y}=\\frac{M_x}{m}=\\frac{1}{m} \\iint_D y \\rho(x, y) d AxË‰=mMyâ€‹â€‹=m1â€‹âˆ¬Dâ€‹xÏ(x,y)dAyË‰â€‹=mMxâ€‹â€‹=m1â€‹âˆ¬Dâ€‹yÏ(x,y)dA  donde la masa mmm estÃ¡ dada por  m=âˆ¬DÏ(x,y)dAm=\\iint_D \\rho(x, y) d Am=âˆ¬Dâ€‹Ï(x,y)dA  Centro de masa  ","version":"Next","tagName":"h3"},{"title":"Curvas de nivelâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#curvas-de-nivel","content":" Las curvas de nivel de una funciÃ³n fff de dos variables son las curvas cuyas ecuaciones son f(x,y)=kf(x, y)=kf(x,y)=k, donde kkk es una constante en el rango de fff.  Curvas de nivel  ","version":"Next","tagName":"h2"},{"title":"Derivadas direccionalesâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#derivadas-direccionales","content":" ","version":"Next","tagName":"h2"},{"title":"DefiniciÃ³nâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#definiciÃ³n","content":" La derivada direccional de fff en (x0,y0)\\left(x_0, y_0\\right)(x0â€‹,y0â€‹) en la direcciÃ³n de un vector unitario u=âŸ¨a,bâŸ©\\mathbf{u}=\\langle a, b\\rangleu=âŸ¨a,bâŸ© es  Duf(x0,y0)=limâ¡hâ†’0f(x0+ha,y0+hb)âˆ’f(x0,y0)hD_{\\mathbf{u}} f\\left(x_0, y_0\\right)=\\lim _{h \\rightarrow 0} \\frac{f\\left(x_0+h a, y_0+h b\\right)-f\\left(x_0, y_0\\right)}{h}Duâ€‹f(x0â€‹,y0â€‹)=hâ†’0limâ€‹hf(x0â€‹+ha,y0â€‹+hb)âˆ’f(x0â€‹,y0â€‹)â€‹  si este lÃ­mite existe.  Derivada direccional  Si fff es una funciÃ³n derivable de xxx y de yyy, entonces fff tiene una derivada direccional en la direcciÃ³n de cualquier vector unitario u=âŸ¨a,bâŸ©\\mathbf{u}=\\langle a, b\\rangleu=âŸ¨a,bâŸ© y  Duf(x,y)=fx(x,y)a+fy(x,y)bD_{\\mathrm{u}} f(x, y)=f_x(x, y) a+f_y(x, y) bDuâ€‹f(x,y)=fxâ€‹(x,y)a+fyâ€‹(x,y)b  ","version":"Next","tagName":"h2"},{"title":"Gradienteâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#gradiente","content":" Si fff es una funciÃ³n de dos variables xxx y yyy, entonces el gradiente de fff es la funciÃ³n vectorial âˆ‡f\\nabla fâˆ‡f definida por  âˆ‡f(x,y)=âŸ¨fx(x,y),fy(x,y)âŸ©=âˆ‚fâˆ‚xi+âˆ‚fâˆ‚yj\\nabla f(x, y)=\\left\\langle f_x(x, y), f_y(x, y)\\right\\rangle=\\frac{\\partial f}{\\partial x} \\mathbf{i}+\\frac{\\partial f}{\\partial y} \\mathbf{j}âˆ‡f(x,y)=âŸ¨fxâ€‹(x,y),fyâ€‹(x,y)âŸ©=âˆ‚xâˆ‚fâ€‹i+âˆ‚yâˆ‚fâ€‹j  Con esta notaciÃ³n para el vector gradiente, podemos escribir la expresiÃ³n para la derivada direccional como  Duf(x,y)=âˆ‡f(x,y)â‹…uD_{\\mathbf{u}} f(x, y)=\\nabla f(x, y) \\cdot \\mathbf{u}Duâ€‹f(x,y)=âˆ‡f(x,y)â‹…u  ","version":"Next","tagName":"h3"},{"title":"MÃ¡ximo de la derivada direccionalâ€‹","type":1,"pageTitle":"CÃ¡lculo III","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_III#mÃ¡ximo-de-la-derivada-direccional","content":" Supongamos que fff es una funciÃ³n derivable de dos o tres variables. El valor mÃ¡ximo de la derivada direccional Duf(x)D_{\\mathbf{u}} f(\\mathbf{x})Duâ€‹f(x) es âˆ£âˆ‡f(x)âˆ£|\\nabla f(\\mathbf{x})|âˆ£âˆ‡f(x)âˆ£ y se presenta cuando u\\mathbf{u}u tiene la misma direcciÃ³n que el vector gradiente âˆ‡f(x)\\nabla f(\\mathbf{x})âˆ‡f(x). ","version":"Next","tagName":"h3"},{"title":"CÃ¡lculo II","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II","content":"","keywords":"","version":"Next"},{"title":"Integral definidaâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#integral-definida","content":" El Ã¡rea AAA de la regiÃ³n SSS que se encuentra bajo la grÃ¡fica de la funciÃ³n continua fff es el lÃ­mite de la suma de las Ã¡reas de los rectÃ¡ngulos de aproximaciÃ³n:  A=limâ¡nâ†’âˆRn=limâ¡nâ†’âˆ[f(x1)Î”x+f(x2)Î”x+â‹¯+f(xn)Î”x]A=\\lim _{n \\rightarrow \\infty} R_n=\\lim _{n \\rightarrow \\infty}\\left[f\\left(x_1\\right) \\Delta x+f\\left(x_2\\right) \\Delta x+\\cdots+f\\left(x_n\\right) \\Delta x\\right]A=nâ†’âˆlimâ€‹Rnâ€‹=nâ†’âˆlimâ€‹[f(x1â€‹)Î”x+f(x2â€‹)Î”x+â‹¯+f(xnâ€‹)Î”x]  Suma de Riemman  Si fff es una funciÃ³n continua definida para aâ©½xâ©½ba \\leqslant x \\leqslant baâ©½xâ©½b, dividimos el intervalo [a,b][a, b][a,b] en nnn subintervalos de igual ancho Î”x=(bâˆ’a)/n\\Delta x=(b-a) / nÎ”x=(bâˆ’a)/n. Sean x0(=a),x1,x2,â€¦,xn(=b)x_0(=a), x_1, x_2, \\ldots, x_n(=b)x0â€‹(=a),x1â€‹,x2â€‹,â€¦,xnâ€‹(=b) los puntos extremos de estos subintervalos y sean x1âˆ—,x2âˆ—,â€¦,xnâˆ—x_1^*, x_2^*, \\ldots, x_n^*x1âˆ—â€‹,x2âˆ—â€‹,â€¦,xnâˆ—â€‹ los puntos muestra en estos subintervalos, de modo que xiâˆ—x_i^*xiâˆ—â€‹ se encuentre en el iii-Ã©simo subintervalo [xiâˆ’1,xi]\\left[x_{i-1}, x_i\\right][xiâˆ’1â€‹,xiâ€‹]. Entonces la integral definida de fff, desde aaa hasta bbb, es  âˆ«abf(x)dx=limâ¡nâ†’âˆâˆ‘i=1nf(xiâˆ—)Î”x\\int_a^b f(x) d x=\\lim _{n \\rightarrow \\infty} \\sum_{i=1}^n f\\left(x_i^*\\right) \\Delta xâˆ«abâ€‹f(x)dx=nâ†’âˆlimâ€‹i=1âˆ‘nâ€‹f(xiâˆ—â€‹)Î”x  siempre que este lÃ­mite exista y dÃ© el mismo valor para todos las posibles elecciones de los puntos muestra. Si existe, decimos que fff es integrable sobre [a,b][a, b][a,b].  Integral definida  ","version":"Next","tagName":"h2"},{"title":"Propiedades de la integral definidaâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#propiedades-de-la-integral-definida","content":" Cuando se definiÃ³ la integral definida âˆ«abf(x)dx\\int_a^b f(x) d xâˆ«abâ€‹f(x)dx, de manera implÃ­cita se supuso que a&lt;ba&lt;ba&lt;b. Pero la definiciÃ³n como un lÃ­mite de la suma de Riemann tiene sentido aun cuando a&gt;ba&gt;ba&gt;b. Note que si invertimos aaa y bbb, entonces Î”x\\Delta xÎ”x cambia de (bâˆ’a)/n(b-a) / n(bâˆ’a)/n a (aâˆ’b)/n(a-b) / n(aâˆ’b)/n. En consecuencia,  âˆ«baf(x)dx=âˆ’âˆ«abf(x)dx\\int_b^a f(x) d x=-\\int_a^b f(x) d xâˆ«baâ€‹f(x)dx=âˆ’âˆ«abâ€‹f(x)dx  Si a=ba=ba=b, entonces Î”x=0\\Delta x=0Î”x=0 de manera que  âˆ«aaf(x)dx=0\\int_a^a f(x) d x=0âˆ«aaâ€‹f(x)dx=0  Otras propiedades:  âˆ«abcdx=c(bâˆ’a)\\displaystyle\\int_a^b c d x=c(b-a)âˆ«abâ€‹cdx=c(bâˆ’a), donde ccc es cualquier constanteâˆ«ab[f(x)+g(x)]dx=âˆ«abf(x)dx+âˆ«abg(x)dx\\displaystyle\\int_a^b[f(x)+g(x)] d x=\\int_a^b f(x) d x+\\int_a^b g(x) d xâˆ«abâ€‹[f(x)+g(x)]dx=âˆ«abâ€‹f(x)dx+âˆ«abâ€‹g(x)dxâˆ«abcf(x)dx=câˆ«abf(x)dx\\displaystyle\\int_a^b c f(x) d x=c \\int_a^b f(x) d xâˆ«abâ€‹cf(x)dx=câˆ«abâ€‹f(x)dx, donde ccc es cualquier constanteâˆ«ab[f(x)âˆ’g(x)]dx=âˆ«abf(x)dxâˆ’âˆ«abg(x)dx\\displaystyle\\int_a^b[f(x)-g(x)] d x=\\int_a^b f(x) d x-\\int_a^b g(x) d xâˆ«abâ€‹[f(x)âˆ’g(x)]dx=âˆ«abâ€‹f(x)dxâˆ’âˆ«abâ€‹g(x)dxâˆ«acf(x)dx+âˆ«cbf(x)dx=âˆ«abf(x)dx\\displaystyle\\int_a^c f(x) d x+\\int_c^b f(x) d x=\\int_a^b f(x) d xâˆ«acâ€‹f(x)dx+âˆ«cbâ€‹f(x)dx=âˆ«abâ€‹f(x)dx  ","version":"Next","tagName":"h3"},{"title":"Teorema fundamental del cÃ¡lculoâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#teorema-fundamental-del-cÃ¡lculo","content":" Suponga que fff es continua sobre [a,b][a, b][a,b].  Si g(x)=âˆ«axf(t)dt\\displaystyle g(x)=\\int_a^x f(t) d tg(x)=âˆ«axâ€‹f(t)dt, entonces gâ€²(x)=f(x)g^{\\prime}(x)=f(x)gâ€²(x)=f(x).âˆ«abf(x)dx=F(b)âˆ’F(a)\\displaystyle \\int_a^b f(x) d x=F(b)-F(a)âˆ«abâ€‹f(x)dx=F(b)âˆ’F(a), donde FFF es cualquier antiderivada de fff; es decir, Fâ€²=fF^{\\prime}=fFâ€²=f.  ","version":"Next","tagName":"h2"},{"title":"Criterios de convergenciaâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#criterios-de-convergencia","content":" Para series, la siguiente tabla resume los criterios de convergencia:  Criterio\tCuando usar\tConclusiÃ³nSerie geomÃ©trica\tâˆ‘n=0âˆarn\\displaystyle \\sum_{n=0}^{\\infty} a r^nn=0âˆ‘âˆâ€‹arn\tConverge si âˆ£râˆ£&lt;1\\lvert r \\rvert&lt;1âˆ£râˆ£&lt;1, diverge si âˆ£râˆ£â‰¥1\\lvert r \\rvert \\geq 1âˆ£râˆ£â‰¥1 Test de la divergencia\tCualquier serie âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹\tSi limâ¡nâ†’âˆanâ‰ 0\\displaystyle \\lim_{n \\to \\infty} a_n \\neq 0nâ†’âˆlimâ€‹anâ€‹î€ =0, entonces la serie diverge Serie ppp\tâˆ‘n=1âˆ1np\\displaystyle \\sum_{n=1}^{\\infty} \\frac{1}{n^p}n=1âˆ‘âˆâ€‹np1â€‹\tConverge si p&gt;1p&gt;1p&gt;1, diverge si pâ‰¤1p \\leq 1pâ‰¤1 Test de comparaciÃ³n\tâˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ y âˆ‘n=1âˆbn\\displaystyle \\sum_{n=1}^{\\infty} b_nn=1âˆ‘âˆâ€‹bnâ€‹, con 0â‰¤anâ‰¤bn0 \\leq a_n \\leq b_n0â‰¤anâ€‹â‰¤bnâ€‹\tSi âˆ‘n=1âˆbn\\displaystyle \\sum_{n=1}^{\\infty} b_nn=1âˆ‘âˆâ€‹bnâ€‹ converge, entonces âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ converge Si âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ diverge, entonces âˆ‘n=1âˆbn\\displaystyle \\sum_{n=1}^{\\infty} b_nn=1âˆ‘âˆâ€‹bnâ€‹ diverge Test de comparaciÃ³n en el lÃ­mite\tâˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ y âˆ‘n=1âˆbn\\displaystyle \\sum_{n=1}^{\\infty} b_nn=1âˆ‘âˆâ€‹bnâ€‹, con an,bn&gt;0a_n, b_n &gt; 0anâ€‹,bnâ€‹&gt;0\tSi limâ¡nâ†’âˆanbn=c\\displaystyle \\lim_{n \\to \\infty} \\frac{a_n}{b_n}=cnâ†’âˆlimâ€‹bnâ€‹anâ€‹â€‹=c, donde ccc es una constante positiva, entonces âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ y âˆ‘n=1âˆbn\\displaystyle \\sum_{n=1}^{\\infty} b_nn=1âˆ‘âˆâ€‹bnâ€‹ convergen o divergen juntas Serie alternante\tâˆ‘n=1âˆ(âˆ’1)nâˆ’1an\\displaystyle \\sum_{n=1}^{\\infty} (-1)^{n-1} a_nn=1âˆ‘âˆâ€‹(âˆ’1)nâˆ’1anâ€‹ o âˆ‘n=1âˆ(âˆ’1)nan\\displaystyle \\sum_{n=1}^{\\infty} (-1)^n a_nn=1âˆ‘âˆâ€‹(âˆ’1)nanâ€‹, con an&gt;0a_n &gt; 0anâ€‹&gt;0\tConverge si limâ¡nâ†’âˆan=0\\displaystyle \\lim_{n \\to \\infty} a_n=0nâ†’âˆlimâ€‹anâ€‹=0 y anâ‰¥an+1a_n \\geq a_{n+1}anâ€‹â‰¥an+1â€‹ para todo nnn Convergencia absoluta\tSeries âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ con valores positivos y negativos\tSi âˆ‘n=1âˆâˆ£anâˆ£\\displaystyle \\sum_{n=1}^{\\infty} \\lvert a_n \\rvertn=1âˆ‘âˆâ€‹âˆ£anâ€‹âˆ£ converge, entonces âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ converge Test de la integral\tâˆ‘n=1âˆf(n)\\displaystyle \\sum_{n=1}^{\\infty} f(n)n=1âˆ‘âˆâ€‹f(n)\tSi f(x)\\displaystyle f(x)f(x) es una funciÃ³n decreciente y positiva para xâ‰¥1x \\geq 1xâ‰¥1, entonces âˆ‘n=1âˆf(n)\\displaystyle \\sum_{n=1}^{\\infty} f(n)n=1âˆ‘âˆâ€‹f(n) converge si y solo si âˆ«1âˆf(x)dx\\displaystyle \\int_{1}^{\\infty} f(x) d xâˆ«1âˆâ€‹f(x)dx converge Test de la razÃ³n\tSeries âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ que especialmente son exponenciales o factoriales\tSi limâ¡nâ†’âˆâˆ£an+1anâˆ£=L\\displaystyle \\lim_{n \\to \\infty} \\left\\lvert \\frac{a_{n+1}}{a_n} \\right\\rvert=Lnâ†’âˆlimâ€‹â€‹anâ€‹an+1â€‹â€‹â€‹=L, entonces âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ converge si L&lt;1L&lt;1L&lt;1 y diverge si L&gt;1L&gt;1L&gt;1. Si L=1L = 1L=1, el test es inconcluso Test de la raÃ­z\tSeries âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ qie especialmente son exponenciales\tSi limâ¡nâ†’âˆâˆ£anâˆ£n=L\\displaystyle \\lim_{n \\to \\infty} \\sqrt[n]{\\lvert a_n \\rvert}=Lnâ†’âˆlimâ€‹nâˆ£anâ€‹âˆ£â€‹=L, entonces âˆ‘n=1âˆan\\displaystyle \\sum_{n=1}^{\\infty} a_nn=1âˆ‘âˆâ€‹anâ€‹ converge si L&lt;1L&lt;1L&lt;1 y diverge si L&gt;1L&gt;1L&gt;1. Si L=1L = 1L=1, el test es inconcluso  ","version":"Next","tagName":"h2"},{"title":"Ecuaciones de rectas y planos en el espacioâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#ecuaciones-de-rectas-y-planos-en-el-espacio","content":" ","version":"Next","tagName":"h2"},{"title":"Rectasâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#rectas","content":" La ecuaciÃ³n vectorial de la recta estÃ¡ dada por  râƒ—=râƒ—0+tvâƒ—\\vec{r} = \\vec{r}_0 + t \\vec{v}r=r0â€‹+tv  donde râƒ—0\\vec{r}_0r0â€‹ es un punto de la recta y vâƒ—\\vec{v}v es un vector paralelo a la recta.  La ecuaciÃ³n paramÃ©trica de la recta estÃ¡ dada por  {x=x0+aty=y0+btz=z0+ct\\begin{cases} x = x_0 + at \\\\ y = y_0 + bt \\\\ z = z_0 + ct \\end{cases}â©â¨â§â€‹x=x0â€‹+aty=y0â€‹+btz=z0â€‹+ctâ€‹  mientras que la ecuaciÃ³n simÃ©trica de la recta estÃ¡ dada por  xâˆ’x0a=yâˆ’y0b=zâˆ’z0c\\frac{x-x_0}{a} = \\frac{y-y_0}{b} = \\frac{z-z_0}{c}axâˆ’x0â€‹â€‹=byâˆ’y0â€‹â€‹=czâˆ’z0â€‹â€‹  donde (x0,y0,z0)(x_0, y_0, z_0)(x0â€‹,y0â€‹,z0â€‹) es un punto de la recta y (a,b,c)(a, b, c)(a,b,c) es un vector paralelo a la recta.  Recta en el espacio  ","version":"Next","tagName":"h3"},{"title":"Planosâ€‹","type":1,"pageTitle":"CÃ¡lculo II","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_II#planos","content":" La ecuaciÃ³n vectorial de un plano estÃ¡ dada por  nâƒ—â‹…(râƒ—âˆ’râƒ—0)=0\\vec{n} \\cdot (\\vec{r} - \\vec{r}_0) = 0nâ‹…(râˆ’r0â€‹)=0  donde râƒ—0\\vec{r}_0r0â€‹ es un punto del plano y nâƒ—\\vec{n}n es un vector normal al plano.  La ecuaciÃ³n escalar de un plano estÃ¡ dada por  a(xâˆ’x0)+b(yâˆ’y0)+c(zâˆ’z0)=0a(x-x_0) + b(y-y_0) + c(z-z_0) = 0a(xâˆ’x0â€‹)+b(yâˆ’y0â€‹)+c(zâˆ’z0â€‹)=0  donde (x0,y0,z0)(x_0, y_0, z_0)(x0â€‹,y0â€‹,z0â€‹) es un punto del plano y (a,b,c)(a, b, c)(a,b,c) es un vector normal al plano.  Si se reescriben los tÃ©rminos de la ecuaciÃ³n anterior, podemos establecer la ecuaciÃ³n lineal del plano como  ax+by+cz=dax + by + cz = dax+by+cz=d  donde d=ax0+by0+cz0d = ax_0 + by_0 + cz_0d=ax0â€‹+by0â€‹+cz0â€‹.  Plano en el espacio ","version":"Next","tagName":"h3"},{"title":"CÃ¡lculo I","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I","content":"","keywords":"","version":"Next"},{"title":"GrÃ¡ficos de funcionesâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#grÃ¡ficos-de-funciones","content":" Los grÃ¡ficos de funciones bÃ¡sicas, exponenciales y logarÃ­tmicas son:    ","version":"Next","tagName":"h2"},{"title":"Derivadasâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#derivadas","content":" La derivada de una funciÃ³n fff en un nÃºmero x=ax = ax=a, denotada por fâ€²(a)f'(a)fâ€²(a), es  fâ€²(a)=limâ¡hâ†’0f(a+h)âˆ’f(a)hf^{\\prime}(a) = \\lim _{h \\rightarrow 0} \\frac{f(a+h)-f(a)}{h}fâ€²(a)=hâ†’0limâ€‹hf(a+h)âˆ’f(a)â€‹  si este lÃ­mite existe.  A continuaciÃ³n se presentan las derivadas de las funciones mÃ¡s comunes:  FunciÃ³n\tDerivada\tDerivada con regla de la cadenaPotencia\tddx[xn]=nxnâˆ’1\\frac{d}{d x}\\left[x^n\\right]=n x^{n-1}dxdâ€‹[xn]=nxnâˆ’1\tddx[un]=nunâˆ’1â‹…uâ€²\\frac{d}{d x}\\left[u^n\\right]=n u^{n-1} \\cdot u^{\\prime}dxdâ€‹[un]=nunâˆ’1â‹…uâ€² Exponencial (base eee)\tddx[ex]=ex\\frac{d}{d x}\\left[e^x\\right]=e^xdxdâ€‹[ex]=ex\tddx[eu]=euâ‹…uâ€²\\frac{d}{d x}\\left[e^u\\right]=e^u \\cdot u^{\\prime}dxdâ€‹[eu]=euâ‹…uâ€² Exponencial (base aaa)\tddx[ax]=axlnâ¡(a)\\frac{d}{d x}\\left[a^x\\right]=a^x \\ln (a)dxdâ€‹[ax]=axln(a)\tddx[au]=aulnâ¡(a)â‹…uâ€²\\frac{d}{d x}\\left[a^u\\right]=a^u \\ln (a) \\cdot u^{\\prime}dxdâ€‹[au]=auln(a)â‹…uâ€² Logaritmo Natural\tddx[lnâ¡(x)]=1x\\frac{d}{d x}[\\ln (x)]=\\frac{1}{x}dxdâ€‹[ln(x)]=x1â€‹\tddx[lnâ¡(u)]=1uâ‹…uâ€²\\frac{d}{d x}[\\ln (u)]=\\frac{1}{u} \\cdot u^{\\prime}dxdâ€‹[ln(u)]=u1â€‹â‹…uâ€² o uâ€²u\\frac{u^{\\prime}}{u}uuâ€²â€‹ Logaritmo (base aaa)\tddx[logâ¡a(x)]=1xâ‹…lnâ¡(a)\\frac{d}{d x}\\left[\\log _a(x)\\right]=\\frac{1}{x \\cdot \\ln (a)}dxdâ€‹[logaâ€‹(x)]=xâ‹…ln(a)1â€‹\tddx[logâ¡a(u)]=1uâ‹…lnâ¡(a)â‹…uâ€²\\frac{d}{d x}\\left[\\log _a(u)\\right]=\\frac{1}{u \\cdot \\ln (a)} \\cdot u^{\\prime}dxdâ€‹[logaâ€‹(u)]=uâ‹…ln(a)1â€‹â‹…uâ€² Seno\tddx[sinâ¡(x)]=cosâ¡(x)\\frac{d}{d x}[\\sin (x)]=\\cos (x)dxdâ€‹[sin(x)]=cos(x)\tddx[sinâ¡u]=cosâ¡(u)â‹…uâ€²\\frac{d}{d x}[\\sin u]=\\cos (u) \\cdot u^{\\prime}dxdâ€‹[sinu]=cos(u)â‹…uâ€² Coseno\tddx[cosâ¡(x)]=âˆ’sinâ¡(x)\\frac{d}{d x}[\\cos (x)]=-\\sin (x)dxdâ€‹[cos(x)]=âˆ’sin(x)\tddx[cosâ¡u]=âˆ’sinâ¡(u)â‹…uâ€²\\frac{d}{d x}[\\cos u]=-\\sin (u) \\cdot u^{\\prime}dxdâ€‹[cosu]=âˆ’sin(u)â‹…uâ€² Tangente\tddx[tanâ¡(x)]=secâ¡2(x)\\frac{d}{d x}[\\tan (x)]=\\sec ^2(x)dxdâ€‹[tan(x)]=sec2(x)\tddx[tanâ¡(u)]=secâ¡2(u)â‹…uâ€²\\frac{d}{d x}[\\tan (u)]=\\sec ^2(u) \\cdot u^{\\prime}dxdâ€‹[tan(u)]=sec2(u)â‹…uâ€² Cosecante\tddx[cscâ¡(x)]=âˆ’cscâ¡(x)cotâ¡(x)\\frac{d}{d x}[\\csc (x)]=-\\csc (x) \\cot (x)dxdâ€‹[csc(x)]=âˆ’csc(x)cot(x)\tddx[cscâ¡(u)]=âˆ’cscâ¡(u)cotâ¡(u)â‹…uâ€²\\frac{d}{d x}[\\csc (u)]=-\\csc (u) \\cot (u) \\cdot u^{\\prime}dxdâ€‹[csc(u)]=âˆ’csc(u)cot(u)â‹…uâ€² Secante\tddx[secâ¡(x)]=secâ¡(x)tanâ¡(x)\\frac{d}{d x}[\\sec (x)]=\\sec (x) \\tan (x)dxdâ€‹[sec(x)]=sec(x)tan(x)\tddx[secâ¡(u)]=secâ¡(u)tanâ¡(u)â‹…uâ€²\\frac{d}{d x}[\\sec (u)]=\\sec (u) \\tan (u) \\cdot u^{\\prime}dxdâ€‹[sec(u)]=sec(u)tan(u)â‹…uâ€² Cotangente\tddx[cotâ¡(x)]=âˆ’cscâ¡2(x)\\frac{d}{d x}[\\cot (x)]=-\\csc ^2(x)dxdâ€‹[cot(x)]=âˆ’csc2(x)\tddx[cotâ¡(u)]=âˆ’cscâ¡2(u)â‹…uâ€²\\frac{d}{d x}[\\cot (u)]=-\\csc ^2(u) \\cdot u^{\\prime}dxdâ€‹[cot(u)]=âˆ’csc2(u)â‹…uâ€² Arcoseno\tddxsinâ¡âˆ’1(x)=11âˆ’x2\\frac{d}{d x} \\sin ^{-1}(x)=\\frac{1}{\\sqrt{1-x^2}}dxdâ€‹sinâˆ’1(x)=1âˆ’x2â€‹1â€‹\tddxsinâ¡âˆ’1(u)=11âˆ’u2â‹…uâ€²\\frac{d}{d x} \\sin ^{-1}(u)=\\frac{1}{\\sqrt{1-u^2}} \\cdot u^{\\prime}dxdâ€‹sinâˆ’1(u)=1âˆ’u2â€‹1â€‹â‹…uâ€² Arcocoseno\tddxcosâ¡âˆ’1(x)=âˆ’11âˆ’x2\\frac{d}{d x} \\cos ^{-1}(x)=\\frac{-1}{\\sqrt{1-x^2}}dxdâ€‹cosâˆ’1(x)=1âˆ’x2â€‹âˆ’1â€‹\tddxcosâ¡âˆ’1(u)=âˆ’11âˆ’u2â‹…uâ€²\\frac{d}{d x} \\cos ^{-1}(u)=\\frac{-1}{\\sqrt{1-u^2}} \\cdot u^{\\prime}dxdâ€‹cosâˆ’1(u)=1âˆ’u2â€‹âˆ’1â€‹â‹…uâ€² Arcotangente\tddxtanâ¡âˆ’1(x)=11+x2\\frac{d}{d x} \\tan ^{-1}(x)=\\frac{1}{1+x^2}dxdâ€‹tanâˆ’1(x)=1+x21â€‹\tddxtanâ¡âˆ’1(u)=11+u2â‹…uâ€²\\frac{d}{d x} \\tan ^{-1}(u)=\\frac{1}{1+u^2} \\cdot u^{\\prime}dxdâ€‹tanâˆ’1(u)=1+u21â€‹â‹…uâ€² Arcocosecante\tddxcscâ¡âˆ’1(x)=âˆ’1âˆ¥xâˆ¥x2âˆ’1\\frac{d}{d x} \\csc ^{-1}(x)=\\frac{-1}{\\|x\\| \\sqrt{x^2-1}}dxdâ€‹cscâˆ’1(x)=âˆ¥xâˆ¥x2âˆ’1â€‹âˆ’1â€‹\tddxcscâ¡âˆ’1(u)=âˆ’1âˆ¥uâˆ¥u2âˆ’1â‹…uâ€²\\frac{d}{d x} \\csc ^{-1}(u)=\\frac{-1}{\\|u\\| \\sqrt{u^2-1}} \\cdot u^{\\prime}dxdâ€‹cscâˆ’1(u)=âˆ¥uâˆ¥u2âˆ’1â€‹âˆ’1â€‹â‹…uâ€² Arcosecante\tddxsecâ¡âˆ’1(x)=1âˆ¥xâˆ¥x2âˆ’1\\frac{d}{d x} \\sec ^{-1}(x)=\\frac{1}{\\|x\\| \\sqrt{x^2-1}}dxdâ€‹secâˆ’1(x)=âˆ¥xâˆ¥x2âˆ’1â€‹1â€‹\tddxsecâ¡âˆ’1(u)=1âˆ¥uâˆ¥u2âˆ’1â‹…uâ€²\\frac{d}{d x} \\sec ^{-1}(u)=\\frac{1}{\\|u\\| \\sqrt{u^2-1}} \\cdot u^{\\prime}dxdâ€‹secâˆ’1(u)=âˆ¥uâˆ¥u2âˆ’1â€‹1â€‹â‹…uâ€² Arcocotangente\tddxcotâ¡âˆ’1(x)=âˆ’11+x2\\frac{d}{d x} \\cot ^{-1}(x)=\\frac{-1}{1+x^2}dxdâ€‹cotâˆ’1(x)=1+x2âˆ’1â€‹\tddxcotâ¡âˆ’1(u)=âˆ’11+u2â‹…uâ€²\\frac{d}{d x} \\cot ^{-1}(u)=\\frac{-1}{1+u^2} \\cdot u^{\\prime}dxdâ€‹cotâˆ’1(u)=1+u2âˆ’1â€‹â‹…uâ€²  ","version":"Next","tagName":"h2"},{"title":"Propiedades analÃ­ticas de los grÃ¡ficosâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#propiedades-analÃ­ticas-de-los-grÃ¡ficos","content":" ","version":"Next","tagName":"h2"},{"title":"MÃ¡ximos y mÃ­nimosâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#mÃ¡ximos-y-mÃ­nimos","content":" Sea ccc un nÃºmero en el dominio DDD de una funciÃ³n fff. Entonces f(c)f(c)f(c) es el:  valor mÃ¡ximo absoluto de fff sobre DDD si f(c)â©¾f(x)f(c) \\geqslant f(x)f(c)â©¾f(x) para toda xxx en DDD.valor mÃ­nimo absoluto de fff sobre DDD si f(c)â©½f(x)f(c) \\leqslant f(x)f(c)â©½f(x) para toda xxx en DDD.  El nÃºmero f(c)f(c)f(c) es un:  valor mÃ¡ximo local de fff si f(c)â©¾f(x)f(c) \\geqslant f(x)f(c)â©¾f(x) cuando xxx estÃ¡ cerca de ccc.valor mÃ­nimo local de fff si f(c)â©½f(x)f(c) \\leqslant f(x)f(c)â©½f(x) cuando xxx estÃ¡ cerca de ccc.    ","version":"Next","tagName":"h3"},{"title":"Teorema del valor extremoâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#teorema-del-valor-extremo","content":" Si fff es continua solve un intervalo cerrado [a,b][a, b][a,b], entonces fff alcanza un valor mÃ¡ximo absoluto f(c)f(c)f(c) y un valor minimo absoluto f(d)f(d)f(d) en algunos nÃºmeros ccc y ddd en [a,b][a, b][a,b].  Teorema del valor extremo  ","version":"Next","tagName":"h3"},{"title":"Teorema de Fermatâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#teorema-de-fermat","content":" Si fff tiene un valor mÃ¡ximo o mÃ­nimo local en ccc, y si fâ€²(c)f'(c)fâ€²(c) existe, entonces fâ€²(c)=0f'(c) = 0fâ€²(c)=0.  Teorema de Fermat  ","version":"Next","tagName":"h3"},{"title":"NÃºmeros crÃ­ticosâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#nÃºmeros-crÃ­ticos","content":" Un nÃºmero crÃ­tico de una funciÃ³n fff es un nÃºmero ccc en el dominio de fff tal que fâ€²(c)=0f'(c) = 0fâ€²(c)=0 o fâ€²(c)f'(c)fâ€²(c) no existe.    ","version":"Next","tagName":"h3"},{"title":"MÃ©todo del intervalo cerradoâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#mÃ©todo-del-intervalo-cerrado","content":" Para hallar los valores mÃ¡ximo y mÃ­nimo absolutos de una funciÃ³n continua fff sobre un intervalo cerrado [a,b][a, b][a,b]:  Encuentre los valores de fff en los nÃºmeros crÃ­ticos de fff en (a,b)(a, b)(a,b).Halle los valores de fff en los puntos extremos del intervalo.El mÃ¡s grande de los valores de los pasos 1 y 2 es el valor mÃ¡ximo absoluto; el mÃ¡s pequeÃ±o, el valor mÃ­nimo absoluto.  Ejemplo de mÃ©todo del intervalo cerrado  ","version":"Next","tagName":"h3"},{"title":"Teorema del valor medioâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#teorema-del-valor-medio","content":" Si fff es una funciÃ³n que satisface las siguientes hipÃ³tesis:  fff es continua sobre el intervalo cerrado [a,b][a, b][a,b]fff es derivable sobre el intervalo abierto (a,b)(a, b)(a,b) entonces existe un nÃºmero x=cx=cx=c en (a,b)(a, b)(a,b) tal que  fâ€²(c)=f(b)âˆ’f(a)bâˆ’af^{\\prime}(c)=\\frac{f(b)-f(a)}{b-a}fâ€²(c)=bâˆ’af(b)âˆ’f(a)â€‹  o, equivalentemente,  f(b)âˆ’f(a)=fâ€²(c)(bâˆ’a)f(b)-f(a)=f^{\\prime}(c)(b-a)f(b)âˆ’f(a)=fâ€²(c)(bâˆ’a)  Teorema del valor medio  ","version":"Next","tagName":"h3"},{"title":"Prueba de crecimiento y decrecimientoâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#prueba-de-crecimiento-y-decrecimiento","content":" Si fâ€²(x)&gt;0f'(x) &gt; 0fâ€²(x)&gt;0 para todo xxx en un intervalo, entonces fff es creciente en ese intervalo.Si fâ€²(x)&lt;0f'(x) &lt; 0fâ€²(x)&lt;0 para todo xxx en un intervalo, entonces fff es decreciente en ese intervalo.  Prueba creciente/decreciente  ","version":"Next","tagName":"h3"},{"title":"Prueba de concavidadâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#prueba-de-concavidad","content":" Si fâ€²â€²(x)&gt;0f''(x) &gt; 0fâ€²â€²(x)&gt;0 para todo xxx en un intervalo, entonces fff es cÃ³ncava hacia arriba en ese intervalo.Si fâ€²â€²(x)&lt;0f''(x) &lt; 0fâ€²â€²(x)&lt;0 para todo xxx en un intervalo, entonces fff es cÃ³ncava hacia abajo en ese intervalo.    ","version":"Next","tagName":"h3"},{"title":"AsÃ­ntotasâ€‹","type":1,"pageTitle":"CÃ¡lculo I","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/calculo_I#asÃ­ntotas","content":" La recta x=ax=ax=a se llama asÃ­ntota vertical de la curva y=f(x)y=f(x)y=f(x) si al menos una de las siguientes afirmaciones son verdaderas:  limâ¡xâ†’af(x)=âˆlimâ¡xâ†’aâˆ’f(x)=âˆlimâ¡xâ†’a+f(x)=âˆlimâ¡xâ†’af(x)=âˆ’âˆlimâ¡xâ†’aâˆ’f(x)=âˆ’âˆlimâ¡xâ†’a+f(x)=âˆ’âˆ\\begin{array}{lll} \\displaystyle{\\lim _{x \\rightarrow a} f(x)=\\infty} &amp; \\displaystyle{\\lim _{x \\rightarrow a^{-}} f(x)=\\infty} &amp; \\displaystyle{\\lim _{x \\rightarrow a^{+}} f(x)=\\infty} \\\\[20pt] \\displaystyle{\\lim _{x \\rightarrow a} f(x)=-\\infty} &amp; \\displaystyle{\\lim _{x \\rightarrow a^{-}} f(x)=-\\infty} &amp; \\displaystyle{\\lim _{x \\rightarrow a^{+}} f(x)=-\\infty} \\end{array}xâ†’alimâ€‹f(x)=âˆxâ†’alimâ€‹f(x)=âˆ’âˆâ€‹xâ†’aâˆ’limâ€‹f(x)=âˆxâ†’aâˆ’limâ€‹f(x)=âˆ’âˆâ€‹xâ†’a+limâ€‹f(x)=âˆxâ†’a+limâ€‹f(x)=âˆ’âˆâ€‹  AsÃ­ntotas verticales  La recta y=Ly=Ly=L se llama asÃ­ntota horizontal de la curva y=f(x)y=f(x)y=f(x) si  limâ¡xâ†’âˆf(x)=LÂ oÂ limâ¡xâ†’âˆ’âˆf(x)=L\\lim _{x \\rightarrow \\infty} f(x)=L \\quad \\text { o } \\quad \\lim _{x \\rightarrow-\\infty} f(x)=Lxâ†’âˆlimâ€‹f(x)=LÂ oÂ xâ†’âˆ’âˆlimâ€‹f(x)=L  AsÃ­ntotas horizontales  Algunas curvas tienen asÃ­ntotas que son oblicuas; esto es, no son horizontales ni verticales. Si  limâ¡xâ†’âˆ[f(x)âˆ’(mx+b)]=0\\lim _{x \\rightarrow \\infty}[f(x)-(m x+b)]=0xâ†’âˆlimâ€‹[f(x)âˆ’(mx+b)]=0  entonces la recta y=mx+by=m x+by=mx+b se llama asÃ­ntota inclinada (oblicua).  AsÃ­ntotas oblicuas ","version":"Next","tagName":"h3"},{"title":"Fundamentos de los modelos de probabilidad","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos","content":"","keywords":"","version":"Next"},{"title":"Ãlgebra de eventosâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#Ã¡lgebra-de-eventos","content":" ","version":"Next","tagName":"h2"},{"title":"Definiciones importantesâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#definiciones-importantes","content":" Consideremos algunas definiciones relacionadas a un fenÃ³meno aleatorio:  Espacio muestral: Conjunto de todos los resultados posibles.Punto muestral: Un resultado particular.Evento: Subconjunto de resultados posibles.  El espacio muestral puede ser discreto o continuo. El caso discreto corresponde a un espacio muestral compuesto por un conjunto contable (numerable) de puntos muestrales, mientras que el caso continuo corresponde a un espacio muestral compuesto de un continuo de puntos muestrales.  Evento Imposible: Denotado por Ï•\\phiÏ• es un evento sin puntos muestrales. Evento Certeza: Denotado por SSS u Î©\\OmegaÎ©, es un evento que contiene a todos los puntos muestrales. Evento Complemento: Denotado por EË‰\\bar{E}EË‰, contiene todos los puntos muestrales de SSS que no estÃ¡n contenidos en un evento EEE. UniÃ³n de Eventos: Para dos eventos E1E_1E1â€‹ y E2E_2E2â€‹, su union forma un nuevo evento que contiene los puntos muestrales de E1E_1E1â€‹ y los contenidos en E2E_2E2â€‹ que no se encuentran en E1E_1E1â€‹. IntersecciÃ³n de Eventos: Para dos eventos E1E_1E1â€‹ y E2E_2E2â€‹, su intersecciÃ³n forma un nuevo evento que contiene los puntos muestrales contenidos en E1E_1E1â€‹ y en E2E_2E2â€‹ a la vez. Eventos Mutuamente Excluyentes (Disjuntos): Son eventos que no tienen puntos muestrales en comÃºn, es decir, su intersecciÃ³n es vacÃ­a. Eventos Colectivamente Exhaustivos: Son eventos que unidos conforman el espacio muestral.  U:Â UnioËŠnâˆ©:Â InterseccioËŠnEË‰:Â ComplementoÂ deÂ E\\begin{align*} U: &amp; \\text{ UniÃ³n} \\\\ \\cap: &amp; \\text{ IntersecciÃ³n} \\\\ \\bar{E}: &amp; \\text{ Complemento de } E \\\\ \\end{align*}U:âˆ©:EË‰:â€‹Â UnioËŠnÂ InterseccioËŠnÂ ComplementoÂ deÂ Eâ€‹  ","version":"Next","tagName":"h3"},{"title":"Operaciones matemÃ¡ticas de conjuntosâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#operaciones-matemÃ¡ticas-de-conjuntos","content":" Igualdad de Conjuntos: Dos conjuntos son iguales si y sÃ³lo si ambos conjuntos contienen exactamente los mismos puntos muestrales. Un caso bÃ¡sico es el siguiente AâˆªÏ•=AA \\cup \\phi=AAâˆªÏ•=A donde Ï•\\phiÏ• representa un conjunto vacÃ­o. TambiÃ©n se tiene que Aâˆ©Ï•=Ï•A \\cap \\phi=\\phiAâˆ©Ï•=Ï• Por lo tanto AâˆªA=AÂ yÂ Aâˆ©A=AA \\cup A=A \\quad \\text { y } A \\cap A=AAâˆªA=AÂ yÂ Aâˆ©A=A Con respecto al espacio muestral SSS AâˆªS=SÂ yÂ Aâˆ©S=AA \\cup S=S \\quad \\text { y } A \\cap S=AAâˆªS=SÂ yÂ Aâˆ©S=A Conjunto complemento: Con respecto a un evento EEE y su complemento EË‰\\bar{E}EË‰, se observa que EâˆªEË‰=SÂ yÂ Eâˆ©EË‰=Ï•E \\cup \\bar{E}=S \\quad \\text { y } \\quad E \\cap \\bar{E}=\\phiEâˆªEË‰=SÂ yÂ Eâˆ©EË‰=Ï• Finalmente EË‰â€¾=E\\overline{\\bar{E}}=EEË‰=E Ley Conmutativa: La uniÃ³n e intersecciÃ³n de conjuntos son conmutativas, es decir, para dos conjuntos AAA y BBB se cumple que AâˆªB=BâˆªAAâˆ©B=Bâˆ©A\\begin{aligned} &amp; A \\cup B=B \\cup A \\\\ &amp; A \\cap B=B \\cap A \\end{aligned}â€‹AâˆªB=BâˆªAAâˆ©B=Bâˆ©Aâ€‹ Ley Asociativa: La uniÃ³n e intersecciÃ³n de conjuntos es asociativa, es decir, para tres conjuntos A,BA, BA,B y CCC se cumple que (AâˆªB)âˆªC=Aâˆª(BâˆªC)=Bâˆª(AâˆªC)(Aâˆ©B)âˆ©C=Aâˆ©(Bâˆ©C)=Bâˆ©(Aâˆ©C)\\begin{aligned} &amp; (A \\cup B) \\cup C=A \\cup(B \\cup C)=B \\cup(A \\cup C) \\\\ &amp; (A \\cap B) \\cap C=A \\cap(B \\cap C)=B \\cap(A \\cap C) \\end{aligned}â€‹(AâˆªB)âˆªC=Aâˆª(BâˆªC)=Bâˆª(AâˆªC)(Aâˆ©B)âˆ©C=Aâˆ©(Bâˆ©C)=Bâˆ©(Aâˆ©C)â€‹ Ley Distributiva: La uniÃ³n e intersecciÃ³n de conjuntos es distributiva, es decir, para tres conjuntos A,BA, BA,B y CCC se cumple que (AâˆªB)âˆ©C=(Aâˆ©C)âˆª(Bâˆ©C)(Aâˆ©B)âˆªC=(AâˆªC)âˆ©(BâˆªC)\\begin{aligned} &amp; (A \\cup B) \\cap C=(A \\cap C) \\cup(B \\cap C) \\\\ &amp; (A \\cap B) \\cup C=(A \\cup C) \\cap(B \\cup C) \\end{aligned}â€‹(AâˆªB)âˆ©C=(Aâˆ©C)âˆª(Bâˆ©C)(Aâˆ©B)âˆªC=(AâˆªC)âˆ©(BâˆªC)â€‹ Ley de De Morgan: Esta ley relaciona conjuntos y sus complementos. Para dos conjuntos (eventos), E1E_1E1â€‹ y E2E_2E2â€‹, la ley de De Morgan dice que (E1âˆªE2)â€¾=EË‰1âˆ©EË‰2Â yÂ (E1âˆ©E2)â€¾=EË‰1âˆªEË‰2\\overline{\\left(E_1 \\cup E_2\\right)}=\\bar{E}_1 \\cap \\bar{E}_2 \\quad \\text { y } \\overline{\\left(E_1 \\cap E_2\\right)}=\\bar{E}_1 \\cup \\bar{E}_2(E1â€‹âˆªE2â€‹)â€‹=EË‰1â€‹âˆ©EË‰2â€‹Â yÂ (E1â€‹âˆ©E2â€‹)â€‹=EË‰1â€‹âˆªEË‰2â€‹ Generalizando (E1âˆªE2âˆªâ‹¯âˆªEn)â€¾=EË‰1âˆ©EË‰2âˆ©â‹¯âˆ©EË‰n\\overline{\\left(E_1 \\cup E_2 \\cup \\cdots \\cup E_n\\right)}=\\bar{E}_1 \\cap \\bar{E}_2 \\cap \\cdots \\cap \\bar{E}_n(E1â€‹âˆªE2â€‹âˆªâ‹¯âˆªEnâ€‹)â€‹=EË‰1â€‹âˆ©EË‰2â€‹âˆ©â‹¯âˆ©EË‰nâ€‹ y (E1âˆ©E2âˆ©â‹¯âˆ©En)â€¾=EË‰1âˆªEË‰2âˆªâ‹¯âˆªEË‰n\\overline{\\left(E_1 \\cap E_2 \\cap \\cdots \\cap E_n\\right)}=\\bar{E}_1 \\cup \\bar{E}_2 \\cup \\cdots \\cup \\bar{E}_n(E1â€‹âˆ©E2â€‹âˆ©â‹¯âˆ©Enâ€‹)â€‹=EË‰1â€‹âˆªEË‰2â€‹âˆªâ‹¯âˆªEË‰nâ€‹  ","version":"Next","tagName":"h3"},{"title":"Axiomas fundamentalesâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#axiomas-fundamentales","content":" Los axiomas son los siguientes:  Axioma 1: Para cada evento EEE contenido en un espacio muestral SSS se tiene que P(E)â‰¥0P(E) \\geq 0P(E)â‰¥0 Axioma 2: La probabilidad del evento certeza SSS es P(S)=1P(S)=1P(S)=1 Axioma 3: Para dos eventos E1E_1E1â€‹ y E2E_2E2â€‹ mutuamente excluyentes (disjuntos), P(E1âˆªE2)=P(E1)+P(E2)P\\left(E_1 \\cup E_2\\right)=P\\left(E_1\\right)+P\\left(E_2\\right)P(E1â€‹âˆªE2â€‹)=P(E1â€‹)+P(E2â€‹)  ","version":"Next","tagName":"h3"},{"title":"Ley aditivaâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#ley-aditiva","content":" Sea un evento EEE y su complemento EË‰\\bar{E}EË‰. Por ser eventos disjuntos se tiene que  P(EâˆªEË‰)=P(E)+P(EË‰)P(E \\cup \\bar{E})=P(E)+P(\\bar{E})P(EâˆªEË‰)=P(E)+P(EË‰)  AdemÃ¡s como (EâˆªEË‰)=S(E \\cup \\bar{E})=S(EâˆªEË‰)=S, se tiene que  P(EË‰)=1âˆ’P(E)P(\\bar{E})=1-P(E)P(EË‰)=1âˆ’P(E)  Por otra parte  P(Eâˆ©EË‰)=P(Ï•)=0P(E \\cap \\bar{E})=P(\\phi)=0P(Eâˆ©EË‰)=P(Ï•)=0  Finalmente para dos eventos cualquiera E1E_1E1â€‹ y E2E_2E2â€‹ la ley aditiva dice que  P(E1âˆªE2)=P(E1)+P(E2)âˆ’P(E1âˆ©E2)P\\left(E_1 \\cup E_2\\right)=P\\left(E_1\\right)+P\\left(E_2\\right)-P\\left(E_1 \\cap E_2\\right)P(E1â€‹âˆªE2â€‹)=P(E1â€‹)+P(E2â€‹)âˆ’P(E1â€‹âˆ©E2â€‹)  Esta ecuaciÃ³n aplicada a la uniÃ³n de tres eventos E1E_1E1â€‹, E2E_2E2â€‹ y E3E_3E3â€‹ es  P(E1âˆªE2âˆªE3)=P[(E1âˆªE2)âˆªE3]=P(E1âˆªE2)+P(E3)âˆ’P[(E1âˆªE2)âˆ©E3]=P(E1)+P(E2)âˆ’P(E1âˆ©E2)+P(E3)âˆ’P[(E1âˆ©E3)âˆª(E2âˆ©E3)]=P(E1)+P(E2)+P(E3)âˆ’P(E1âˆ©E2)âˆ’P(E1âˆ©E3)âˆ’P(E2âˆ©E3)+P(E1âˆ©E2âˆ©E3)\\begin{aligned} P\\left(E_1 \\cup E_2 \\cup E_3\\right)= &amp; P\\left[\\left(E_1 \\cup E_2\\right) \\cup E_3\\right] \\\\ = &amp; P\\left(E_1 \\cup E_2\\right)+P\\left(E_3\\right)-P\\left[\\left(E_1 \\cup E_2\\right) \\cap E_3\\right] \\\\ = &amp; P\\left(E_1\\right)+P\\left(E_2\\right)-P\\left(E_1 \\cap E_2\\right)+P\\left(E_3\\right)-P\\left[\\left(E_1 \\cap E_3\\right) \\cup\\left(E_2 \\cap E_3\\right)\\right] \\\\ = &amp; P\\left(E_1\\right)+P\\left(E_2\\right)+P\\left(E_3\\right)-P\\left(E_1 \\cap E_2\\right)-P\\left(E_1 \\cap E_3\\right)-P\\left(E_2 \\cap E_3\\right) \\\\ &amp; +P\\left(E_1 \\cap E_2 \\cap E_3\\right) \\end{aligned}P(E1â€‹âˆªE2â€‹âˆªE3â€‹)====â€‹P[(E1â€‹âˆªE2â€‹)âˆªE3â€‹]P(E1â€‹âˆªE2â€‹)+P(E3â€‹)âˆ’P[(E1â€‹âˆªE2â€‹)âˆ©E3â€‹]P(E1â€‹)+P(E2â€‹)âˆ’P(E1â€‹âˆ©E2â€‹)+P(E3â€‹)âˆ’P[(E1â€‹âˆ©E3â€‹)âˆª(E2â€‹âˆ©E3â€‹)]P(E1â€‹)+P(E2â€‹)+P(E3â€‹)âˆ’P(E1â€‹âˆ©E2â€‹)âˆ’P(E1â€‹âˆ©E3â€‹)âˆ’P(E2â€‹âˆ©E3â€‹)+P(E1â€‹âˆ©E2â€‹âˆ©E3â€‹)â€‹  Para nnn eventos cualquiera, por De Morgan se tiene lo siguiente:  P(E1âˆªE2âˆªâ‹¯âˆªEn)=1âˆ’P(E1âˆªE2âˆªâ‹¯âˆªEnâ€¾)=1âˆ’P(EË‰1âˆ©EË‰2âˆ©â‹¯âˆ©EË‰n)\\begin{aligned} P\\left(E_1 \\cup E_2 \\cup \\cdots \\cup E_n\\right) &amp; =1-P\\left(\\overline{E_1 \\cup E_2 \\cup \\cdots \\cup E_n}\\right) \\\\ &amp; =1-P\\left(\\bar{E}_1 \\cap \\bar{E}_2 \\cap \\cdots \\cap \\bar{E}_n\\right) \\end{aligned}P(E1â€‹âˆªE2â€‹âˆªâ‹¯âˆªEnâ€‹)â€‹=1âˆ’P(E1â€‹âˆªE2â€‹âˆªâ‹¯âˆªEnâ€‹â€‹)=1âˆ’P(EË‰1â€‹âˆ©EË‰2â€‹âˆ©â‹¯âˆ©EË‰nâ€‹)â€‹  En el caso de E1,â€¦,EnE_1, \\ldots, E_nE1â€‹,â€¦,Enâ€‹ sean eventos mutuamente excluyentes  P(E1âˆªE2âˆªâ‹¯âˆªEn)=âˆ‘i=1nP(Ei)P\\left(E_1 \\cup E_2 \\cup \\cdots \\cup E_n\\right)=\\sum_{i=1}^n P\\left(E_i\\right)P(E1â€‹âˆªE2â€‹âˆªâ‹¯âˆªEnâ€‹)=i=1âˆ‘nâ€‹P(Eiâ€‹)  ","version":"Next","tagName":"h3"},{"title":"MÃ©todos de conteoâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#mÃ©todos-de-conteo","content":" Cuando los espacios muestrales son finitos, basta con asignar probabilidades a cada uno de los resultados posibles para luego obtener las probabilidad de un suceso simplemente sumando las probabilidades de ocurrencia de cada resultado bÃ¡sico que lo componen.  S={Ï‰1,â€¦,Ï‰N}S=\\left\\{\\omega_1, \\ldots, \\omega_N\\right\\}S={Ï‰1â€‹,â€¦,Ï‰Nâ€‹}  con pi=P({Ï‰i}),i=1,â€¦,Np_i=P\\left(\\left\\{\\omega_i\\right\\}\\right), i=1, \\ldots, Npiâ€‹=P({Ï‰iâ€‹}),i=1,â€¦,N. Para el caso de Probabilidad ClÃ¡sica se tiene que para un suceso AAA :  P(A)=#A#S=NuËŠmeroÂ deÂ casosÂ favorablesNuËŠmeroÂ deÂ casosÂ posiblesP(A)=\\frac{\\# A}{\\# S} = \\frac{\\text{NÃºmero de casos favorables}}{\\text{NÃºmero de casos posibles}}P(A)=#S#Aâ€‹=NuËŠmeroÂ deÂ casosÂ posiblesNuËŠmeroÂ deÂ casosÂ favorablesâ€‹  ","version":"Next","tagName":"h2"},{"title":"Principio de multiplicaciÃ³nâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#principio-de-multiplicaciÃ³n","content":" Si un experimento estÃ¡ compuesto de kkk experimentos con tamaÃ±os muestrales n1,â€¦,nkn_1, \\ldots, n_kn1â€‹,â€¦,nkâ€‹, entonces  #S=n1Ã—n2Ã—â‹¯Ã—nk\\# S=n_1 \\times n_2 \\times \\cdots \\times n_k#S=n1â€‹Ã—n2â€‹Ã—â‹¯Ã—nkâ€‹  Por ejemplo, si se tienen n1n_1n1â€‹ maneras de realizar el primer experimento, n2n_2n2â€‹ maneras de realizar el segundo experimento, y asÃ­ sucesivamente, entonces el nÃºmero total de maneras de realizar el experimento compuesto es n1Ã—n2Ã—â‹¯Ã—nkn_1 \\times n_2 \\times \\cdots \\times n_kn1â€‹Ã—n2â€‹Ã—â‹¯Ã—nkâ€‹.  ","version":"Next","tagName":"h3"},{"title":"PermutaciÃ³nâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#permutaciÃ³n","content":" Consideremos un conjunto de objetos  C={c1,â€¦,cn}C=\\left\\{c_1, \\ldots, c_n\\right\\}C={c1â€‹,â€¦,cnâ€‹}  y queremos seleccionar una muestra de rrr objetos. Â¿De cuÃ¡ntas maneras lo podemos hacer?  Muestreo Con Reemplazo: nrn^rnr.Muestreo Sin Reemplazo: nÃ—(nâˆ’1)Ã—(nâˆ’2)Ã—â‹¯Ã—(nâˆ’r+1)n \\times(n-1) \\times(n-2) \\times \\cdots \\times(n-r+1)nÃ—(nâˆ’1)Ã—(nâˆ’2)Ã—â‹¯Ã—(nâˆ’r+1).  La permutaciÃ³n se denota como P(n,r)P(n, r)P(n,r) y se define como  P(n,r)=nÃ—(nâˆ’1)Ã—(nâˆ’2)Ã—â‹¯Ã—(nâˆ’r+1)=n!(nâˆ’r)!P(n, r)=n \\times(n-1) \\times(n-2) \\times \\cdots \\times(n-r+1)=\\frac{n!}{(n-r)!}P(n,r)=nÃ—(nâˆ’1)Ã—(nâˆ’2)Ã—â‹¯Ã—(nâˆ’r+1)=(nâˆ’r)!n!â€‹  ","version":"Next","tagName":"h3"},{"title":"CombinaciÃ³nâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#combinaciÃ³n","content":" Consideremos un Muestreo Sin Reemplazo. Si nos interesa una muestra sin importar el orden de ingreso, la cantidad de muestras distintas de tamaÃ±o rrr son  (nr)=n!r!Ã—(nâˆ’r)!\\left(\\begin{array}{l} n \\\\ r \\end{array}\\right)=\\frac{n !}{r ! \\times(n-r) !}(nrâ€‹)=r!Ã—(nâˆ’r)!n!â€‹  Estos &quot;nÃºmeros&quot; se conocen como coeficientes binomiales y tienen la siguiente propiedad  (a+b)n=âˆ‘k=0n(nk)akbnâˆ’k(a+b)^n=\\sum_{k=0}^n\\left(\\begin{array}{l} n \\\\ k \\end{array}\\right) a^k b^{n-k}(a+b)n=k=0âˆ‘nâ€‹(nkâ€‹)akbnâˆ’k  ","version":"Next","tagName":"h3"},{"title":"Ordenamiento multinomialâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#ordenamiento-multinomial","content":" Queremos asignar nnn objetos a kkk grupos distintos de tamaÃ±os n1,â€¦n_1, \\ldotsn1â€‹,â€¦, nkn_knkâ€‹, con âˆ‘i=1kni=n\\displaystyle\\sum_{i=1}^k n_i=ni=1âˆ‘kâ€‹niâ€‹=n. El nÃºmero de grupos distintos con las caracterÃ­sticas dadas son  (nn1n2â‹¯nk)=n!n1!Ã—â‹¯Ã—nk!\\left(\\begin{array}{c} n \\\\ n_1 n_2 \\cdots n_k \\end{array}\\right)=\\frac{n !}{n_{1} ! \\times \\cdots \\times n_{k} !}(nn1â€‹n2â€‹â‹¯nkâ€‹â€‹)=n1â€‹!Ã—â‹¯Ã—nkâ€‹!n!â€‹  Estos &quot;nÃºmeros&quot; se conocen como ordenamientos multinomiales y tienen la siguiente propiedad  (x1+â‹¯+xk)n=âˆ‘n1=0nâˆ‘n2=0nâˆ’n1â‹¯âˆ‘nk=0nâˆ’n1âˆ’â‹¯âˆ’nkâˆ’1n!n1!Ã—â‹¯Ã—nk!x1n1Ã—â‹¯Ã—xknk\\left(x_1+\\cdots+x_k\\right)^n=\\sum_{n_1=0}^n \\sum_{n_2=0}^{n-n_1} \\cdots \\sum_{n_k=0}^{n-n_1-\\cdots-n_{k-1}} \\frac{n !}{n_{1} ! \\times \\cdots \\times n_{k} !} x_1^{n_1} \\times \\cdots \\times x_k^{n_k}(x1â€‹+â‹¯+xkâ€‹)n=n1â€‹=0âˆ‘nâ€‹n2â€‹=0âˆ‘nâˆ’n1â€‹â€‹â‹¯nkâ€‹=0âˆ‘nâˆ’n1â€‹âˆ’â‹¯âˆ’nkâˆ’1â€‹â€‹n1â€‹!Ã—â‹¯Ã—nkâ€‹!n!â€‹x1n1â€‹â€‹Ã—â‹¯Ã—xknkâ€‹â€‹  ","version":"Next","tagName":"h3"},{"title":"Probabilidad condicionalâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#probabilidad-condicional","content":" Cuando la ocurrencia de un evento (o no ocurrencia) depende de otro evento, es relevante ver la probabilidad como una probabilidad condicional.  Se define la probabilidad que un evento E1E_1E1â€‹ ocurra bajo el supuesto que otro evento E2E_2E2â€‹ ocurre con certeza a  P(E1âˆ£E2)=P(E1âˆ©E2)P(E2)P\\left(E_1 \\mid E_2\\right)=\\frac{P\\left(E_1 \\cap E_2\\right)}{P\\left(E_2\\right)}P(E1â€‹âˆ£E2â€‹)=P(E2â€‹)P(E1â€‹âˆ©E2â€‹)â€‹  En general, la probabilidad de un evento EEE ya estÃ¡ condicionada se condiciona a la ocurrencia del evento certeza SSS:  P(Eâˆ£S)=P(Eâˆ©S)P(S)=P(E)P(E \\mid S)=\\frac{P(E \\cap S)}{P(S)}=P(E)P(Eâˆ£S)=P(S)P(Eâˆ©S)â€‹=P(E)  Consideremos las probabilidades de un evento E1E_1E1â€‹ y su complemento EË‰1\\bar{E}_1EË‰1â€‹ condicionados a la ocurrencia previa de un evento E2E_2E2â€‹.  P(E1âˆ£E2)=P(E1âˆ©E2)P(E2)Â yÂ P(EË‰1âˆ£E2)=P(EË‰1âˆ©E2)P(E2)P\\left(E_1 \\mid E_2\\right)=\\frac{P\\left(E_1 \\cap E_2\\right)}{P\\left(E_2\\right)} \\quad \\text { y } \\quad P\\left(\\bar{E}_1 \\mid E_2\\right)=\\frac{P\\left(\\bar{E}_1 \\cap E_2\\right)}{P\\left(E_2\\right)}P(E1â€‹âˆ£E2â€‹)=P(E2â€‹)P(E1â€‹âˆ©E2â€‹)â€‹Â yÂ P(EË‰1â€‹âˆ£E2â€‹)=P(E2â€‹)P(EË‰1â€‹âˆ©E2â€‹)â€‹  Si las sumamos tenemos que  P(EË‰1âˆ£E2)=1âˆ’P(E1âˆ£E2)P\\left(\\bar{E}_1 \\mid E_2\\right)=1-P\\left(E_1 \\mid E_2\\right)P(EË‰1â€‹âˆ£E2â€‹)=1âˆ’P(E1â€‹âˆ£E2â€‹)  ","version":"Next","tagName":"h2"},{"title":"Independencia estadÃ­sticaâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#independencia-estadÃ­stica","content":" Dos eventos E1E_1E1â€‹ y E2E_2E2â€‹ se dice que son estadÃ­sticamente independientes si la ocurrencia de un evento no depende de la ocurrencia o no ocurrencia del otro.  Es decir,  P(E1âˆ£E2)=P(E1)Â oËŠÂ P(E2âˆ£E1)=P(E2)P\\left(E_1 \\mid E_2\\right)=P\\left(E_1\\right) \\text { Ã³ } P\\left(E_2 \\mid E_1\\right)=P\\left(E_2\\right)P(E1â€‹âˆ£E2â€‹)=P(E1â€‹)Â oËŠÂ P(E2â€‹âˆ£E1â€‹)=P(E2â€‹)  A partir de la ecuaciÃ³n de probabilidad condicional se deduce que si E1E_1E1â€‹ y E2E_2E2â€‹ son eventos posibles entonces  P(E1âˆ©E2)=P(E1âˆ£E2)â‹…P(E2)Â oËŠÂ P(E1âˆ©E2)=P(E2âˆ£E1)â‹…P(E1)P\\left(E_1 \\cap E_2\\right)=P\\left(E_1 \\mid E_2\\right) \\cdot P\\left(E_2\\right) \\quad \\text { Ã³ } \\quad P\\left(E_1 \\cap E_2\\right)=P\\left(E_2 \\mid E_1\\right) \\cdot P\\left(E_1\\right)P(E1â€‹âˆ©E2â€‹)=P(E1â€‹âˆ£E2â€‹)â‹…P(E2â€‹)Â oËŠÂ P(E1â€‹âˆ©E2â€‹)=P(E2â€‹âˆ£E1â€‹)â‹…P(E1â€‹)  Si E1E_1E1â€‹ y E2E_2E2â€‹ fuesen eventos estadÃ­sticamente independientes entonces  P(E1âˆ©E2)=P(E1)â‹…P(E2)P\\left(E_1 \\cap E_2\\right)=P\\left(E_1\\right) \\cdot P\\left(E_2\\right)P(E1â€‹âˆ©E2â€‹)=P(E1â€‹)â‹…P(E2â€‹)  ","version":"Next","tagName":"h3"},{"title":"Ley multiplicativaâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#ley-multiplicativa","content":" Para tres eventos E1,E2E_1, E_2E1â€‹,E2â€‹ y E3E_3E3â€‹ la ley multiplicativa implica por ejemplo que  P(E1âˆ©E2âˆ©E3)={P(E3âˆ£E1âˆ©E2)â‹…P(E2âˆ£E1)â‹…P(E1)P(E1âˆ©E2âˆ£E3)â‹…P(E3)P\\left(E_1 \\cap E_2 \\cap E_3\\right)=\\left\\{\\begin{array}{l} P\\left(E_3 \\mid E_1 \\cap E_2\\right) \\cdot P\\left(E_2 \\mid E_1\\right) \\cdot P\\left(E_1\\right) \\\\ P\\left(E_1 \\cap E_2 \\mid E_3\\right) \\cdot P\\left(E_3\\right) \\end{array}\\right.P(E1â€‹âˆ©E2â€‹âˆ©E3â€‹)={P(E3â€‹âˆ£E1â€‹âˆ©E2â€‹)â‹…P(E2â€‹âˆ£E1â€‹)â‹…P(E1â€‹)P(E1â€‹âˆ©E2â€‹âˆ£E3â€‹)â‹…P(E3â€‹)â€‹  ","version":"Next","tagName":"h2"},{"title":"Independenciaâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#independencia","content":" Consideremos ahora los eventos E1,E2,â€¦,EnE_1, E_2, \\ldots, E_nE1â€‹,E2â€‹,â€¦,Enâ€‹. Estos eventos se dicen mutuamente independientes si y solo si, cualquier sub-colecciÃ³n de eventos de ellos Ei1,Ei2,â€¦,EimE_{i 1}, E_{i 2}, \\ldots, E_{i m}Ei1â€‹,Ei2â€‹,â€¦,Eimâ€‹ cumple con la siguiente condiciÃ³n  P(Ei1âˆ©Ei2âˆ©â‹¯âˆ©Eim)=P(Ei1)Ã—P(Ei2)Ã—â‹¯Ã—P(Eim)P\\left(E_{i 1} \\cap E_{i 2} \\cap \\cdots \\cap E_{i m}\\right)=P\\left(E_{i 1}\\right) \\times P\\left(E_{i 2}\\right) \\times \\cdots \\times P\\left(E_{i m}\\right)P(Ei1â€‹âˆ©Ei2â€‹âˆ©â‹¯âˆ©Eimâ€‹)=P(Ei1â€‹)Ã—P(Ei2â€‹)Ã—â‹¯Ã—P(Eimâ€‹)  ","version":"Next","tagName":"h3"},{"title":"Propiedadesâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#propiedades","content":" Si E1E_1E1â€‹ y E2E_2E2â€‹ son eventos estadÃ­sticamente independientes, entonces EË‰1\\bar{E}_1EË‰1â€‹ y EË‰2\\bar{E}_2EË‰2â€‹ tambiÃ©n lo son.Si E1E_1E1â€‹ y E2E_2E2â€‹ son eventos estadÃ­sticamente independientes dado un evento AAA, entoncesP(E1âˆ©E2âˆ£A)=P(E1âˆ£A)â‹…P(E2âˆ£A)P\\left(E_1 \\cap E_2 \\mid A\\right)=P\\left(E_1 \\mid A\\right) \\cdot P\\left(E_2 \\mid A\\right)P(E1â€‹âˆ©E2â€‹âˆ£A)=P(E1â€‹âˆ£A)â‹…P(E2â€‹âˆ£A)Para dos eventos cualesquiera E1E_1E1â€‹ y E2E_2E2â€‹ se tiene queP(E1âˆªE2âˆ£A)=P(E1âˆ£A)+P(E2âˆ£A)âˆ’P(E1âˆ©E2âˆ£A)P\\left(E_1 \\cup E_2 \\mid A\\right)=P\\left(E_1 \\mid A\\right)+P\\left(E_2 \\mid A\\right)-P\\left(E_1 \\cap E_2 \\mid A\\right)P(E1â€‹âˆªE2â€‹âˆ£A)=P(E1â€‹âˆ£A)+P(E2â€‹âˆ£A)âˆ’P(E1â€‹âˆ©E2â€‹âˆ£A)  ","version":"Next","tagName":"h3"},{"title":"Teorema de probabilidades totalesâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#teorema-de-probabilidades-totales","content":" Considere nnn eventos posibles E1,E2,â€¦,EnE_1, E_2, \\ldots, E_nE1â€‹,E2â€‹,â€¦,Enâ€‹ colectivamente exhaustivos y mutuamente excluyentes, es decir,  â‹ƒi=1nEi=SÂ yÂ Eiâˆ©Ej=Ï•âˆ€iâ‰ j\\bigcup_{i=1}^n E_i=S \\quad \\text { y } \\quad E_i \\cap E_j=\\phi \\quad \\forall i \\neq ji=1â‹ƒnâ€‹Eiâ€‹=SÂ yÂ Eiâ€‹âˆ©Ejâ€‹=Ï•âˆ€iî€ =j  Entonces  A=Aâˆ©S=Aâˆ©[â‹ƒi=1nEi]=â‹ƒi=1n(Aâˆ©Ei),A=A \\cap S=A \\cap\\left[\\bigcup_{i=1}^n E_i\\right]=\\bigcup_{i=1}^n\\left(A \\cap E_i\\right),A=Aâˆ©S=Aâˆ©[i=1â‹ƒnâ€‹Eiâ€‹]=i=1â‹ƒnâ€‹(Aâˆ©Eiâ€‹),  con (Aâˆ©E1),â€¦,(Aâˆ©En)\\left(A \\cap E_1\\right), \\ldots,\\left(A \\cap E_n\\right)(Aâˆ©E1â€‹),â€¦,(Aâˆ©Enâ€‹) eventos mutuamente excluyentes. Por lo tanto, por axioma 3 y ley multiplcativa  P(A)=âˆ‘i=1nP(Aâˆ©Ei)=âˆ‘i=1nP(Aâˆ£Ei)â‹…P(Ei)P(A)=\\sum_{i=1}^n P\\left(A \\cap E_i\\right)=\\sum_{i=1}^n P\\left(A \\mid E_i\\right) \\cdot P\\left(E_i\\right)P(A)=i=1âˆ‘nâ€‹P(Aâˆ©Eiâ€‹)=i=1âˆ‘nâ€‹P(Aâˆ£Eiâ€‹)â‹…P(Eiâ€‹)  ","version":"Next","tagName":"h2"},{"title":"Teorema de Bayesâ€‹","type":1,"pageTitle":"Fundamentos de los modelos de probabilidad","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/fundamentos#teorema-de-bayes","content":" Si cada evento EjE_jEjâ€‹ de la particiÃ³n de SSS y el evento AAA son posibles, entonces por la ley multiplicativa se tiene que  P(Aâˆ£Ej)â‹…P(Ej)=P(Ejâˆ£A)â‹…P(A)P\\left(A \\mid E_j\\right) \\cdot P\\left(E_j\\right)=P\\left(E_j \\mid A\\right) \\cdot P(A)P(Aâˆ£Ejâ€‹)â‹…P(Ejâ€‹)=P(Ejâ€‹âˆ£A)â‹…P(A)  Es decir,  P(Ejâˆ£A)=P(Aâˆ£Ej)â‹…P(Ej)P(A)P\\left(E_j \\mid A\\right)=\\frac{P\\left(A \\mid E_j\\right) \\cdot P\\left(E_j\\right)}{P(A)}P(Ejâ€‹âˆ£A)=P(A)P(Aâˆ£Ejâ€‹)â‹…P(Ejâ€‹)â€‹  Aplicando el teorema de probabilidades totales se tiene que  P(Ejâˆ£A)=P(Aâˆ£Ej)â‹…P(Ej)âˆ‘i=1nP(Aâˆ£Ei)â‹…P(Ei)=P(Aâˆ£Ej)â‹…P(Ej)P(A)P\\left(E_j \\mid A\\right)=\\frac{P\\left(A \\mid E_j\\right) \\cdot P\\left(E_j\\right)}{\\sum_{i=1}^n P\\left(A \\mid E_i\\right) \\cdot P\\left(E_i\\right)} = \\frac{P\\left(A \\mid E_j\\right) \\cdot P\\left(E_j\\right)}{P(A)}P(Ejâ€‹âˆ£A)=âˆ‘i=1nâ€‹P(Aâˆ£Eiâ€‹)â‹…P(Eiâ€‹)P(Aâˆ£Ejâ€‹)â‹…P(Ejâ€‹)â€‹=P(A)P(Aâˆ£Ejâ€‹)â‹…P(Ejâ€‹)â€‹  Este resultado se conoce como el Teorema de Bayes. En general, una fÃ³rmula del teorema de Bayes para dos eventos AAA y BBB es  P(Aâˆ£B)=P(Bâˆ£A)â‹…P(A)P(B)P\\left(A \\mid B\\right)=\\frac{P\\left(B \\mid A\\right) \\cdot P(A)}{P(B)}P(Aâˆ£B)=P(B)P(Bâˆ£A)â‹…P(A)â€‹  Diagrama de Ã¡rbol para dos eventos A y B ","version":"Next","tagName":"h2"},{"title":"Ãlgebra lineal","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal","content":"","keywords":"","version":"Next"},{"title":"Ecuaciones lineales en Ã¡lgebra linealâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#ecuaciones-lineales-en-Ã¡lgebra-lineal","content":" ","version":"Next","tagName":"h2"},{"title":"Operaciones elementales de filaâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#operaciones-elementales-de-fila","content":" Las operaciones elementales de fila son tres operaciones que se pueden realizar en las filas de una matriz:  (Reemplazo) Sustituir una fila por la suma de sÃ­ misma y un mÃºltiplo de otra fila.(Intercambio) Intercambiar dos filas.(Escalamiento) Multiplicar todos los elementos de una fila por una constante diferente de cero.  ","version":"Next","tagName":"h3"},{"title":"Forma escalonada de una matrizâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#forma-escalonada-de-una-matriz","content":" Una matriz rectangular estÃ¡ en forma escalonada (o forma escalonada por filas) si tiene las siguientes tres propiedades:  Todas las filas diferentes de cero estÃ¡n arriba de las filas que solo contienen ceros.Cada entrada principal de una fila estÃ¡ en una columna a la derecha de la entrada principal de la fila superior.En una columna todas las entradas debajo de la entrada principal son ceros.  Si una matriz de forma escalonada satisface las siguientes condiciones adicionales, entonces estÃ¡ en forma escalonada reducida (o forma escalonada reducida por filas):  La entrada principal en cada fila diferente de cero es 111.Cada entrada principal 111 es la Ãºnica entrada distinta de cero en su columna.  Por ejemplo, las matrices  [2âˆ’32101âˆ’480005/2]Â yÂ [10029010160013]\\left[\\begin{array}{rrrc} 2 &amp; -3 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; -4 &amp; 8 \\\\ 0 &amp; 0 &amp; 0 &amp; 5 / 2 \\end{array}\\right] \\text { y }\\left[\\begin{array}{llll} 1 &amp; 0 &amp; 0 &amp; 29 \\\\ 0 &amp; 1 &amp; 0 &amp; 16 \\\\ 0 &amp; 0 &amp; 1 &amp; 3 \\end{array}\\right]â€‹200â€‹âˆ’310â€‹2âˆ’40â€‹185/2â€‹â€‹Â yÂ â€‹100â€‹010â€‹001â€‹29163â€‹â€‹  estÃ¡n en forma escalonada y forma escalonada reducida, respectivamente.  De forma general, las matrices en forma escalonada pueden tener entradas principales (â– \\small\\blacksquareâ– ) con cualquier valor diferente de cero, pero las entradas con asterisco (âˆ—*âˆ—) pueden tener cualquier valor, incluyendo cero.  [â– âˆ—âˆ—âˆ—0â– âˆ—âˆ—00000000]\\left[\\begin{array}{llll} \\small\\blacksquare &amp; * &amp; * &amp; * \\\\ 0 &amp; \\small\\blacksquare &amp; * &amp; * \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right]â€‹â– 000â€‹âˆ—â– 00â€‹âˆ—âˆ—00â€‹âˆ—âˆ—00â€‹â€‹  En cambio, las matrices en forma escalonada reducida tienen entradas principales iguales a 111 y hay ceros abajo y arriba de cada entrada principal 111.  [10âˆ—âˆ—01âˆ—âˆ—00000000]\\left[\\begin{array}{llll} 1 &amp; 0 &amp; * &amp; * \\\\ 0 &amp; 1 &amp; * &amp; * \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right]â€‹1000â€‹0100â€‹âˆ—âˆ—00â€‹âˆ—âˆ—00â€‹â€‹  ","version":"Next","tagName":"h3"},{"title":"Unicidad de la forma escalonada reducidaâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#unicidad-de-la-forma-escalonada-reducida","content":" Teorema Cada matriz es equivalente por filas a una, y solo a una, matriz escalonada reducida.  Si una matriz AAA es equivalente por filas a una matriz escalonada UUU, entonces UUU se llama una forma escalonada (o una forma escalonada por filas) de AAA; si UUU estÃ¡ en forma escalonada reducida, entonces UUU es la forma escalonada reducida de AAA.  ","version":"Next","tagName":"h3"},{"title":"Posiciones pivoteâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#posiciones-pivote","content":" Una posiciÃ³n pivote en una matriz AAA es una ubicaciÃ³n en AAA que corresponde a un 111 principal en la forma escalonada reducida de AAA. Una columna pivote es una columna de AAA que contiene una posiciÃ³n pivote.  En los ejemplos anteriores, los cuadrados (â– \\small\\blacksquareâ– ) identifican las posiciones pivote.  ","version":"Next","tagName":"h3"},{"title":"Algoritmo de reducciÃ³n por filasâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#algoritmo-de-reducciÃ³n-por-filas","content":" El algoritmo que sigue consta de cuatro pasos y produce una matriz en forma escalonada. Un quinto paso da por resultado una matriz en forma escalonada reducida.  Se inicia con la columna diferente de cero del extremo izquierdo. Esta es una columna pivote. La posiciÃ³n pivote se ubica en la parte superior.Seleccione como pivote una entrada diferente de cero en la columna pivote. Si es necesario, intercambie filas para mover esta entrada a la posiciÃ³n pivote.Utilice operaciones de remplazo de filas para crear ceros en todas las posiciones ubicadas debajo del pivote.Cubra (o ignore) la fila que contiene la posiciÃ³n pivote y cubra todas las filas, si las hay, por encima de esta. Aplique los pasos 1 a 3 a la submatriz restante. Repita el proceso hasta que no haya filas diferentes de cero por modificar.Empezando con la posiciÃ³n pivote del extremo derecho y trabajando hacia arriba y hacia la izquierda, genere ceros arriba de cada pivote. Si un pivote no es 1 , conviÃ©rtalo en 1 mediante una operaciÃ³n de escalamiento.  La combinaciÃ³n de los pasos 1 a 4 se conoce como fase progresiva del algoritmo de reducciÃ³n por filas. El paso 5, que produce la Ãºnica forma escalonada reducida, se conoce como fase regresiva.  Ejemplo: Algoritmo de reducciÃ³n por filas Transforme la siguiente matriz a la forma escalonada, y luego a la forma escalonada reducida: [03âˆ’664âˆ’53âˆ’78âˆ’5893âˆ’912âˆ’9615]\\left[\\begin{array}{rrrrrr} 0 &amp; 3 &amp; -6 &amp; 6 &amp; 4 &amp; -5 \\\\ 3 &amp; -7 &amp; 8 &amp; -5 &amp; 8 &amp; 9 \\\\ 3 &amp; -9 &amp; 12 &amp; -9 &amp; 6 &amp; 15 \\end{array}\\right]â€‹033â€‹3âˆ’7âˆ’9â€‹âˆ’6812â€‹6âˆ’5âˆ’9â€‹486â€‹âˆ’5915â€‹â€‹ Paso 1: Elegimos el pivote. Paso 2: Intercambiamos las filas 1 y 3. (O bien, tambiÃ©n se podrÃ­an intercambiar las filas 1 y 2). Paso 3: Como paso preliminar, se podrÃ­a dividir la fila superior entre el pivote, 333. Pero con dos nÃºmeros 333 en la columna 1, esto es tan fÃ¡cil como sumar la fila 1 multiplicada por âˆ’1-1âˆ’1 a la fila 2. Paso 4: Con la fila 1 cubierta, el paso 1 muestra que la columna 2 es la prÃ³xima columna pivote; para el paso 2, seleccione como pivote la entrada &quot;superior&quot; en esa columna. En el paso 3, se podrÃ­a insertar un paso adicional de dividir la fila &quot;superior&quot; de la submatriz entre el pivote, 222. En vez de ello, se suma la fila &quot;superior&quot; multiplicada por âˆ’3/2-3 / 2âˆ’3/2 a la fila de abajo. Esto produce Para el paso 4, cuando se cubre la fila que contiene la segunda posiciÃ³n pivote, se obtiene una nueva submatriz con una sola fila: Los pasos 1 a 3 no necesitan aplicarse para esta submatriz, pues ya se ha alcanzado una forma escalonada para la matriz completa. Si se desea la forma escalonada reducida, se efectÃºa un paso mÃ¡s. Paso 5: El pivote del extremo derecho estÃ¡ en la fila 3. Genere ceros sobre Ã©l, sumando mÃºltiplos adecuados de la fila 3 a las filas 1 y 2. El siguiente pivote se encuentra en la fila 2 . Se escala esta fila dividiÃ©ndola entre el pivote. Cree un cero en la columna 2 sumando la fila 2 multiplicada por 9 a la fila 1. Finalmente, escale la fila 1 dividiÃ©ndola entre el pivote, 3. Esta es la forma escalonada reducida de la matriz original.  ","version":"Next","tagName":"h3"},{"title":"Soluciones de sistemas linealesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#soluciones-de-sistemas-lineales","content":" El algoritmo de reducciÃ³n por filas conduce directamente a una descripciÃ³n explÃ­cita del conjunto soluciÃ³n de un sistema lineal cuando se aplica a la matriz aumentada del sistema.  Suponga, por ejemplo, que la matriz aumentada de un sistema lineal se transformÃ³ a la forma escalonada reducida equivalente  [10âˆ’5101140000]\\left[\\begin{array}{rrrr}1 &amp; 0 &amp; -5 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]â€‹100â€‹010â€‹âˆ’510â€‹140â€‹â€‹  Existen tres variables porque la matriz aumentada tiene cuatro columnas. El sistema de ecuaciones asociado es  x1âˆ’5x3=1x2+x3=40=0\\begin{aligned} x_1-5 x_3 &amp; =1 \\\\ x_2+x_3 &amp; =4 \\\\ 0 &amp; =0 \\end{aligned}x1â€‹âˆ’5x3â€‹x2â€‹+x3â€‹0â€‹=1=4=0â€‹  Las variables x1x_1x1â€‹ y x2x_2x2â€‹ correspondientes a las columnas pivote se conocen como variables bÃ¡sicas. La otra variable, x3x_3x3â€‹, se denomina variable libre.  Siempre que un sistema es consistente, como el anterior, el conjunto soluciÃ³n se puede describir explÃ­citamente al despejar en el sistema de ecuaciones reducido las variables bÃ¡sicas en tÃ©rminos de las variables libres. Esta operaciÃ³n es posible porque la forma escalonada reducida coloca a cada variable bÃ¡sica en una y solo una ecuaciÃ³n. En el sistema anterior, despeje x1x_1x1â€‹ de la primera ecuaciÃ³n y x2x_2x2â€‹ de la segunda. (Ignore la tercera ecuaciÃ³n, ya que no ofrece restricciones sobre las variables).  {x1=1+5x3x2=4âˆ’x3x3Â esÂ libreÂ \\left\\{\\begin{array}{l} x_1=1+5 x_3 \\\\ x_2=4-x_3 \\\\ x_3 \\text { es libre } \\end{array}\\right.â©â¨â§â€‹x1â€‹=1+5x3â€‹x2â€‹=4âˆ’x3â€‹x3â€‹Â esÂ libreÂ â€‹  El enunciado &quot;x3x_3x3â€‹ es libre&quot; significa que existe libertad de elegir cualquier valor para x3x_3x3â€‹. Una vez hecho esto, las fÃ³rmulas anteriores determinan los valores de x1x_1x1â€‹ y x2x_2x2â€‹. Por ejemplo, cuando x3=0x_3=0x3â€‹=0, la soluciÃ³n es (1,4,0)(1,4,0)(1,4,0); cuando x3=1x_3=1x3â€‹=1, la soluciÃ³n es (6,3,1)(6,3,1)(6,3,1). Cada asignaciÃ³n diferente de x3x_3x3â€‹ determina una soluciÃ³n (distinta) del sistema, y cada soluciÃ³n del sistema estÃ¡ determinada por una asignaciÃ³n de x3x_3x3â€‹.  ","version":"Next","tagName":"h3"},{"title":"Preguntas de existencia y unicidadâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#preguntas-de-existencia-y-unicidad","content":" Teorema de existencia y unicidad Un sistema lineal es consistente si y solo si la columna mÃ¡s a la derecha de la matriz aumentada no es una columna pivote, es decir, si y solo si una forma escalonada de la matriz aumentada no tiene filas del tipo [0â‹¯0b]conÂ bÂ diferenteÂ deÂ cero\\left[\\begin{array}{llll}0 &amp; \\cdots &amp; 0 &amp; b\\end{array}\\right] \\quad \\text{con } b \\text{ diferente de cero}[0â€‹â‹¯â€‹0â€‹bâ€‹]conÂ bÂ diferenteÂ deÂ cero Si un sistema lineal es consistente, entonces el conjunto soluciÃ³n contiene: i. una Ãºnica soluciÃ³n, cuando no existen variables libres, o ii. una infinidad de soluciones, cuando hay al menos una variable libre.  El siguiente procedimiento indica cÃ³mo encontrar y describir todas las soluciones de un sistema lineal.  Escriba la matriz aumentada del sistema.Emplee el algoritmo de reducciÃ³n por filas para obtener una matriz aumentada equivalente en forma escalonada. Determine si el sistema es consistente o no. Si no existe soluciÃ³n, detÃ©ngase; en caso contrario, continÃºe con el siguiente paso.Prosiga con la reducciÃ³n por filas para obtener la forma escalonada reducida.Escriba el sistema de ecuaciones correspondiente a la matriz obtenida en el paso 3.Rescriba cada ecuaciÃ³n no nula del paso 4 de manera que su Ãºnica variable bÃ¡sica se exprese en tÃ©rminos de cualquiera de las variables libres que aparecen en la ecuaciÃ³n.  ","version":"Next","tagName":"h3"},{"title":"Combinaciones linealesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#combinaciones-lineales","content":" Dados los vectores v1,v2,â€¦,vp\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_pv1â€‹,v2â€‹,â€¦,vpâ€‹ en Rn\\mathbb{R}^nRn y dados los escalares c1,c2,â€¦,cpc_1, c_2, \\ldots, c_pc1â€‹,c2â€‹,â€¦,cpâ€‹, el vector y\\mathbf{y}y definido por  y=c1v1+â‹¯+cpvp\\mathbf{y}=c_1 \\mathbf{v}_1+\\cdots+c_p \\mathbf{v}_py=c1â€‹v1â€‹+â‹¯+cpâ€‹vpâ€‹  se llama combinaciÃ³n lineal de v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹ con pesos c1,â€¦,cpc_1, \\ldots, c_pc1â€‹,â€¦,cpâ€‹.  La siguiente figura identifica combinaciones lineales seleccionadas de v1=[âˆ’11]\\mathbf{v}_1=\\left[\\begin{array}{r}-1 \\\\ 1\\end{array}\\right]v1â€‹=[âˆ’11â€‹]y v2=[21]\\mathbf{v}_2=\\left[\\begin{array}{l}2 \\\\ 1\\end{array}\\right]v2â€‹=[21â€‹].  Ejemplo de combinaciÃ³n lineal  ","version":"Next","tagName":"h3"},{"title":"Espacio generadoâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#espacio-generado","content":" Si v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹ estÃ¡n en Rn\\mathbb{R}^nRn, entonces el conjunto de todas las combinaciones lineales de v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹ se denota como Gen{v1,â€¦,vp}\\text{Gen}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}Gen{v1â€‹,â€¦,vpâ€‹} y se llama el subconjunto de Rn\\mathbb{R}^nRn extendido o generado por v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹. Es decir, Gen{v1,â€¦,vp}\\text{Gen}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}Gen{v1â€‹,â€¦,vpâ€‹} es el conjunto de todos los vectores que se pueden escribir en la forma  c1v1+c2v2+â‹¯+cpvpc_1 \\mathbf{v}_1+c_2 \\mathbf{v}_2+\\cdots+c_p \\mathbf{v}_pc1â€‹v1â€‹+c2â€‹v2â€‹+â‹¯+cpâ€‹vpâ€‹  con escalares c1,â€¦,cpc_1, \\ldots, c_pc1â€‹,â€¦,cpâ€‹.  Sea v\\mathbf{v}v un vector diferente de cero en R3\\mathbb{R}^3R3. Entonces Gen{v}\\text{Gen}\\{\\mathbf{v}\\}Gen{v} es el conjunto de todos los mÃºltiplos escalares de v\\mathbf{v}v, que es el conjunto de puntos sobre la recta en R3\\mathbb{R}^3R3 que pasa por v\\mathbf{v}v y 0\\mathbf{0}0.  Si u\\mathbf{u}u y v\\mathbf{v}v son vectores diferentes de cero en R3\\mathbb{R}^3R3, y v\\mathbf{v}v no es un mÃºltiplo de u\\mathbf{u}u, entonces Gen {u,v}\\{\\mathbf{u}, \\mathbf{v}\\}{u,v} es el plano en R3\\mathbb{R}^3R3 que contiene a u,v\\mathbf{u}, \\mathbf{v}u,v y 0\\mathbf{0}0. En particular, Gen {u,v}\\{\\mathbf{u}, \\mathbf{v}\\}{u,v} contiene la recta en R3\\mathbb{R}^3R3 que pasa por u\\mathbf{u}u y 0\\mathbf{0}0, y la recta que pasa por v\\mathbf{v}v y 0\\mathbf{0}0.  Espacio generado  ","version":"Next","tagName":"h3"},{"title":"EcuaciÃ³n matricialâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#ecuaciÃ³n-matricial","content":" Si AAA es una matriz de mÃ—nm \\times nmÃ—n, con columnas a1,â€¦,an\\mathbf{a}_1, \\ldots, \\mathbf{a}_na1â€‹,â€¦,anâ€‹, y si x\\mathbf{x}x estÃ¡ en Rn\\mathbb{R}^nRn, entonces el producto de AAA y x\\mathbf{x}x, denotado como AxA \\mathbf{x}Ax, es la combinaciÃ³n lineal de las columnas de AAA utilizando como pesos las entradas correspondientes en x\\mathbf{x}x; es decir,  Ax=[a1a2â‹¯an][x1â‹®xn]=x1a1+x2a2+â‹¯+xnanA \\mathbf{x}=\\left[\\begin{array}{llll} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n \\end{array}\\right]\\left[\\begin{array}{c} x_1 \\\\ \\vdots \\\\ x_n \\end{array}\\right]=x_1 \\mathbf{a}_1+x_2 \\mathbf{a}_2+\\cdots+x_n \\mathbf{a}_nAx=[a1â€‹â€‹a2â€‹â€‹â‹¯â€‹anâ€‹â€‹]â€‹x1â€‹â‹®xnâ€‹â€‹â€‹=x1â€‹a1â€‹+x2â€‹a2â€‹+â‹¯+xnâ€‹anâ€‹  Teorema Si AAA es una matriz de mÃ—nm \\times nmÃ—n, con columnas a1,â€¦,an\\mathbf{a}_1, \\ldots, \\mathbf{a}_na1â€‹,â€¦,anâ€‹, y si b\\mathbf{b}b estÃ¡ en Rm\\mathbb{R}^mRm, la ecuaciÃ³n matricial Ax=bA \\mathbf{x}=\\mathbf{b}Ax=b tiene el mismo conjunto soluciÃ³n que la ecuaciÃ³n vectorial x1a1+x2a2+â‹¯+xnan=bx_1 \\mathbf{a}_1+x_2 \\mathbf{a}_2+\\cdots+x_n \\mathbf{a}_n=\\mathbf{b}x1â€‹a1â€‹+x2â€‹a2â€‹+â‹¯+xnâ€‹anâ€‹=b la cual, a la vez, tiene el mismo conjunto soluciÃ³n que el sistema de ecuaciones lineales cuya matriz aumentada es [a1a2â‹¯anb]\\left[\\begin{array}{lllll} \\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n &amp; \\mathbf{b} \\end{array}\\right][a1â€‹â€‹a2â€‹â€‹â‹¯â€‹anâ€‹â€‹bâ€‹]  La definiciÃ³n de AxA \\mathbf{x}Ax conduce directamente al siguiente hecho que resulta Ãºtil:  Existencia de soluciÃ³n La ecuaciÃ³n Ax=bA \\mathbf{x}=\\mathbf{b}Ax=b tiene una soluciÃ³n si y solo si b es una combinaciÃ³n lineal de las columnas de AAA.  Dado lo anterior, podemos enunciar el siguiente teorema:  Teorema Sea AAA una matriz de mÃ—nm \\times nmÃ—n. Entonces, los siguientes enunciados son lÃ³gicamente equivalentes. Es decir, para una AAA particular, todos los enunciados son verdaderos o todos son falsos. Para cada b\\mathbf{b}b en Rm\\mathbb{R}^mRm, la ecuaciÃ³n Ax=bA \\mathbf{x}=\\mathbf{b}Ax=b tiene una soluciÃ³n.Cada b\\mathbf{b}b en Rm\\mathbb{R}^mRm es una combinaciÃ³n lineal de las columnas de AAA.Las columnas de AAA generan Rm\\mathbb{R}^mRm.AAA tiene una posiciÃ³n pivote en cada fila.  ","version":"Next","tagName":"h3"},{"title":"Sistemas lineales homogÃ©neosâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#sistemas-lineales-homogÃ©neos","content":" Se dice que un sistema de ecuaciones lineales es homogÃ©neo si se puede escribir en la forma Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0, donde AAA es una matriz de mÃ—nm \\times nmÃ—n, y 0\\mathbf{0}0 es el vector cero en Rm\\mathbb{R}^mRm. Tal sistema Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 siempre tiene al menos una soluciÃ³n, a saber, x=0\\mathbf{x}=\\mathbf{0}x=0 (el vector cero en Rn\\mathbb{R}^nRn ). Esta soluciÃ³n cero generalmente se conoce como soluciÃ³n trivial. Para una ecuaciÃ³n dada Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0, la pregunta importante es si existe una soluciÃ³n no trivial, es decir, un vector x\\mathbf{x}x diferente de cero que satisfaga Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0. El teorema de existencia y unicidad conduce de inmediato al siguiente resultado.  Resultado La ecuaciÃ³n homogÃ©nea Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 tiene una soluciÃ³n no trivial si y solo si la ecuaciÃ³n tiene al menos una variable libre.  ","version":"Next","tagName":"h3"},{"title":"Independencia linealâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#independencia-lineal","content":" Las ecuaciones homogÃ©neas se pueden estudiar desde una perspectiva diferente si las escribimos como ecuaciones vectoriales. De esta manera, la atenciÃ³n se transfiere de las soluciones desconocidas de Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 a los vectores que aparecen en las ecuaciones vectoriales.  Por ejemplo, considere la ecuaciÃ³n  x1[123]+x2[456]+x3[210]=[000]x_1\\left[\\begin{array}{l} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]+x_2\\left[\\begin{array}{l} 4 \\\\ 5 \\\\ 6 \\end{array}\\right]+x_3\\left[\\begin{array}{l} 2 \\\\ 1 \\\\ 0 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\\\ 0 \\\\ 0 \\end{array}\\right]x1â€‹â€‹123â€‹â€‹+x2â€‹â€‹456â€‹â€‹+x3â€‹â€‹210â€‹â€‹=â€‹000â€‹â€‹  Esta ecuaciÃ³n tiene una soluciÃ³n trivial, desde luego, donde x1=x2=x3=0x_1=x_2=x_3=0x1â€‹=x2â€‹=x3â€‹=0. Al igual que hemos discutido antes, el asunto principal es si la soluciÃ³n trivial es la Ãºnica.  DefiniciÃ³n Se dice que un conjunto indexado de vectores {v1,â€¦,vp}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}{v1â€‹,â€¦,vpâ€‹} en Rn\\mathbb{R}^nRn es linealmente independiente si la ecuaciÃ³n vectorial x1v1+x2v2+â‹¯+xpvp=0x_1 \\mathbf{v}_1+x_2 \\mathbf{v}_2+\\cdots+x_p \\mathbf{v}_p=\\mathbf{0}x1â€‹v1â€‹+x2â€‹v2â€‹+â‹¯+xpâ€‹vpâ€‹=0 solo tiene la soluciÃ³n trivial. Se dice que el conjunto {v1,â€¦,vp}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}{v1â€‹,â€¦,vpâ€‹} es linealmente dependiente si existen pesos c1,â€¦,cpc_1, \\ldots, c_pc1â€‹,â€¦,cpâ€‹, no todos cero, tales que c1v1+c2v2+â‹¯+cpvp=0c_1 \\mathbf{v}_1+c_2 \\mathbf{v}_2+\\cdots+c_p \\mathbf{v}_p=\\mathbf{0}c1â€‹v1â€‹+c2â€‹v2â€‹+â‹¯+cpâ€‹vpâ€‹=0  La ecuaciÃ³n que define el concepto de &quot;linealmente dependiente&quot; se llama relaciÃ³n de dependencia lineal entre v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹ cuando no todos los pesos son cero. Un conjunto indexado es linealmente dependiente si y solo si no es linealmente independiente. Por brevedad, puede decirse que v1,â€¦,vp\\mathbf{v}_1, \\ldots, \\mathbf{v}_pv1â€‹,â€¦,vpâ€‹ son linealmente dependientes cuando queremos decir que {v1,â€¦,vp}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}{v1â€‹,â€¦,vpâ€‹} es un conjunto linealmente dependiente. Se utiliza una terminologÃ­a semejante para los conjuntos linealmente independientes.  ","version":"Next","tagName":"h3"},{"title":"Independencia lineal en columnas de una matrizâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#independencia-lineal-en-columnas-de-una-matriz","content":" Suponga que, en vez de utilizar un conjunto de vectores, se inicia con una matriz A=[a1â‹¯an]A=\\left[\\begin{array}{lll}\\mathbf{a}_1 &amp; \\cdots &amp; \\mathbf{a}_n\\end{array}\\right]A=[a1â€‹â€‹â‹¯â€‹anâ€‹â€‹]. En tal caso, la ecuaciÃ³n matricial Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 se puede escribir como  x1a1+x2a2+â‹¯+xnan=0x_1 \\mathbf{a}_1+x_2 \\mathbf{a}_2+\\cdots+x_n \\mathbf{a}_n=\\mathbf{0}x1â€‹a1â€‹+x2â€‹a2â€‹+â‹¯+xnâ€‹anâ€‹=0  Cada relaciÃ³n de dependencia lineal entre las columnas de A corresponde a una soluciÃ³n no trivial de Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0. AsÃ­, tenemos el siguiente resultado importante.  Teorema Las columnas de una matriz AAA son linealmente independientes si y solo si la ecuaciÃ³n Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 tiene solo la soluciÃ³n trivial.  ","version":"Next","tagName":"h3"},{"title":"GeometrÃ­a de la dependencia linealâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#geometrÃ­a-de-la-dependencia-lineal","content":" Siempre es posible determinar por inspecciÃ³n cuÃ¡ndo un conjunto de dos vectores es linealmente dependiente. Las operaciones de fila son innecesarias. Basta con comprobar si al menos uno de los vectores es un escalar multiplicado por el otro. (La prueba solo se aplica a conjuntos de dos vectores).  Dependencia lineal entre dos vectores Un conjunto de dos vectores {v1,v2}\\left\\{\\mathbf{v}_1, \\mathbf{v}_2\\right\\}{v1â€‹,v2â€‹} es linealmente dependiente si al menos uno de los vectores es un mÃºltiplo del otro. El conjunto es linealmente independiente si y solo si ninguno de los vectores es un mÃºltiplo del otro.  Dependencia lineal  En tÃ©rminos geomÃ©tricos, dos vectores son linealmente dependientes si y solo si ambos estÃ¡n sobre la misma recta que pasa por el origen.  Para dos o mÃ¡s vectores, tenemos el siguiente resultado:  CaracterizaciÃ³n de conjuntos linealmente dependientes Un conjunto indexado S={v1,â€¦,vp}S=\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}S={v1â€‹,â€¦,vpâ€‹} de dos o mÃ¡s vectores es linealmente dependiente si y solo si al menos uno de los vectores en SSS es una combinaciÃ³n lineal de los otros. De hecho, si SSS es linealmente dependiente y v1â‰ 0\\mathbf{v}_1 \\neq \\mathbf{0}v1â€‹î€ =0, entonces alguna vj(conâ¡j&gt;1\\mathbf{v}_j(\\operatorname{con} j&gt;1vjâ€‹(conj&gt;1 ) es una combinaciÃ³n lineal de los vectores precedentes, v1,â€¦,vjâˆ’1\\mathbf{v}_1, \\ldots, \\mathbf{v}_{j-1}v1â€‹,â€¦,vjâˆ’1â€‹.  Los siguientes dos teoremas describen casos especiales en los cuales la dependencia lineal de un conjunto es automÃ¡tica.  Teorema Si un conjunto contiene mÃ¡s vectores que entradas en cada vector, entonces el conjunto es linealmente dependiente. Es decir, cualquier conjunto {v1,â€¦,vp}\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}{v1â€‹,â€¦,vpâ€‹} en Rn\\mathbb{R}^nRn es linealmente dependiente si p&gt;np&gt;np&gt;n.  Teorema Si un conjunto S={v1,â€¦,vp}S=\\left\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_p\\right\\}S={v1â€‹,â€¦,vpâ€‹} en Rn\\mathbb{R}^nRn contiene al vector cero, entonces el conjunto es linealmente dependiente.  ","version":"Next","tagName":"h3"},{"title":"Transformaciones linealesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#transformaciones-lineales","content":" La correspondencia de x\\mathbf{x}x a AxA\\mathbf{x}Ax es una funciÃ³n de un conjunto de vectores a otro. Este concepto generaliza la nociÃ³n comÃºn de una funciÃ³n como una regla que transforma un nÃºmero real en otro.  Una transformaciÃ³n (o funciÃ³n o mapeo) TTT de Rn\\mathbb{R}^nRn a Rm\\mathbb{R}^mRm es una regla que asigna a cada vector x\\mathbf{x}x en Rn\\mathbb{R}^nRn un vector T(x)T(\\mathbf{x})T(x) en Rm\\mathbb{R}^mRm. El conjunto Rn\\mathbb{R}^nRn se llama el dominio de TTT, y Rm\\mathbb{R}^mRm se llama el codominio de TTT. La notaciÃ³n T:Rnâ†’RmT: \\mathbb{R}^n \\rightarrow \\mathbb{R}^mT:Rnâ†’Rm indica que el dominio de TTT es Rn\\mathbb{R}^nRn y que el codominio es Rm\\mathbb{R}^mRm. Para x\\mathbf{x}x en Rn\\mathbb{R}^nRn, el vector T(x)T(\\mathbf{x})T(x) en Rm\\mathbb{R}^mRm es la imagen de x\\mathbf{x}x (bajo la acciÃ³n de TTT ). El conjunto de todas las imÃ¡genes T(x)T(\\mathbf{x})T(x) es el rango de TTT.    Para cada x\\mathbf{x}x en Rn,T(x)\\mathbb{R}^n, T(\\mathbf{x})Rn,T(x) se calcula como AxA \\mathbf{x}Ax, donde AAA es una matriz de mÃ—nm \\times nmÃ—n. Para simplificar, algunas veces esta transformaciÃ³n matricial se denota como xâ†¦Ax\\mathbf{x} \\mapsto A \\mathbf{x}xâ†¦Ax. Observe que el dominio de TTT es Rn\\mathbb{R}^nRn cuando AAA tiene nnn columnas, y el codominio de TTT es Rm\\mathbb{R}^mRm cuando cada columna de AAA tiene mmm entradas. El rango de TTT es el conjunto de todas las combinaciones lineales de las columnas de AAA, porque cada imagen T(x)T(\\mathbf{x})T(x) es de la forma AxA \\mathbf{x}Ax.  DefiniciÃ³n Una transformaciÃ³n (o mapeo) TTT es lineal si: T(u+v)=T(u)+T(v)T(\\mathbf{u}+\\mathbf{v})=T(\\mathbf{u})+T(\\mathbf{v})T(u+v)=T(u)+T(v) para todas las u\\mathbf{u}u, v\\mathbf{v}v en el dominio de TTT;T(cu)=cT(u)T(c \\mathbf{u})=c T(\\mathbf{u})T(cu)=cT(u) para todos los escalares ccc y para todas las u\\mathbf{u}u en el dominio de TTT.  ","version":"Next","tagName":"h3"},{"title":"Ãlgebra de matricesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#Ã¡lgebra-de-matrices","content":" ","version":"Next","tagName":"h2"},{"title":"Operaciones bÃ¡sicasâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#operaciones-bÃ¡sicas","content":" Sean A,BA, BA,B y CCC matrices del mismo tamaÃ±o, y sean rrr y sss escalares.  A+B=B+AA+B=B+AA+B=B+Ar(A+B)=rA+rBr(A+B)=r A+r Br(A+B)=rA+rB(A+B)+C=A+(B+C)(A+B)+C=A+(B+C)(A+B)+C=A+(B+C)(r+s)A=rA+sA(r+s) A=r A+s A(r+s)A=rA+sAA+0=AA+0=AA+0=Ar(sA)=(rs)Ar(s A)=(r s) Ar(sA)=(rs)AAk=Aâ‹¯AâŸkA^k=\\underbrace{A \\cdots A}_kAk=kAâ‹¯Aâ€‹â€‹  ","version":"Next","tagName":"h3"},{"title":"MultiplicaciÃ³n de matricesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#multiplicaciÃ³n-de-matrices","content":" Si AAA es una matriz de mÃ—nm \\times nmÃ—n, y si BBB es una matriz de nÃ—pn \\times pnÃ—p con columnas b1,â€¦,bp\\mathbf{b}_1, \\ldots, \\mathbf{b}_pb1â€‹,â€¦,bpâ€‹ entonces el producto ABA BAB es la matriz de mÃ—pm \\times pmÃ—p cuyas columnas son Ab1,â€¦,AbpA \\mathbf{b}_1, \\ldots, A \\mathbf{b}_pAb1â€‹,â€¦,Abpâ€‹. Es decir,  AB=A[b1b2â‹¯bp]=[Ab1Ab2â‹¯Abp]A B=A\\left[\\begin{array}{llll} \\mathbf{b}_1 &amp; \\mathbf{b}_2 &amp; \\cdots &amp; \\mathbf{b}_p \\end{array}\\right]=\\left[\\begin{array}{llll} A \\mathbf{b}_1 &amp; A \\mathbf{b}_2 &amp; \\cdots &amp; A \\mathbf{b}_p \\end{array}\\right]AB=A[b1â€‹â€‹b2â€‹â€‹â‹¯â€‹bpâ€‹â€‹]=[Ab1â€‹â€‹Ab2â€‹â€‹â‹¯â€‹Abpâ€‹â€‹]  Es decir, cada columna de ABA BAB es una combinaciÃ³n lineal de las columnas de AAA usando pesos de la columna correspondiente de BBB.  Regla fila-columna para calcular AB Si el producto ABA BAB estÃ¡ definido, entonces la entrada en la fila iii y la columna jjj de ABA BAB es la suma de los productos de las entradas correspondientes de la fila iii de AAA y la columna jjj de BBB. Si (AB)ij(A B)_{i j}(AB)ijâ€‹ denota la entrada (i,j)(i, j)(i,j) en ABA BAB, y si AAA es una matriz de mÃ—nm \\times nmÃ—n, entonces (AB)ij=ai1b1j+ai2b2j+â‹¯+ainbnj(A B)_{i j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\cdots+a_{i n} b_{n j}(AB)ijâ€‹=ai1â€‹b1jâ€‹+ai2â€‹b2jâ€‹+â‹¯+ainâ€‹bnjâ€‹  Propiedades de la multiplicaciÃ³n de matrices Sea AAA una matriz de mÃ—nm \\times nmÃ—n, y sean BBB y CCC matrices con tamaÃ±os para los que las sumas y los productos indicados estÃ¡n definidos. A(BC)=(AB)CA(B C)=(A B) CA(BC)=(AB)CA(B+C)=AB+ACA(B+C)=A B+A CA(B+C)=AB+AC(B+C)A=BA+CA(B+C) A=B A+C A(B+C)A=BA+CAr(AB)=(rA)B=A(rB)r(A B)=(r A) B=A(r B)r(AB)=(rA)B=A(rB) para cualquier escalar rrrImA=A=AInI_m A=A=A I_nImâ€‹A=A=AInâ€‹  Advertencias En general, ABâ‰ BAA B \\neq B AABî€ =BA.Las leyes de la cancelaciÃ³n no se aplican en la multiplicaciÃ³n de matrices. Es decir, si AB=ACA B=A CAB=AC, en general no es cierto que B=CB=CB=C.Si un producto ABA BAB es la matriz cero, en general, no se puede concluir que A=0A=0A=0 o B=0B=0B=0.  ","version":"Next","tagName":"h3"},{"title":"La transpuesta de una matrizâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#la-transpuesta-de-una-matriz","content":" Dada una matriz AAA de mÃ—nm \\times nmÃ—n, la transpuesta de AAA es la matriz de nÃ—mn \\times mnÃ—m, que se denota con ATA^TAT, cuyas columnas se forman a partir de las filas correspondientes de AAA.  Teorema Sean AAA y BBB matrices cuyos tamaÃ±os son adecuados para las siguientes sumas y productos. (AT)T=A\\left(A^T\\right)^T=A(AT)T=A(A+B)T=AT+BT(A+B)^T=A^T+B^T(A+B)T=AT+BTPara cualquier escalar r,(rA)T=rATr,(r A)^T=r A^Tr,(rA)T=rAT(AB)T=BTAT(A B)^T=B^T A^T(AB)T=BTAT  ","version":"Next","tagName":"h3"},{"title":"Inversa de una matrizâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#inversa-de-una-matriz","content":" Se dice que una matriz AAA de nÃ—nn \\times nnÃ—n es invertible si existe otra matriz Aâˆ’1A^{-1}Aâˆ’1 de nÃ—nn \\times nnÃ—n tal que  Aâˆ’1A=IÂ yÂ AAâˆ’1=IA^{-1} A=I \\quad \\text { y } \\quad A A^{-1}=IAâˆ’1A=IÂ yÂ AAâˆ’1=I  donde I=InI=I_nI=Inâ€‹, la matriz identidad de nÃ—nn \\times nnÃ—n. En este caso, Aâˆ’1A^{-1}Aâˆ’1 es una inversa de AAA.  Teorema Sea A=[abcd]A=\\left[\\begin{array}{ll}a &amp; b \\\\ c &amp; d\\end{array}\\right]A=[acâ€‹bdâ€‹]. Si adâˆ’bcâ‰ 0a d-b c \\neq 0adâˆ’bcî€ =0, entonces AAA es invertible y Aâˆ’1=1det(A)[dâˆ’bâˆ’ca]A^{-1}=\\frac{1}{\\text{det}(A)}\\left[\\begin{array}{rr} d &amp; -b \\\\ -c &amp; a \\end{array}\\right]Aâˆ’1=det(A)1â€‹[dâˆ’câ€‹âˆ’baâ€‹] Si det(A)=adâˆ’bc=0\\text{det}(A)=a d-b c=0det(A)=adâˆ’bc=0, entonces AAA no es invertible.  La definiciÃ³n de matriz inversa nos entrega el siguiente teorema:  Teorema Si AAA es una matriz invertible de nÃ—nn \\times nnÃ—n, entonces, para cada b\\mathbf{b}b en Rn\\mathbb{R}^nRn, la ecuaciÃ³n Ax=bA \\mathbf{x}=\\boldsymbol{b}Ax=b tiene la soluciÃ³n Ãºnica x=Aâˆ’1b\\mathbf{x}=A^{-1} \\mathbf{b}x=Aâˆ’1b.  AdemÃ¡s, podemos definir las siguientes propiedades para matrices invertibles:  Si AAA es una matriz invertible, entonces Aâˆ’1A^{-1}Aâˆ’1 es invertible y(Aâˆ’1)âˆ’1=A\\left(A^{-1}\\right)^{-1}=A(Aâˆ’1)âˆ’1=ASi AAA y BBB son matrices invertibles de nÃ—nn \\times nnÃ—n, entonces tambiÃ©n lo es ABA BAB, y la inversa de ABA BAB es el producto de las inversas de AAA y BBB en el orden opuesto. Es decir,(AB)âˆ’1=Bâˆ’1Aâˆ’1(A B)^{-1}=B^{-1} A^{-1}(AB)âˆ’1=Bâˆ’1Aâˆ’1Si AAA es una matriz invertible, tambiÃ©n lo es ATA^TAT, y la inversa de ATA^TAT es la transpuesta de Aâˆ’1A^{-1}Aâˆ’1. Es decir,(AT)âˆ’1=(Aâˆ’1)T\\left(A^T\\right)^{-1}=\\left(A^{-1}\\right)^T(AT)âˆ’1=(Aâˆ’1)T  ","version":"Next","tagName":"h3"},{"title":"Matrices elementalesâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#matrices-elementales","content":" Una matriz elemental es aquella que se obtiene al realizar una Ãºnica operaciÃ³n elemental de fila sobre una matriz identidad. El siguiente ejemplo ilustra los tres tipos de matrices elementales.  Si se realiza una operaciÃ³n elemental de fila con una matriz AAA de mÃ—nm \\times nmÃ—n, la matriz resultante se puede escribir como EAE AEA, donde la matriz EEE de mÃ—mm \\times mmÃ—m se crea al realizar la misma operaciÃ³n de fila sobre ImI_mImâ€‹.  Por ejemplo, si    entonces    Al sumar a la fila 3 la fila 1 de AAA multiplicada por âˆ’4-4âˆ’4 , se obtiene E1AE_1 AE1â€‹A. (Esta es una operaciÃ³n de remplazo de filas). Con un intercambio de las filas 1 y 2 de AAA se obtiene E2AE_2 AE2â€‹A, y multiplicando la fila 3 de AAA por 555 se obtiene E3AE_3 AE3â€‹A.  Teorema Toda matriz elemental EEE es invertible. La inversa de EEE es la matriz elemental del mismo tipo que transforma a EEE de nuevo en III. Una matriz AAA de nÃ—nn \\times nnÃ—n es invertible si y solo si AAA es equivalente por filas a InI_nInâ€‹, y, en este caso, cualquier secuencia de operaciones elementales de fila que reduzca AAA a InI_nInâ€‹ tambiÃ©n transforma a InI_nInâ€‹ en Aâˆ’1A^{-1}Aâˆ’1.  El teorema anterior nos da el siguiente algoritmo para determinar la inversa de una matriz AAA de nÃ—nn \\times nnÃ—n:  Algoritmo para determinar la inversa de una matriz Reduzca por filas la matriz aumentada [AI]\\left[\\begin{array}{ll}A &amp; I\\end{array}\\right][Aâ€‹Iâ€‹]. Si AAA es equivalente por filas a III, entonces [AI]\\left[\\begin{array}{ll}A &amp; I\\end{array}\\right][Aâ€‹Iâ€‹] es equivalente por filas a [IAâˆ’1]\\left[\\begin{array}{ll}I &amp; A^{-1}\\end{array}\\right][Iâ€‹Aâˆ’1â€‹]. De otra manera, AAA no tiene inversa.  ","version":"Next","tagName":"h3"},{"title":"El teorema de la matriz invertibleâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#el-teorema-de-la-matriz-invertible","content":" El resultado principal de todo el estudio anterior es el siguiente teorema.  Teorema Sea AAA una matriz cuadrada de nÃ—nn \\times nnÃ—n. Entonces, los siguientes enunciados son equivalentes. Es decir, para una AAA dada, los enunciados son todos ciertos o todos falsos. AAA es una matriz invertible.AAA es equivalente por filas a la matriz identidad de nÃ—nn \\times nnÃ—n.AAA tiene nnn posiciones pivote.La ecuaciÃ³n Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 tiene solamente la soluciÃ³n trivial.Las columnas de AAA forman un conjunto linealmente independiente.La transformaciÃ³n lineal xâ†¦Ax\\mathbf{x} \\mapsto A \\mathbf{x}xâ†¦Ax es uno a uno.La ecuaciÃ³n Ax=bA \\mathbf{x}=\\mathbf{b}Ax=b tiene al menos una soluciÃ³n para toda b\\mathbf{b}b en Rn\\mathbb{R}^nRn.Las columnas de AAA generan Rn\\mathbb{R}^nRn.La transformaciÃ³n lineal xâ†¦Ax\\mathbf{x} \\mapsto A \\mathbf{x}xâ†¦Ax mapea Rn\\mathbb{R}^nRn sobre Rn\\mathbb{R}^nRn.Existe una matriz CCC de nÃ—nn \\times nnÃ—n tal que CA=IC A=ICA=I,Existe una matriz DDD de nÃ—nn \\times nnÃ—n tal que AD=IA D=IAD=I.ATA^TAT es una matriz invertible.  ","version":"Next","tagName":"h3"},{"title":"TransformaciÃ³n lineal invertibleâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#transformaciÃ³n-lineal-invertible","content":" Se dice que una transformaciÃ³n lineal T:Rnâ†’RnT: \\mathbb{R}^n \\rightarrow \\mathbb{R}^nT:Rnâ†’Rn es invertible si existe una funciÃ³n S:Rnâ†’RnS: \\mathbb{R}^n \\rightarrow \\mathbb{R}^nS:Rnâ†’Rn tal que  S(T(x))=xÂ paraÂ todaÂ xÂ enÂ RnT(S(x))=xÂ paraÂ todaÂ xÂ enÂ Rn\\begin{array}{ll} S(T(\\mathbf{x}))=\\mathbf{x} &amp; \\text { para toda } \\mathbf{x} \\text { en } \\mathbb{R}^n \\\\ T(S(\\mathbf{x}))=\\mathbf{x} &amp; \\text { para toda } \\mathbf{x} \\text { en } \\mathbb{R}^n \\end{array}S(T(x))=xT(S(x))=xâ€‹Â paraÂ todaÂ xÂ enÂ RnÂ paraÂ todaÂ xÂ enÂ Rnâ€‹  El siguiente teorema establece que si dicha SSS existe, es Ãºnica y debe ser una transformaciÃ³n lineal. Se dice que SSS es la inversa de TTT y se escribe como Tâˆ’1T^{-1}Tâˆ’1.  Teorema Sea T:Rnâ†’RnT: \\mathbb{R}^n \\rightarrow \\mathbb{R}^nT:Rnâ†’Rn una transformaciÃ³n lineal y sea AAA la matriz estÃ¡ndar para TTT. AsÃ­, TTT es invertible si y solo si AAA es una matriz invertible. En tal caso, la transformaciÃ³n lineal SSS dada por S(x)=Aâˆ’1xS(\\mathbf{x})=A^{-1} \\mathbf{x}S(x)=Aâˆ’1x es la Ãºnica funciÃ³n que satisface las ecuaciones mostradas.  ","version":"Next","tagName":"h3"},{"title":"Subespaciosâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#subespacios","content":" Un subespacio de Rn\\mathbb{R}^nRn es cualquier conjunto HHH en Rn\\mathbb{R}^nRn que tenga tres propiedades:  El vector cero estÃ¡ en HHH.Para cada u\\mathbf{u}u y v\\mathbf{v}v en HHH, la suma u+v\\mathbf{u}+\\mathbf{v}u+v estÃ¡ en HHH.Para cada u\\mathbf{u}u en HHH y cada escalar ccc, el vector cuc \\mathbf{u}cu estÃ¡ en HHH.  Dicho con palabras, un subespacio es cerrado bajo la suma y la multiplicaciÃ³n escalar.    ","version":"Next","tagName":"h3"},{"title":"Espacio de columnasâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#espacio-de-columnas","content":" Los subespacios de Rn\\mathbb{R}^nRn generalmente se presentan en aplicaciones y en la teorÃ­a en una de dos formas. En ambos casos, es posible relacionar el subespacio con una matriz.  DefiniciÃ³n El espacio de columnas de una matriz AAA es el conjunto ColA\\mathrm{Col} AColA de todas las combinaciones de las columnas de AAA.  Si A=[a1â‹¯an]A=\\left[\\begin{array}{lll}\\mathbf{a}_1 &amp; \\cdots &amp; \\mathbf{a}_n\\end{array}\\right]A=[a1â€‹â€‹â‹¯â€‹anâ€‹â€‹], con las columnas en Rm\\mathbb{R}^mRm, entonces ColA\\text{Col} AColA es lo mismo que Gen{a1,â€¦,an}\\text{Gen}\\left\\{\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\right\\}Gen{a1â€‹,â€¦,anâ€‹}. De hecho, el espacio columna de una matriz de mÃ—n\\boldsymbol{m} \\times \\boldsymbol{n}mÃ—n es un subespacio de Rm\\mathbb{R}^mRm. Observe que ColA\\text{Col} AColA es igual a Rm\\mathbb{R}^mRm solo cuando las columnas de AAA generan a Rm\\mathbb{R}^mRm. Si no la generan, ColA\\text{Col}AColA es solo una parte de Rm\\mathbb{R}^mRm.  Cuando un sistema de ecuaciones lineales estÃ¡ escrito en la forma Ax=bA \\mathbf{x}=\\mathbf{b}Ax=b, el espacio columna de AAA es el conjunto de todas las b\\mathbf{b}b para las que el sistema tiene una soluciÃ³n.  ","version":"Next","tagName":"h3"},{"title":"Espacio nuloâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#espacio-nulo","content":" El espacio nulo de una matriz AAA es el conjunto NulA\\mathrm{Nul} ANulA de todas las soluciones posibles para la ecuaciÃ³n homogÃ©nea Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0.  Cuando AAA tiene nnn columnas, las soluciones de Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 pertenecen a Rn\\mathbb{R}^nRn, y el espacio nulo de AAA es un subconjunto de Rn\\mathbb{R}^nRn. De hecho, Nul AAA tiene las propiedades de un subespacio de matrices de Rn\\mathbb{R}^nRn.  Teorema El espacio nulo de una matriz AAA de mÃ—nm \\times nmÃ—n es un subespacio de Rn\\mathbb{R}^nRn. De manera equivalente, el conjunto de todas las soluciones posibles para un sistema Ax=0A \\mathbf{x}=\\mathbf{0}Ax=0 de mmm ecuaciones lineales homogÃ©neas con nnn incÃ³gnitas es un subespacio de Rn\\mathbb{R}^nRn.  ","version":"Next","tagName":"h3"},{"title":"Base para un subespacioâ€‹","type":1,"pageTitle":"Ãlgebra lineal","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/algebra_lineal#base-para-un-subespacio","content":" Como, por lo general, un subespacio contiene un nÃºmero infinito de vectores, algunos problemas relacionados con subespacios se manejan mejor trabajando con un conjunto finito y pequeÃ±o de vectores que genere el subespacio. Cuanto menor sea el conjunto, serÃ¡ mejor. Es factible demostrar que el conjunto generador mÃ¡s pequeÃ±o posible debe ser linealmente independiente.  DefiniciÃ³n Una base de un subespacio HHH de Rn\\mathbb{R}^nRn es un conjunto linealmente independiente en HHH, que genera a HHH.  La base estÃ¡ndar para todo el espacio  warning Solo son 4 preguntas de Lineal, despuÃ©s me sigo pegando el show terminando esto ğŸ˜¢ ","version":"Next","tagName":"h3"},{"title":"Modelos analÃ­ticos de fenÃ³menos aleatorios","type":0,"sectionRef":"#","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos","content":"","keywords":"","version":"Next"},{"title":"Variables y distribucionesâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#variables-y-distribuciones","content":" ","version":"Next","tagName":"h2"},{"title":"Variables aleatoriasâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#variables-aleatorias","content":" Una variable aleatoria es el vehÃ­culo matemÃ¡tico para representar un evento en tÃ©rminos analÃ­ticos. El valor de una variable aleatoria puede estar definida para un conjunto de posibles valores.  Si XXX es una variable aleatoria, entonces  X=x,X&lt;x,X&gt;xX=x, \\quad X&lt;x, \\quad X&gt;xX=x,X&lt;x,X&gt;x  representa un evento, donde (a&lt;X&lt;b)(a&lt;X&lt;b)(a&lt;X&lt;b) es el rango de valores posibles de XXX. La asignaciÃ³n numÃ©rica puede ser natural o artificial.  Formalmente, una variable aleatoria puede ser considerada como una funciÃ³n o regla sobre los eventos del espacio muestral a un sistema numÃ©rico (o lÃ­nea real).    AsÃ­, los eventos E1E_1E1â€‹ y E2E_2E2â€‹ pueden corresponder a  E1=(a&lt;Xâ‰¤b)E2=(c&lt;Xâ‰¤d)E1âˆªE2â€¾=(Xâ‰¤a)âˆª(X&gt;d)E1âˆ©E2=(c&lt;Xâ‰¤b)\\begin{aligned} E_1 &amp; =(a&lt;X \\leq b) \\\\ E_2 &amp; =(c&lt;X \\leq d) \\\\ \\overline{E_1 \\cup E_2} &amp; =(X \\leq a) \\cup(X&gt;d) \\\\ E_1 \\cap E_2 &amp; =(c&lt;X \\leq b) \\end{aligned}E1â€‹E2â€‹E1â€‹âˆªE2â€‹â€‹E1â€‹âˆ©E2â€‹â€‹=(a&lt;Xâ‰¤b)=(c&lt;Xâ‰¤d)=(Xâ‰¤a)âˆª(X&gt;d)=(c&lt;Xâ‰¤b)â€‹  Una variable aleatoria puede ser discreta o continua.  ","version":"Next","tagName":"h3"},{"title":"Distribuciones de probabilidadâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#distribuciones-de-probabilidad","content":" Para los valores o rango de valores que puede tomar una variable aleatoria tienen asociados una probabilidad especifica o medidas de probabilidad. La regla que asigna las medidas de probabilidad se denomina distribuciÃ³n o ley de probabilidad.  Si XXX es variable aleatoria, la distribuciÃ³n de probabilidad puede ser descrita por su funciÃ³n de distribuciÃ³n de probabilidad acumulada denotada por:  FX(x)=P(Xâ‰¤x)Â paraÂ todoÂ xâˆˆRF_X(x)=P(X \\leq x) \\text { para todo } x \\in \\mathbb{R}FXâ€‹(x)=P(Xâ‰¤x)Â paraÂ todoÂ xâˆˆR  Si XXX es una variable aleatoria discreta, entonces esta funciÃ³n puede ser expresada a travÃ©s de la funciÃ³n de probabilidad &quot;puntual&quot; denotada por  pX(x)=P(X=x)p_X(x)=P(X=x)pXâ€‹(x)=P(X=x)  AsÃ­,  FX(x)=âˆ‘xiâ‰¤xP(X=xi)=âˆ‘xiâ‰¤xpX(xi)F_X(x)=\\sum_{x_i \\leq x} P\\left(X=x_i\\right)=\\sum_{x_i \\leq x} p_X\\left(x_i\\right)FXâ€‹(x)=xiâ€‹â‰¤xâˆ‘â€‹P(X=xiâ€‹)=xiâ€‹â‰¤xâˆ‘â€‹pXâ€‹(xiâ€‹)  con xiâˆˆÎ˜Xx_i \\in \\Theta_Xxiâ€‹âˆˆÎ˜Xâ€‹ (soporte de X)\\left.X\\right)X).  Ahora, si XXX es una variable aleatoria continua, las probabilidades estÃ¡n asociadas a intervalos de xxx. En este caso se define la funciÃ³n de densidad fX(x)f_X(x)fXâ€‹(x) tal que  P(a&lt;Xâ‰¤b)=âˆ«abfX(x)dxP(a&lt;X \\leq b)=\\int_a^b f_X(x) d xP(a&lt;Xâ‰¤b)=âˆ«abâ€‹fXâ€‹(x)dx  y  FX(x)=P(Xâ‰¤x)=âˆ«âˆ’âˆxfX(t)dtF_X(x)=P(X \\leq x)=\\int_{-\\infty}^x f_X(t) d tFXâ€‹(x)=P(Xâ‰¤x)=âˆ«âˆ’âˆxâ€‹fXâ€‹(t)dt  con  fX(x)=ddxFX(x)f_X(x)=\\frac{d}{d x} F_X(x)fXâ€‹(x)=dxdâ€‹FXâ€‹(x)  Notar que  P(x&lt;Xâ‰¤x+dx)=fX(x)dxP(x&lt;X \\leq x+d x)=f_X(x) d xP(x&lt;Xâ‰¤x+dx)=fXâ€‹(x)dx  Caso discreto y continuo  Caso mixto  ","version":"Next","tagName":"h3"},{"title":"Propiedadesâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#propiedades","content":" FX(âˆ’âˆ)=0F_X(-\\infty)=0FXâ€‹(âˆ’âˆ)=0 y FX(âˆ)=1F_X(\\infty)=1FXâ€‹(âˆ)=1.FX(x)â‰¥0F_X(x) \\geq 0FXâ€‹(x)â‰¥0 para todo valor de xxx y es no decreciente.FX(x)F_X(x)FXâ€‹(x) es continua por la derecha  Para el caso continuo, la ecuaciÃ³n la podemos escribir como  P(a&lt;Xâ‰¤b)=âˆ«âˆ’âˆbfX(x)dxâˆ’âˆ«âˆ’âˆafX(x)dxP(a&lt;X \\leq b)=\\int_{-\\infty}^b f_X(x) d x-\\int_{-\\infty}^a f_X(x) d xP(a&lt;Xâ‰¤b)=âˆ«âˆ’âˆbâ€‹fXâ€‹(x)dxâˆ’âˆ«âˆ’âˆaâ€‹fXâ€‹(x)dx  mientras que en el caso discreto  P(a&lt;Xâ‰¤b)=âˆ‘xiâ‰¤bpX(xi)âˆ’âˆ‘xiâ‰¤apX(xi)P(a&lt;X \\leq b)=\\sum_{x_i \\leq b} p_X\\left(x_i\\right)-\\sum_{x_i \\leq a} p_X\\left(x_i\\right)P(a&lt;Xâ‰¤b)=xiâ€‹â‰¤bâˆ‘â€‹pXâ€‹(xiâ€‹)âˆ’xiâ€‹â‰¤aâˆ‘â€‹pXâ€‹(xiâ€‹)  es decir, para ambos casos  P(a&lt;Xâ‰¤b)=FX(b)âˆ’FX(a)P(a&lt;X \\leq b)=F_X(b)-F_X(a)P(a&lt;Xâ‰¤b)=FXâ€‹(b)âˆ’FXâ€‹(a)  ","version":"Next","tagName":"h3"},{"title":"Medidas descriptivas de una variable aleatoriaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-descriptivas-de-una-variable-aleatoria","content":" Una variable aleatoria puede ser descrita totalmente por su funciÃ³n de distribuciÃ³n de probabilidad o de densidad, o bien por su funciÃ³n de distribuciÃ³n de probabilidad acumulada. Sin embargo, en la prÃ¡ctica la forma exacta puede no ser totalmente conocida.  En tales casos se requieren ciertas &quot;medidas&quot; para tener una idea de la forma de la distribuciÃ³n.  ","version":"Next","tagName":"h2"},{"title":"Medidas centralesâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-centrales","content":" En el rango de posibles valores de una variable aleatoria, existe un interÃ©s natural con respecto a los valores centrales, por ejemplo, el promedio.  Consideremos una variable aleatoria XXX con soporte Î˜X\\Theta_XÎ˜Xâ€‹. Como cada valor de Î˜X\\Theta_XÎ˜Xâ€‹ tiene una medida de probabilidad, el promedio ponderado es de especial interÃ©s.  Valor esperadoâ€‹  Al promedio ponderado se le llama tambiÃ©n valor medio o valor esperado de la variable aleatoria XXX. Para una variable aleatoria XXX se define el valor esperado, Î¼X\\mu_XÎ¼Xâ€‹, como:  Î¼X=E(X)={âˆ‘xâˆˆÎ˜Xxâ‹…pX(x),Â CasoÂ DiscretoÂ âˆ«âˆ’âˆâˆxâ‹…fX(x)dx,Â CasoÂ ContinuoÂ \\mu_X=\\mathrm{E}(X)= \\begin{cases}\\displaystyle\\sum_{x \\in \\Theta_X} x \\cdot p_X(x), &amp; \\text { Caso Discreto } \\\\[20pt] \\displaystyle\\int_{-\\infty}^{\\infty} x \\cdot f_X(x) d x, &amp; \\text { Caso Continuo }\\end{cases}Î¼Xâ€‹=E(X)=â©â¨â§â€‹xâˆˆÎ˜Xâ€‹âˆ‘â€‹xâ‹…pXâ€‹(x),âˆ«âˆ’âˆâˆâ€‹xâ‹…fXâ€‹(x)dx,â€‹Â CasoÂ Discreto CasoÂ ContinuoÂ â€‹  Este valor existe siempre y cuando  âˆ‘xâˆˆÎ˜Xâˆ£xâˆ£â‹…pX(x)&lt;âˆâˆ˜âˆ«âˆ’âˆâˆâˆ£xâˆ£â‹…fX(x)dx&lt;âˆ\\sum_{x \\in \\Theta_X}|x| \\cdot p_X(x)&lt;\\infty \\quad \\circ \\quad \\int_{-\\infty}^{\\infty}|x| \\cdot f_X(x) d x&lt;\\inftyxâˆˆÎ˜Xâ€‹âˆ‘â€‹âˆ£xâˆ£â‹…pXâ€‹(x)&lt;âˆâˆ˜âˆ«âˆ’âˆâˆâ€‹âˆ£xâˆ£â‹…fXâ€‹(x)dx&lt;âˆ  Modaâ€‹  Es el valor mÃ¡s frecuente o con mayor probabilidad de ocurrencia. Para los casos discretos y continuos, tenemos que  Â CasoÂ Discreto: ModaÂ =maxâ¡xâˆˆÎ˜XpX(x)Â CasoÂ Continuo: ModaÂ =maxâ¡xâˆˆÎ˜XfX(x)\\begin{aligned} \\text { Caso Discreto: } &amp; \\quad \\text { Moda }=\\max _{x \\in \\Theta_X} p_X(x) \\\\ \\text { Caso Continuo: } &amp; \\quad \\text { Moda }=\\max _{x \\in \\Theta_X} f_X(x) \\end{aligned}Â CasoÂ Discreto: CasoÂ Continuo:Â â€‹Â ModaÂ =xâˆˆÎ˜Xâ€‹maxâ€‹pXâ€‹(x)Â ModaÂ =xâˆˆÎ˜Xâ€‹maxâ€‹fXâ€‹(x)â€‹  Medianaâ€‹  Sea xmedÂ x_{\\text {med }}xmedÂ â€‹ el valor que toma la mediana, entonces  FX(xmedÂ )=1/2F_X\\left(x_{\\text {med }}\\right)=1 / 2FXâ€‹(xmedÂ â€‹)=1/2  En resumen, el valor esperado de una variable aleatoria es un valor promedio que puede ser visto como un indicador del valor central de la distribuciÃ³n de probabilidad, por esta razÃ³n se considera como un parÃ¡metro de localizaciÃ³n.  Por otra parte, la mediana y la moda de una distribuciÃ³n tambiÃ©n son parÃ¡metros de localizaciÃ³n que no necesariamente son iguales a la media.  Nota Cuando la distribuciÃ³n es simÃ©trica, estas tres medidas son parecidas.  ","version":"Next","tagName":"h3"},{"title":"Medidas de posiciÃ³nâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-de-posiciÃ³n","content":" Percentilesâ€‹  Si xpx_pxpâ€‹ es el valor que toma el percentil pÃ—100%p \\times 100 \\%pÃ—100%, entonces FX(xp)=F_X\\left(x_p\\right)=FXâ€‹(xpâ€‹)= ppp.  Algunos casos particulares de percentil son: quintiles, cuartiles, deciles, mediana.  Nota Los valores para cada tipo de percentil son: Quintiles: p=0.2p=0.2p=0.2Cuartiles: p=0.25p=0.25p=0.25Deciles: p=0.1p=0.1p=0.1Mediana: p=0.5p=0.5p=0.5  Esperanza matemÃ¡ticaâ€‹  La nociÃ³n del valor esperado como un promedio ponderado puede ser generalizado para funciones de la variable aleatoria XXX. Dada una funciÃ³n g(X)g(X)g(X), entonces el valor esperado de esta puede ser obtenido como:  E[g(X)]={âˆ‘xâˆˆÎ˜Xg(x)â‹…pX(x),Â CasoÂ DiscretoÂ âˆ«âˆ’âˆâˆg(x)â‹…fX(x)dx,Â CasoÂ ContinuoÂ E[g(X)]= \\begin{cases}\\displaystyle\\sum_{x \\in \\Theta_X} g(x) \\cdot p_X(x), &amp; \\text { Caso Discreto } \\\\[20pt] \\displaystyle\\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) d x, &amp; \\text { Caso Continuo }\\end{cases}E[g(X)]=â©â¨â§â€‹xâˆˆÎ˜Xâ€‹âˆ‘â€‹g(x)â‹…pXâ€‹(x),âˆ«âˆ’âˆâˆâ€‹g(x)â‹…fXâ€‹(x)dx,â€‹Â CasoÂ Discreto CasoÂ ContinuoÂ â€‹  FunciÃ³n generadora de momentosâ€‹  La funciÃ³n generadora de momentos de una variable aleatoria XXX se define como  MX(t)=E[expâ¡(tX)]M_X(t)=\\mathrm{E}[\\exp (t X)]MXâ€‹(t)=E[exp(tX)]  Esta funciÃ³n puede no estar definida para algunos valores de ttt, pero si existe en un intervalo abierto que contenga al cero, entonces esta funciÃ³n tiene la propiedad de determinar la distribuciÃ³n de probabilidad de XXX.  Cuando esto Ãºltimo ocurra, esta funciÃ³n permite obtener el rrr-Ã©simo momento de XXX de la siguiente forma  M(r)(0)=E(Xr)M^{(r)}(0)=\\mathrm{E}\\left(X^r\\right)M(r)(0)=E(Xr)  ","version":"Next","tagName":"h3"},{"title":"Medidas de dispersiÃ³nâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-de-dispersiÃ³n","content":" Es de interÃ©s cuantificar el nivel de dispersiÃ³n que tienen una variable aleatoria con respecto a un valor de referencia. Por ejemplo, nos podrÃ­a interesar la distancia esperada de los valores de una variable aleatoria XXX con respeto al valor esperado Î¼X\\mu_XÎ¼Xâ€‹, es decir, E[(Xâˆ’Î¼X)]\\mathrm{E}\\left[\\left(X-\\mu_X\\right)\\right]E[(Xâˆ’Î¼Xâ€‹)].  Esta idea de dispersiÃ³n tiene el problema que siempre da como resultado cero.  Varianzaâ€‹  Una alternativa es utilizar la definiciÃ³n de varianza, es decir  ÏƒX2=Varâ¡(X)=E[(Xâˆ’Î¼X)2]={âˆ‘xâˆˆÎ˜X(xâˆ’Î¼X)2â‹…pX(x),Â CasoÂ DiscretoÂ âˆ«âˆ’âˆâˆ(xâˆ’Î¼X)2â‹…fX(x)dx,Â CasoÂ ContinuoÂ =E(X2)âˆ’Î¼X2\\begin{aligned} \\sigma_X^2 &amp; =\\operatorname{Var}(X)=\\mathrm{E}\\left[\\left(X-\\mu_X\\right)^2\\right] \\\\ &amp; = \\begin{cases} \\displaystyle\\sum_{x \\in \\Theta_X}\\left(x-\\mu_X\\right)^2 \\cdot p_X(x), &amp; \\text { Caso Discreto } \\\\[20pt] \\displaystyle\\int_{-\\infty}^{\\infty}\\left(x-\\mu_X\\right)^2 \\cdot f_X(x) d x, &amp; \\text { Caso Continuo }\\end{cases} \\\\[30pt] &amp; =\\mathrm{E}\\left(X^2\\right)-\\mu_X^2 \\end{aligned}ÏƒX2â€‹â€‹=Var(X)=E[(Xâˆ’Î¼Xâ€‹)2]=â©â¨â§â€‹xâˆˆÎ˜Xâ€‹âˆ‘â€‹(xâˆ’Î¼Xâ€‹)2â‹…pXâ€‹(x),âˆ«âˆ’âˆâˆâ€‹(xâˆ’Î¼Xâ€‹)2â‹…fXâ€‹(x)dx,â€‹Â CasoÂ Discreto CasoÂ ContinuoÂ â€‹=E(X2)âˆ’Î¼X2â€‹â€‹  DesviaciÃ³n estÃ¡ndarâ€‹  En tÃ©rminos de dimensionalidad, es conveniente utilizar la desviaciÃ³n estandar, es decir,  ÏƒX=Varâ¡(X)\\sigma_X=\\sqrt{\\operatorname{Var}(X)}ÏƒXâ€‹=Var(X)â€‹  Coeficiente de variaciÃ³nâ€‹  Ahora, si Î¼X&gt;0\\mu_X&gt;0Î¼Xâ€‹&gt;0, una medida adimensional de la variabilidad es el coeficiente de variaciÃ³n (c.o.v)  Î´X=ÏƒXÎ¼X\\delta_X=\\frac{\\sigma_X}{\\mu_X}Î´Xâ€‹=Î¼Xâ€‹ÏƒXâ€‹â€‹  Rango y IQRâ€‹  Las definiciones para el rango y el rango intercuartÃ­lico (IQR) son  Â RangoÂ =maxâ¡âˆ’minâ¡IQR=x0.75âˆ’x0.25\\begin{aligned} \\text { Rango } &amp; =\\max -\\min \\\\ \\mathrm{IQR} &amp; =x_{0.75}-x_{0.25} \\end{aligned}Â RangoÂ IQRâ€‹=maxâˆ’min=x0.75â€‹âˆ’x0.25â€‹â€‹  ","version":"Next","tagName":"h3"},{"title":"Medidas de asimetrÃ­aâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-de-asimetrÃ­a","content":" Skewnessâ€‹  Se define una medida de asimetrÃ­a (skewness) corresponde al tercer momento central:  E[(Xâˆ’Î¼X)3]={âˆ‘xiâˆˆÎ˜X(xiâˆ’Î¼X)3â‹…pX(xi),Â CasoÂ DiscretoÂ âˆ«âˆ’âˆâˆ(xâˆ’Î¼X)3â‹…fX(x)dx,Â CasoÂ ContinuoÂ \\mathrm{E}\\left[\\left(X-\\mu_X\\right)^3\\right]= \\begin{cases} \\displaystyle\\sum_{x_i \\in \\Theta_X}\\left(x_i-\\mu_X\\right)^3 \\cdot p_X\\left(x_i\\right), &amp; \\text { Caso Discreto } \\\\[20pt] \\displaystyle\\int_{-\\infty}^{\\infty}\\left(x-\\mu_X\\right)^3 \\cdot f_X(x) d x, &amp; \\text { Caso Continuo }\\end{cases}E[(Xâˆ’Î¼Xâ€‹)3]=â©â¨â§â€‹xiâ€‹âˆˆÎ˜Xâ€‹âˆ‘â€‹(xiâ€‹âˆ’Î¼Xâ€‹)3â‹…pXâ€‹(xiâ€‹),âˆ«âˆ’âˆâˆâ€‹(xâˆ’Î¼Xâ€‹)3â‹…fXâ€‹(x)dx,â€‹Â CasoÂ Discreto CasoÂ ContinuoÂ â€‹  Coeficiente de asimetrÃ­aâ€‹  Una medida conveniente es el coeficiente de asimetrÃ­a que se define como:  Î¸X=E[(Xâˆ’Î¼X)3]ÏƒX3\\theta_X=\\frac{E\\left[\\left(X-\\mu_X\\right)^3\\right]}{\\sigma_X^3}Î¸Xâ€‹=ÏƒX3â€‹E[(Xâˆ’Î¼Xâ€‹)3]â€‹  Skewness  ","version":"Next","tagName":"h3"},{"title":"Medidas de curtosisâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#medidas-de-curtosis","content":" Curtosisâ€‹  Finalmente, el cuarto momento central se conoce como la curtosis  E[(Xâˆ’Î¼X)4]={âˆ‘xiâˆˆÎ˜X(xiâˆ’Î¼X)4â‹…pX(xi),Â CasoÂ DiscretoÂ âˆ«âˆâˆ(xâˆ’Î¼X)4â‹…fX(x)dx,Â CasoÂ ContinuoÂ \\mathrm{E}\\left[\\left(X-\\mu_X\\right)^4\\right]= \\begin{cases} \\displaystyle\\sum_{x_i \\in \\Theta_X}\\left(x_i-\\mu_X\\right)^4 \\cdot p_X\\left(x_i\\right), &amp; \\text { Caso Discreto } \\\\[20pt] \\displaystyle\\int_{\\infty}^{\\infty}\\left(x-\\mu_X\\right)^4 \\cdot f_X(x) d x, &amp; \\text { Caso Continuo }\\end{cases}E[(Xâˆ’Î¼Xâ€‹)4]=â©â¨â§â€‹xiâ€‹âˆˆÎ˜Xâ€‹âˆ‘â€‹(xiâ€‹âˆ’Î¼Xâ€‹)4â‹…pXâ€‹(xiâ€‹),âˆ«âˆâˆâ€‹(xâˆ’Î¼Xâ€‹)4â‹…fXâ€‹(x)dx,â€‹Â CasoÂ Discreto CasoÂ ContinuoÂ â€‹  que es una medida del &quot;apuntamiento&quot; o &quot;achatamiento&quot; de la distribuciÃ³n de probabilidad o de densidad.  Coeficiente de curtosisâ€‹  Usualmente se prefiere el coeficiente de curtosis  KX=E[(Xâˆ’Î¼X)4]ÏƒX4âˆ’3K_X=\\frac{E\\left[\\left(X-\\mu_X\\right)^4\\right]}{\\sigma_X^4}-3KXâ€‹=ÏƒX4â€‹E[(Xâˆ’Î¼Xâ€‹)4]â€‹âˆ’3  ","version":"Next","tagName":"h3"},{"title":"Distribuciones de probabilidadâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#distribuciones-de-probabilidad-1","content":" ","version":"Next","tagName":"h2"},{"title":"Normalâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#normal","content":" La funciÃ³n densidad de una variable aleatoria XXX con distribuciÃ³n Normalâ¡(Î¼,Ïƒ)\\operatorname{Normal}(\\mu, \\sigma)Normal(Î¼,Ïƒ) es de la forma:  fX(x)=12Ï€Ïƒ2expâ¡{âˆ’12(xâˆ’Î¼Ïƒ)2},âˆ’âˆ&lt;x&lt;âˆf_X(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right\\}, \\quad-\\infty&lt;x&lt;\\inftyfXâ€‹(x)=2Ï€Ïƒ2â€‹1â€‹exp{âˆ’21â€‹(Ïƒxâˆ’Î¼â€‹)2},âˆ’âˆ&lt;x&lt;âˆ  con Î¼\\muÎ¼ parÃ¡metro de localizaciÃ³n y Ïƒ\\sigmaÏƒ un parÃ¡metro de escala o forma tales que:  âˆ’âˆ&lt;Î¼&lt;âˆ,0&lt;Ïƒ&lt;âˆ-\\infty&lt;\\mu&lt;\\infty, \\quad 0&lt;\\sigma&lt;\\inftyâˆ’âˆ&lt;Î¼&lt;âˆ,0&lt;Ïƒ&lt;âˆ  DistribuciÃ³n normal  Sea XXX una variable aleatoria Normalâ¡(Î¼,Ïƒ)\\operatorname{Normal}(\\mu, \\sigma)Normal(Î¼,Ïƒ) con funciÃ³n de distribuciÃ³n acumulada FXF_XFXâ€‹. Para dos valores dados aaa y bbb (con a&lt;ba&lt;ba&lt;b) se tiene que:  P(a&lt;Xâ‰¤b)=FX(b)âˆ’FX(a)P(a&lt;X \\leq b)=F_X(b)-F_X(a)P(a&lt;Xâ‰¤b)=FXâ€‹(b)âˆ’FXâ€‹(a)  DistribuciÃ³n normal acumulada  Algunas propiedades:  E(X)=Î¼E(X)=\\muE(X)=Î¼.Varâ¡(X)=Ïƒ2\\operatorname{Var}(X)=\\sigma^2Var(X)=Ïƒ2.FX(x)=Î¦(xâˆ’Î¼Ïƒ)F_X(x)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)FXâ€‹(x)=Î¦(Ïƒxâˆ’Î¼â€‹).  ","version":"Next","tagName":"h3"},{"title":"Normal estÃ¡ndarâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#normal-estÃ¡ndar","content":" Un caso especial es cuando Î¼=0\\mu=0Î¼=0 y Ïƒ=1\\sigma=1Ïƒ=1. Este caso es conocido como la distribuciÃ³n normal estÃ¡ndar.  fX(x)=12Ï€eâˆ’x2/2f_X(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{-x^2 / 2}fXâ€‹(x)=2Ï€â€‹1â€‹eâˆ’x2/2  La ventaja es que funciÃ³n de distribuciÃ³n de probabilidad acumulada se encuentra tabulada, la cual se denota por Î¦(â‹…)\\Phi(\\cdot)Î¦(â‹…).  Sea SSS una variable aleatoria con distribuciÃ³n normal estÃ¡ndar, cuya funciÃ³n de distribuciÃ³n de probabilidad acumulada esta dada por  Î¦(s)=FS(s)=âˆ«âˆ’âˆs12Ï€eâˆ’x2/2dx\\Phi(s)=F_S(s)=\\int_{-\\infty}^s \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2 / 2} d xÎ¦(s)=FSâ€‹(s)=âˆ«âˆ’âˆsâ€‹2Ï€â€‹1â€‹eâˆ’x2/2dx  Algunas propiedades son:  Sp=Î¦âˆ’1(p)=âˆ’Î¦âˆ’1(1âˆ’p)S_p=\\Phi^{-1}(p)=-\\Phi^{-1}(1-p)Spâ€‹=Î¦âˆ’1(p)=âˆ’Î¦âˆ’1(1âˆ’p)Î¦(âˆ’s)=1âˆ’Î¦(s)\\Phi(-s)=1-\\Phi(s)Î¦(âˆ’s)=1âˆ’Î¦(s).  La tabla normal estÃ¡ndar es el resultado de Î¦(Sp)=p\\Phi\\left(S_p\\right)=pÎ¦(Spâ€‹)=p para Spâ‰¥0S_p \\geq 0Spâ€‹â‰¥0:  DistribuciÃ³n normal estÃ¡ndar  ","version":"Next","tagName":"h3"},{"title":"Log-Normalâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#log-normal","content":" Se dice que XXX sigue una ley de probabilidad Log-Normal si su funciÃ³n de densidad esta dada por  fX(x)=12Ï€1(Î¶x)expâ¡[âˆ’12(lnâ¡xâˆ’Î»Î¶)2],xâ‰¥0f_X(x)=\\frac{1}{\\sqrt{2 \\pi}} \\frac{1}{(\\zeta x)} \\exp \\left[-\\frac{1}{2}\\left(\\frac{\\ln x-\\lambda}{\\zeta}\\right)^2\\right], \\quad x \\geq 0fXâ€‹(x)=2Ï€â€‹1â€‹(Î¶x)1â€‹exp[âˆ’21â€‹(Î¶lnxâˆ’Î»â€‹)2],xâ‰¥0  Donde,  Î»=E(lnâ¡X)Â yÂ Î¶=Varâ¡(lnâ¡X)\\lambda=E(\\ln X) \\quad \\text { y } \\quad \\zeta=\\sqrt{\\operatorname{Var}(\\ln X)}Î»=E(lnX)Â yÂ Î¶=Var(lnX)â€‹  Algunas propiedades:  lnâ¡Xâˆ¼Normalâ¡(Î»,Î¶)\\ln X \\sim \\operatorname{Normal}(\\lambda, \\zeta)lnXâˆ¼Normal(Î»,Î¶)Î¼X=expâ¡(Î»+Î¶2/2)\\mu_X=\\exp \\left(\\lambda+\\zeta^2 / 2\\right)Î¼Xâ€‹=exp(Î»+Î¶2/2)Â MedianaÂ =expâ¡(Î»)\\text { Mediana }=\\exp (\\lambda)Â MedianaÂ =exp(Î»)E(Xk)=expâ¡(Î»k)â‹…MZ(Î¶k),Â conÂ Zâˆ¼Normalâ¡(0,1)\\mathrm{E}\\left(X^k\\right)=\\exp (\\lambda k) \\cdot M_Z(\\zeta k), \\text { con } Z \\sim \\operatorname{Normal}(0,1)E(Xk)=exp(Î»k)â‹…MZâ€‹(Î¶k),Â conÂ Zâˆ¼Normal(0,1)ÏƒX2=Î¼X2(eÎ¶2âˆ’1)\\sigma_X^2=\\mu_X^2\\left(e^{\\zeta^2}-1\\right)ÏƒX2â€‹=Î¼X2â€‹(eÎ¶2âˆ’1)Î¶=lnâ¡(1+Î´X2)\\zeta=\\sqrt{\\ln \\left(1+\\delta_X^2\\right)}Î¶=ln(1+Î´X2â€‹)â€‹  DistribuciÃ³n log-normal  Nota En la distribuciÃ³n log-normal, la relaciÃ³n entre el coeficiente de variaciÃ³n (c.o.v.) Î´\\deltaÎ´ y el parÃ¡metro Î¶\\zetaÎ¶ es tal que si los dos son suficientemente pequeÃ±os, entonces Î´â‰ˆÎ¶\\delta \\approx \\zetaÎ´â‰ˆÎ¶.  RelaciÃ³n entre c.o.v. y parÃ¡metro zeta  ","version":"Next","tagName":"h3"},{"title":"Binomial y Bernoulliâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#binomial-y-bernoulli","content":" En las mÃ¡s diversas Ã¡reas de la IngenierÃ­a, a menudo los problemas involucran la ocurrencia o recurrencia de un evento, el cual es impredecible, como una secuencia de &quot;experimentos&quot;. Por ejemplo:  Para un dÃ­a de lluvia, Â¿colapsa o no un sistema de drenaje?Al comprar un producto, Â¿Ã©ste satisface o no los requerimientos de calidad?Un alumno Â¿aprueba o reprueba el curso?  Notar que hay sÃ³lo dos resultados posibles para cada &quot;experimento&quot;. Las variables descritas pueden ser modeladas por una secuencia Bernoulli, la cual se basa en los siguientes supuestos:  Cada experimento, tiene una de dos opciones: ocurrencia o no ocurrencia del evento.La probabilidad de ocurrencia del evento (&quot;Ã©xito&quot;) en cada experimento es constante (digamos ppp).Los experimentos son estadÃ­sticamente independientes.  Dada una secuencia Bernoulli, si XXX es el nÃºmero de ocurrencias del evento Ã©xito entre los nnn experimentos, con probabilidad de ocurrencia igual a ppp, entonces la probabilidad que ocurran exactamente xxx Ã©xitos en los nnn experimentos esta representada por la distribuciÃ³n Binomial, descrita por  pX(x)=(nx)px(1âˆ’p)nâˆ’x,x=0,1,â€¦,nFX(x)={0,x&lt;0âˆ‘k=0[x](nk)pk(1âˆ’p)nâˆ’k,0â‰¤x&lt;n1,xâ‰¥n\\begin{gathered} p_X(x)=\\binom{n}{x} p^x(1-p)^{n-x}, \\quad x=0,1, \\ldots, n \\\\[20pt] F_X(x)= \\begin{cases} 0, &amp; x&lt;0 \\\\[8pt] \\displaystyle\\sum_{k=0}^{[x]}\\binom{n}{k} p^k(1-p)^{n-k}, &amp; 0 \\leq x&lt;n \\\\[15pt] 1, &amp; x \\geq n\\end{cases} \\end{gathered}pXâ€‹(x)=(xnâ€‹)px(1âˆ’p)nâˆ’x,x=0,1,â€¦,nFXâ€‹(x)=â©â¨â§â€‹0,k=0âˆ‘[x]â€‹(knâ€‹)pk(1âˆ’p)nâˆ’k,1,â€‹x&lt;00â‰¤x&lt;nxâ‰¥nâ€‹â€‹  El valor esperado y varianza estÃ¡n dados por  E(X)=np,Varâ¡(X)=np(1âˆ’p)E(X)=n p, \\quad \\operatorname{Var}(X)=n p(1-p)E(X)=np,Var(X)=np(1âˆ’p)  Por ejemplo, para Binomialâ¡(n=30,p=1/2)\\operatorname{Binomial}(n=30, p=1 / 2)Binomial(n=30,p=1/2), vemos que  DistribuciÃ³n binomial  ","version":"Next","tagName":"h3"},{"title":"GeomÃ©tricaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#geomÃ©trica","content":" Dada una secuencia Bernoulli, el nÃºmero de experimentos hasta la ocurrencia del primer evento exitoso sigue una distribuciÃ³n geomÃ©trica.  Si el primer Ã©xito ocurre en el nnn-Ã©simo experimento, los primeros nâˆ’1n-1nâˆ’1 fueron &quot;fracasos&quot;. Si NNN es la variable aleatoria que representa el nÃºmero de experimentos hasta el primer Ã©xito, entonces:  P(N=n)=p(1âˆ’p)nâˆ’1,n=1,2,â€¦P(N=n)=p(1-p)^{n-1}, \\quad n=1,2, \\ldotsP(N=n)=p(1âˆ’p)nâˆ’1,n=1,2,â€¦  La funciÃ³n distribuciÃ³n esta dada por:  FN(n)=âˆ‘k=1[n]p(1âˆ’p)kâˆ’1=1âˆ’(1âˆ’p)[n]F_N(n)=\\sum_{k=1}^{[n]} p(1-p)^{k-1}=1-(1-p)^{[n]}FNâ€‹(n)=k=1âˆ‘[n]â€‹p(1âˆ’p)kâˆ’1=1âˆ’(1âˆ’p)[n]  para nâ‰¥1n \\geq 1nâ‰¥1 y cero en otro caso. Mientras que su valor esperado y varianza son:  E(N)=1p,Varâ¡(N)=(1âˆ’p)p2E(N)=\\frac{1}{p}, \\quad \\operatorname{Var}(N)=\\frac{(1-p)}{p^2}E(N)=p1â€‹,Var(N)=p2(1âˆ’p)â€‹  Por ejemplo, para GeomeËŠtricaâ¡(p=1/6)\\operatorname{GeomeÌtrica}(p=1 / 6)GeomeËŠtrica(p=1/6), vemos que  DistribuciÃ³n geomÃ©trica  ","version":"Next","tagName":"h3"},{"title":"Binomial negativaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#binomial-negativa","content":" La distribuciÃ³n geomÃ©trica permite modelar el numero de experimentos hasta la primera ocurrencia.  El numero de experimentos hasta la kkk-Ã©sima ocurrencia de un Ã©xito es modelada por la distribuciÃ³n binomial negativa.  P(Tk=x)=(xâˆ’1kâˆ’1)pk(1âˆ’p)xâˆ’k,x=k,k+1,k+2,â€¦E(Tk)=kp,Varâ¡(Tk)=k(1âˆ’p)p2\\begin{gathered} P\\left(T_k=x\\right)=\\binom{x-1}{k-1} p^k(1-p)^{x-k}, \\quad x=k, k+1, k+2, \\ldots \\\\ E\\left(T_k\\right)=\\frac{k}{p}, \\quad \\operatorname{Var}\\left(T_k\\right)=\\frac{k(1-p)}{p^2} \\end{gathered}P(Tkâ€‹=x)=(kâˆ’1xâˆ’1â€‹)pk(1âˆ’p)xâˆ’k,x=k,k+1,k+2,â€¦E(Tkâ€‹)=pkâ€‹,Var(Tkâ€‹)=p2k(1âˆ’p)â€‹â€‹  Por ejemplo, para Binâ¡âˆ’Negâ¡(k=3,p=1/6)\\operatorname{Bin}-\\operatorname{Neg}(k=3, p=1 / 6)Binâˆ’Neg(k=3,p=1/6), vemos que  DistribuciÃ³n binomial negativa  ","version":"Next","tagName":"h3"},{"title":"Poissonâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#poisson","content":" Muchos problemas fÃ­sicos de interÃ©s para ingenieros y cientÃ­ficos que implican las ocurrencias posibles de eventos en cualquier punto en el tiempo y/o en el espacio. Por ejemplo:  Los terremotos pueden ocurrir en cualquier momento y en cualquier lugar en una regiÃ³n con actividad sÃ­smica en el mundo.Las grietas por fatiga puede producirse en cualquier punto de una soldadura continua.Los accidentes de trÃ¡fico pueden suceder en cualquier momento en una autopista.  Este problema puede ser modelado como secuencia Bernoulli, dividiendo el tiempo o el espacio en pequeÃ±os intervalos &quot;apropiados&quot; tal que solo un evento puede ocurrir o no dentro de cada intervalo (Ensayo Bernoulli).  Sin embargo, si el evento puede ocurrir al azar en cualquier instante de tiempo (o en cualquier punto del espacio), esto puede ocurrir mÃ¡s de una vez en cualquier momento o intervalo de espacio.  En tal caso, las ocurrencias del evento puede ser mÃ¡s apropiado el modelo con un proceso de Poisson o la secuencia Poisson.  Supuestosâ€‹  Un evento puede ocurrir al azar y en cualquier instante de tiempo o en cualquier punto en el espacio.La ocurrencia(s) de un evento en un intervalo de tiempo dado (o espacio) es estadÃ­sticamente independiente a lo que ocurra en otros intervalos (o espacios) que no se solapen.La probabilidad de ocurrencia de un evento en un pequeÃ±o intervalo Î”t\\Delta tÎ”t es proporcional a Î”t\\Delta tÎ”t, y puede estar dada por Î½Î”t\\nu \\Delta tÎ½Î”t, donde Î½\\nuÎ½ es la tasa de incidencia media del evento (que se supone constante).La probabilidad de dos o mÃ¡s eventos en Î”t\\Delta tÎ”t es insignificante.  Bajo los supuestos anteriores, el nÃºmero de eventos estadÃ­sticamente independientes en ttt (tiempo o espacio) esta regido por la funciÃ³n de probabilidad del modelo Poisson, donde la variable aleatoria XtX_tXtâ€‹ : nÃºmero de eventos en el intervalo de tiempo (0,t)(0, t)(0,t).  P(Xt=x)=(Î½t)xeâˆ’Î½tx!=Î»xeâˆ’Î»x!,x=0,1,2,â€¦P\\left(X_t=x\\right)=\\frac{(\\nu t)^x e^{-\\nu t}}{x !}=\\frac{\\lambda^x e^{-\\lambda}}{x !}, \\quad x=0,1,2, \\ldotsP(Xtâ€‹=x)=x!(Î½t)xeâˆ’Î½tâ€‹=x!Î»xeâˆ’Î»â€‹,x=0,1,2,â€¦  donde Î½\\nuÎ½ es la tasa de ocurrencia media por unidad de tiempo y Î»\\lambdaÎ» su espe-ranza en (0,t)(0, t)(0,t) :  E(Xt)=Î½t=Î»E\\left(X_t\\right)=\\nu t=\\lambdaE(Xtâ€‹)=Î½t=Î»  ","version":"Next","tagName":"h3"},{"title":"Exponencialâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#exponencial","content":" En un Proceso de Poisson el tiempo transcurrido entre la ocurrencia de eventos puede ser descrito por una distribuciÃ³n exponencial.  Si T1T_1T1â€‹ representa al tiempo transcurrido hasta la ocurrencia del primer evento en un Proceso de Poisson, el evento (T1&gt;t)\\left(T_1&gt;t\\right)(T1â€‹&gt;t) implica que en el intervalo (0,t)(0, t)(0,t) no ocurren eventos, es decir,  P(T1&gt;t)=P(Xt=0)=(Î½t)0eâˆ’Î½t0!=eâˆ’Î½t,P\\left(T_1&gt;t\\right)=P\\left(X_t=0\\right)=\\frac{(\\nu t)^0 e^{-\\nu t}}{0 !}=e^{-\\nu t},P(T1â€‹&gt;t)=P(Xtâ€‹=0)=0!(Î½t)0eâˆ’Î½tâ€‹=eâˆ’Î½t,  con  Xtâˆ¼Poissonâ¡(Î½t)X_t \\sim \\operatorname{Poisson}(\\nu t)Xtâ€‹âˆ¼Poisson(Î½t)  Por lo tanto la funciÃ³n de distribuciÃ³n de probabilidad acumulada de T1T_1T1â€‹ esta dada por:  FT1(t)=P(T1â‰¤t)=1âˆ’P(T1&gt;t)=1âˆ’eâˆ’Î½tF_{T_1}(t)=P\\left(T_1 \\leq t\\right)=1-P\\left(T_1&gt;t\\right)=1-e^{-\\nu t}FT1â€‹â€‹(t)=P(T1â€‹â‰¤t)=1âˆ’P(T1â€‹&gt;t)=1âˆ’eâˆ’Î½t  Su funciÃ³n densidad se obtiene como sigue:  fT1(t)=ddtFT1(t)=Î½eâˆ’Î½tf_{T_1}(t)=\\frac{d}{d t} F_{T_1}(t)=\\nu e^{-\\nu t}fT1â€‹â€‹(t)=dtdâ€‹FT1â€‹â€‹(t)=Î½eâˆ’Î½t  que corresponde a la funciÃ³n densidad de una variable aleatoria con distribuciÃ³n exponencial.  Esta distribuciÃ³n al igual que la geomÃ©trica tiene la propiedad de la carencia de memoria, es decir, si Tâˆ¼Exponencialâ¡(Î½)T \\sim \\operatorname{Exponencial}(\\nu)Tâˆ¼Exponencial(Î½) entonces se tiene que  P(T&gt;t+sâˆ£T&gt;s)=P(T&gt;t)P(T&gt;t+s \\mid T&gt;s)=P(T&gt;t)P(T&gt;t+sâˆ£T&gt;s)=P(T&gt;t)  Este resultado, nos permite asumir que todos los tiempos entre eventos Poisson (Î½t)(\\nu t)(Î½t) distribuyen Exponencial (Î½)(\\nu)(Î½).  Carencia de memoria La carencia de memoria es una propiedad que indica que la probabilidad de que un evento ocurra en el futuro no depende de cuÃ¡nto tiempo ha pasado desde el Ãºltimo evento.  En resumen, una variable aleatoria XXX con distribuciÃ³n Exponencial de parÃ¡metro Î½&gt;0\\nu&gt;0Î½&gt;0, tiene funciÃ³n densidad y de distribuciÃ³n:  fX(x)={Î½eâˆ’Î½x,xâ‰¥00,x&lt;0FX(x)={0,x&lt;01âˆ’eâˆ’Î½x,xâ‰¥0f_X(x)=\\left\\{\\begin{array}{ll} \\nu e^{-\\nu x}, &amp; x \\geq 0 \\\\ 0, &amp; x&lt;0 \\end{array} \\quad F_X(x)= \\begin{cases}0, &amp; x&lt;0 \\\\ 1-e^{-\\nu x}, &amp; x \\geq 0\\end{cases}\\right.fXâ€‹(x)={Î½eâˆ’Î½x,0,â€‹xâ‰¥0x&lt;0â€‹FXâ€‹(x)={0,1âˆ’eâˆ’Î½x,â€‹x&lt;0xâ‰¥0â€‹  Mientras que su valor esperado y varianza son:  Î¼X=1Î½,ÏƒX2=1Î½2\\mu_X=\\frac{1}{\\nu}, \\quad \\sigma_X^2=\\frac{1}{\\nu^2}Î¼Xâ€‹=Î½1â€‹,ÏƒX2â€‹=Î½21â€‹  Exponencial trasladadaâ€‹  Una variable aleatoria XXX con distribuciÃ³n Exponencial de parÃ¡metro Î½&gt;0\\nu&gt;0Î½&gt;0, se llama trasladada en a si su funciÃ³n densidad y de distribuciÃ³n acumulada son  fX(x)={Î½eâˆ’Î½(xâˆ’a),xâ‰¥a0,x&lt;aFX(x)={0,x&lt;a1âˆ’eâˆ’Î½(xâˆ’a),xâ‰¥af_X(x)=\\left\\{\\begin{array}{ll} \\nu e^{-\\nu(x-a)}, &amp; x \\geq a \\\\ 0, &amp; x&lt;a \\end{array} \\quad F_X(x)= \\begin{cases}0, &amp; x&lt;a \\\\ 1-e^{-\\nu(x-a)}, &amp; x \\geq a\\end{cases}\\right.fXâ€‹(x)={Î½eâˆ’Î½(xâˆ’a),0,â€‹xâ‰¥ax&lt;aâ€‹FXâ€‹(x)={0,1âˆ’eâˆ’Î½(xâˆ’a),â€‹x&lt;axâ‰¥aâ€‹  Su valor esperado y varianza estÃ¡n dados por  Î¼X=1Î½+a,ÏƒX2=1Î½2\\mu_X=\\frac{1}{\\nu}+a, \\quad \\sigma_X^2=\\frac{1}{\\nu^2}Î¼Xâ€‹=Î½1â€‹+a,ÏƒX2â€‹=Î½21â€‹  DistribuciÃ³n exponencial trasladada  ","version":"Next","tagName":"h3"},{"title":"Gammaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#gamma","content":" Una variable aleatoria XXX con distribuciÃ³n Gamma tiene funciÃ³n densidad  fX(x)=Î½kÎ“(k)xkâˆ’1eâˆ’Î½x,xâ‰¥0f_X(x)=\\frac{\\nu^k}{\\Gamma(k)} x^{k-1} e^{-\\nu x}, \\quad x \\geq 0fXâ€‹(x)=Î“(k)Î½kâ€‹xkâˆ’1eâˆ’Î½x,xâ‰¥0  donde k,Î½k, \\nuk,Î½ son parÃ¡metros positivos. La funciÃ³n Î“(Î±)=âˆ«0âˆuÎ±âˆ’1eâˆ’udu\\Gamma(\\alpha)=\\displaystyle\\int_0^{\\infty} u^{\\alpha-1} e^{-u} d uÎ“(Î±)=âˆ«0âˆâ€‹uÎ±âˆ’1eâˆ’udu, la cual tiene las siguientes propiedades:  Î“(Î±+1)=Î±Î“(Î±)Î“(n+1)=n!Â siÂ nâˆˆN0Î“(1/2)=Ï€\\begin{aligned} &amp; \\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha) \\\\ &amp; \\Gamma(n+1)=n ! \\text { si } n \\in \\mathbb{N}_0 \\\\ &amp; \\Gamma(1 / 2)=\\sqrt{\\pi} \\end{aligned}â€‹Î“(Î±+1)=Î±Î“(Î±)Î“(n+1)=n!Â siÂ nâˆˆN0â€‹Î“(1/2)=Ï€â€‹â€‹  RelaciÃ³n con distribuciÃ³n Poissonâ€‹  En un Proceso de Poisson el tiempo transcurrido hasta la ocurrencia del kkk-Ã©simo evento puede ser descrito por una distribuciÃ³n Gamma.  Si TkT_kTkâ€‹ representa al tiempo transcurrido hasta la ocurrencia del kkk Ã©simo evento en un Proceso de Poisson, el evento (Tk&gt;t)\\left(T_k&gt;t\\right)(Tkâ€‹&gt;t) implica que en el intervalo (0,t)(0, t)(0,t) ocurren a lo mÃ¡s kâˆ’1k-1kâˆ’1 eventos, es decir,  P(Tk&gt;t)=P(Xtâ‰¤kâˆ’1)=âˆ‘x=0kâˆ’1(Î½t)xeâˆ’Î½tx!P\\left(T_k&gt;t\\right)=P\\left(X_t \\leq k-1\\right)=\\sum_{x=0}^{k-1} \\frac{(\\nu t)^x e^{-\\nu t}}{x !}P(Tkâ€‹&gt;t)=P(Xtâ€‹â‰¤kâˆ’1)=x=0âˆ‘kâˆ’1â€‹x!(Î½t)xeâˆ’Î½tâ€‹  Luego, su funciÃ³n de distribuciÃ³n acumulada esta dada por:  FTk(t)=1âˆ’âˆ‘x=0kâˆ’1(Î½t)xeâˆ’Î½tx!F_{T_k}(t)=1-\\sum_{x=0}^{k-1} \\frac{(\\nu t)^x e^{-\\nu t}}{x !}FTkâ€‹â€‹(t)=1âˆ’x=0âˆ‘kâˆ’1â€‹x!(Î½t)xeâˆ’Î½tâ€‹  Se puede demostrar que  fTk(t)=ddtFTk(t)=Î½kÎ“(k)tkâˆ’1eâˆ’Î½t,tâ‰¥0f_{T_k}(t)=\\frac{d}{d t} F_{T_k}(t)=\\frac{\\nu^k}{\\Gamma(k)} t^{k-1} e^{-\\nu t}, \\quad t \\geq 0fTkâ€‹â€‹(t)=dtdâ€‹FTkâ€‹â€‹(t)=Î“(k)Î½kâ€‹tkâˆ’1eâˆ’Î½t,tâ‰¥0  donde su valor esperado y varianza son  Î¼Tk=kÎ½,ÏƒTk2=kÎ½2\\mu_{T_k}=\\frac{k}{\\nu}, \\quad \\sigma_{T_k}^2=\\frac{k}{\\nu^2}Î¼Tkâ€‹â€‹=Î½kâ€‹,ÏƒTkâ€‹2â€‹=Î½2kâ€‹    Gamma trasladadaâ€‹  Una variable aleatoria XXX tiene distribuciÃ³n Gamma trasladada si su funciÃ³n de densidad esta dada por  fX(x)=Î½kÎ“(k)(xâˆ’Î³)kâˆ’1eâˆ’Î½(xâˆ’Î³),xâ‰¥Î³f_X(x)=\\frac{\\nu^k}{\\Gamma(k)}(x-\\gamma)^{k-1} e^{-\\nu(x-\\gamma)}, \\quad x \\geq \\gammafXâ€‹(x)=Î“(k)Î½kâ€‹(xâˆ’Î³)kâˆ’1eâˆ’Î½(xâˆ’Î³),xâ‰¥Î³  donde k,Î¼k, \\muk,Î¼ y Î³\\gammaÎ³ son parÃ¡metros de la distribuciÃ³n. Su valor esperado y varianza son:  Î¼X=kÎ½+Î³,ÏƒX2=kÎ½2\\mu_X=\\frac{k}{\\nu}+\\gamma, \\quad \\sigma_X^2=\\frac{k}{\\nu^2}Î¼Xâ€‹=Î½kâ€‹+Î³,ÏƒX2â€‹=Î½2kâ€‹  ","version":"Next","tagName":"h3"},{"title":"HipergeomÃ©tricaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#hipergeomÃ©trica","content":" Considere una poblaciÃ³n finita dividida en dos grupos: mmm defectuosos y Nâˆ’mN-mNâˆ’m no defectuosos.  Si se toma una muestra aleatoria de tamaÃ±o nnn al azar, la probabilidad que xxx sean defectuosos esta dada por la funciÃ³n de probabilidad:  pX(x)=(mx)(Nâˆ’mnâˆ’x)(Nn),maxâ¡{0,n+mâˆ’N}â‰¤xâ‰¤minâ¡{n,m}p_X(x)=\\dfrac{\\displaystyle\\binom{m}{x}\\binom{N-m}{n-x}}{\\displaystyle\\binom{N}{n}}, \\quad \\max \\{0, n+m-N\\} \\leq x \\leq \\min \\{n, m\\}pXâ€‹(x)=(nNâ€‹)(xmâ€‹)(nâˆ’xNâˆ’mâ€‹)â€‹,max{0,n+mâˆ’N}â‰¤xâ‰¤min{n,m}  En este caso, se dice que:  Xâˆ¼HipergeomeËŠtrica(n,N,m)X \\sim \\text {HipergeomÃ©trica}(n, N, m)Xâˆ¼HipergeomeËŠtrica(n,N,m)  El cÃ¡lculo de su valor esperado y varianza requiere un desarrollo bastante complejo cuyo resultado final es el siguiente  Î¼X=nâ‹…mN,ÏƒX2=(Nâˆ’nNâˆ’1)â‹…nâ‹…mNâ‹…(1âˆ’mN)\\mu_X=n \\cdot \\frac{m}{N}, \\quad \\sigma_X^2=\\left(\\frac{N-n}{N-1}\\right) \\cdot n \\cdot \\frac{m}{N} \\cdot\\left(1-\\frac{m}{N}\\right)Î¼Xâ€‹=nâ‹…Nmâ€‹,ÏƒX2â€‹=(Nâˆ’1Nâˆ’nâ€‹)â‹…nâ‹…Nmâ€‹â‹…(1âˆ’Nmâ€‹)  ","version":"Next","tagName":"h3"},{"title":"Betaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#beta","content":" Una variable aleatoria XXX con distribuciÃ³n Beta tiene funciÃ³n densidad  fX(x)=1B(r,q)â‹…(xâˆ’a)qâˆ’1(bâˆ’x)râˆ’1(bâˆ’a)q+râˆ’1,aâ‰¤xâ‰¤bf_X(x)=\\frac{1}{B(r, q)} \\cdot \\frac{(x-a)^{q-1}(b-x)^{r-1}}{(b-a)^{q+r-1}}, \\quad a \\leq x \\leq bfXâ€‹(x)=B(r,q)1â€‹â‹…(bâˆ’a)q+râˆ’1(xâˆ’a)qâˆ’1(bâˆ’x)râˆ’1â€‹,aâ‰¤xâ‰¤b  donde qqq y rrr son los parÃ¡metros de la distribuciÃ³n, y B(q,r)B(q, r)B(q,r) es la funciÃ³n beta dada por  B(q,r)=âˆ«01xqâˆ’1(1âˆ’x)râˆ’1dx=Î“(q)Î“(r)Î“(q+r)B(q, r)=\\int_0^1 x^{q-1}(1-x)^{r-1} d x=\\frac{\\Gamma(q) \\Gamma(r)}{\\Gamma(q+r)}B(q,r)=âˆ«01â€‹xqâˆ’1(1âˆ’x)râˆ’1dx=Î“(q+r)Î“(q)Î“(r)â€‹  El valor esperado y la varianza son:  Î¼X=a+q(q+r)(bâˆ’a)ÏƒX2=qr(bâˆ’a)2(q+r)2(q+r+1)\\mu_X=a+\\frac{q}{(q+r)}(b-a) \\quad \\sigma_X^2=\\frac{q r(b-a)^2}{(q+r)^2(q+r+1)}Î¼Xâ€‹=a+(q+r)qâ€‹(bâˆ’a)ÏƒX2â€‹=(q+r)2(q+r+1)qr(bâˆ’a)2â€‹    ","version":"Next","tagName":"h3"},{"title":"Weibullâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#weibull","content":" Si Tâˆ¼Weibullâ¡(Î·,Î²)T \\sim \\operatorname{Weibull}(\\eta, \\beta)Tâˆ¼Weibull(Î·,Î²), se tiene que  FT(t)=1âˆ’expâ¡[âˆ’(tÎ·)Î²]fT(t)=Î²Î·(tÎ·)Î²âˆ’1expâ¡[âˆ’(tÎ·)Î²],t&gt;0\\begin{aligned} F_T(t) &amp; =1-\\exp \\left[-\\left(\\frac{t}{\\eta}\\right)^\\beta\\right] \\\\ f_T(t) &amp; =\\frac{\\beta}{\\eta}\\left(\\frac{t}{\\eta}\\right)^{\\beta-1} \\exp \\left[-\\left(\\frac{t}{\\eta}\\right)^\\beta\\right], \\quad t&gt;0 \\end{aligned}FTâ€‹(t)fTâ€‹(t)â€‹=1âˆ’exp[âˆ’(Î·tâ€‹)Î²]=Î·Î²â€‹(Î·tâ€‹)Î²âˆ’1exp[âˆ’(Î·tâ€‹)Î²],t&gt;0â€‹  Con Î²&gt;0\\beta&gt;0Î²&gt;0, es un parÃ¡metro de forma y Î·&gt;0\\eta&gt;0Î·&gt;0, es un parÃ¡metro de escala.  Si tpt_ptpâ€‹ es el percentil pÃ—100%p \\times 100 \\%pÃ—100%, entonces  lnâ¡(tp)=lnâ¡(Î·)+1Î²â‹…Î¦WeibullÂ âˆ’1(p),Î¦WeibullÂ âˆ’1(p)=lnâ¡[âˆ’lnâ¡(1âˆ’p)]\\ln \\left(t_p\\right)=\\ln (\\eta)+\\frac{1}{\\beta} \\cdot \\Phi_{\\text {Weibull }}^{-1}(p), \\quad \\Phi_{\\text {Weibull }}^{-1}(p)=\\ln [-\\ln (1-p)]ln(tpâ€‹)=ln(Î·)+Î²1â€‹â‹…Î¦WeibullÂ âˆ’1â€‹(p),Î¦WeibullÂ âˆ’1â€‹(p)=ln[âˆ’ln(1âˆ’p)]  Mientras que su mmm-Ã©simo momento estÃ¡ dado por  E(Tm)=Î·mÎ“(1+m/Î²)E\\left(T^m\\right)=\\eta^m \\Gamma(1+m / \\beta)E(Tm)=Î·mÎ“(1+m/Î²)  Luego  Î¼T=Î·Î“(1+1Î²),ÏƒT2=Î·2[Î“(1+2Î²)âˆ’Î“2(1+1Î²)]\\mu_T=\\eta \\Gamma\\left(1+\\frac{1}{\\beta}\\right), \\quad \\sigma_T^2=\\eta^2\\left[\\Gamma\\left(1+\\frac{2}{\\beta}\\right)-\\Gamma^2\\left(1+\\frac{1}{\\beta}\\right)\\right]Î¼Tâ€‹=Î·Î“(1+Î²1â€‹),ÏƒT2â€‹=Î·2[Î“(1+Î²2â€‹)âˆ’Î“2(1+Î²1â€‹)]  ","version":"Next","tagName":"h3"},{"title":"LogÃ­sticaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#logÃ­stica","content":" Si Yâˆ¼LogÄ±ËŠsticaâ¡(Î¼,Ïƒ)Y \\sim \\operatorname{LogiÌstica}(\\mu, \\sigma)Yâˆ¼LogÄ±ËŠstica(Î¼,Ïƒ), se tiene que  FY(y)=Î¦LogÄ±ËŠsticaÂ (yâˆ’Î¼Ïƒ);fY(y)=1ÏƒÏ•LogÄ±ËŠsticaÂ (yâˆ’Î¼Ïƒ),âˆ’âˆ&lt;y&lt;âˆF_Y(y)=\\Phi_{\\text {LogÃ­stica }}\\left(\\frac{y-\\mu}{\\sigma}\\right) ; \\quad f_Y(y)=\\frac{1}{\\sigma} \\phi_{\\text {LogÃ­stica }}\\left(\\frac{y-\\mu}{\\sigma}\\right), \\quad-\\infty&lt;y&lt;\\inftyFYâ€‹(y)=Î¦LogÄ±ËŠsticaÂ â€‹(Ïƒyâˆ’Î¼â€‹);fYâ€‹(y)=Ïƒ1â€‹Ï•LogÄ±ËŠsticaÂ â€‹(Ïƒyâˆ’Î¼â€‹),âˆ’âˆ&lt;y&lt;âˆ  donde  Î¦LogÄ±ËŠsticaÂ (z)=expâ¡(z)[1+expâ¡(z)]Â yÂ Ï•LogÄ±ËŠsticaÂ (z)=expâ¡(z)[1+expâ¡(z)]2\\Phi_{\\text {LogÃ­stica }}(z)=\\frac{\\exp (z)}{[1+\\exp (z)]} \\quad \\text { y } \\quad \\phi_{\\text {LogÃ­stica }}(z)=\\frac{\\exp (z)}{[1+\\exp (z)]^2}Î¦LogÄ±ËŠsticaÂ â€‹(z)=[1+exp(z)]exp(z)â€‹Â yÂ Ï•LogÄ±ËŠsticaÂ â€‹(z)=[1+exp(z)]2exp(z)â€‹  son la funciÃ³n de probabilidad y de densidad de una LogÃ­stica EstÃ¡ndar. Î¼âˆˆR\\mu \\in \\mathbb{R}Î¼âˆˆR, es un parÃ¡metro de localizaciÃ³n y Ïƒ&gt;0\\sigma&gt;0Ïƒ&gt;0, es un parÃ¡metro de escala.  Si ypy_pypâ€‹ es el percentil pÃ—100%p \\times 100 \\%pÃ—100%, entonces  yp=Î¼+ÏƒÎ¦LogÄ±ËŠsticaÂ âˆ’1(p)Â conÂ Î¦LogÄ±ËŠsticaÂ âˆ’1(p)=logâ¡(p1âˆ’p)y_p=\\mu+\\sigma \\Phi_{\\text {LogÃ­stica }}^{-1}(p) \\text { con } \\Phi_{\\text {LogÃ­stica }}^{-1}(p)=\\log \\left(\\frac{p}{1-p}\\right)ypâ€‹=Î¼+ÏƒÎ¦LogÄ±ËŠsticaÂ âˆ’1â€‹(p)Â conÂ Î¦LogÄ±ËŠsticaÂ âˆ’1â€‹(p)=log(1âˆ’ppâ€‹)  Su esperanza y varianza estÃ¡n dadas por:  Î¼Y=Î¼ÏƒY2=Ïƒ2Ï€23\\mu_Y=\\mu \\quad \\qquad \\sigma_Y^2=\\frac{\\sigma^2 \\pi^2}{3}Î¼Yâ€‹=Î¼ÏƒY2â€‹=3Ïƒ2Ï€2â€‹  ","version":"Next","tagName":"h3"},{"title":"Log-LogÃ­sticaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#log-logÃ­stica","content":" Si Tâˆ¼Log-Logisticaâ¡T \\sim \\operatorname{Log-Logistica}Tâˆ¼Log-Logistica(\\mu, \\sigma)$, se tiene que  FT(t)=Î¦LogisticaÂ (lnâ¡(t)âˆ’Î¼Ïƒ);fT(t)=1ÏƒtÏ•LogisticaÂ (lnâ¡(t)âˆ’Î¼Ïƒ)t&gt;0F_T(t)=\\Phi_{\\text {Logistica }}\\left(\\frac{\\ln (t)-\\mu}{\\sigma}\\right) ; \\quad f_T(t)=\\frac{1}{\\sigma t} \\phi_{\\text {Logistica }}\\left(\\frac{\\ln (t)-\\mu}{\\sigma}\\right) \\quad t&gt;0FTâ€‹(t)=Î¦LogisticaÂ â€‹(Ïƒln(t)âˆ’Î¼â€‹);fTâ€‹(t)=Ïƒt1â€‹Ï•LogisticaÂ â€‹(Ïƒln(t)âˆ’Î¼â€‹)t&gt;0  expâ¡(Î¼)\\exp (\\mu)exp(Î¼), es un parÃ¡metro de escala y Ïƒ&gt;0\\sigma&gt;0Ïƒ&gt;0, es un parÃ¡metro de forma.  Si tpt_ptpâ€‹ es el percentil pÃ—100%p \\times 100 \\%pÃ—100%, entonces  lnâ¡(tp)=Î¼+ÏƒÎ¦LogisticaÂ âˆ’1(p)\\ln \\left(t_p\\right)=\\mu+\\sigma \\Phi_{\\text {Logistica }}^{-1}(p)ln(tpâ€‹)=Î¼+ÏƒÎ¦LogisticaÂ âˆ’1â€‹(p)  Momentosâ€‹  Para un entero m&gt;0m&gt;0m&gt;0 se tiene que  E(Tm)=expâ¡(mÎ¼)Î“(1+mÏƒ)Î“(1âˆ’mÏƒ)E\\left(T^m\\right)=\\exp (m \\mu) \\Gamma(1+m \\sigma) \\Gamma(1-m \\sigma)E(Tm)=exp(mÎ¼)Î“(1+mÏƒ)Î“(1âˆ’mÏƒ)  El mmm-Ã©simo momento no es finito si mÏƒâ‰¥1m \\sigma \\geq 1mÏƒâ‰¥1. Para Ïƒ&lt;1\\sigma&lt;1Ïƒ&lt;1  Î¼T=expâ¡(Î¼)Î“(1+Ïƒ)Î“(1âˆ’Ïƒ)\\mu_T=\\exp (\\mu) \\Gamma(1+\\sigma) \\Gamma(1-\\sigma)Î¼Tâ€‹=exp(Î¼)Î“(1+Ïƒ)Î“(1âˆ’Ïƒ)  y para Ïƒ&lt;1/2\\sigma&lt;1 / 2Ïƒ&lt;1/2  ÏƒT2=expâ¡(2Î¼)[Î“(1+2Ïƒ)Î“(1âˆ’2Ïƒ)âˆ’Î“2(1+Ïƒ)Î“2(1âˆ’Ïƒ)]\\sigma_T^2=\\exp (2 \\mu)\\left[\\Gamma(1+2 \\sigma) \\Gamma(1-2 \\sigma)-\\Gamma^2(1+\\sigma) \\Gamma^2(1-\\sigma)\\right]ÏƒT2â€‹=exp(2Î¼)[Î“(1+2Ïƒ)Î“(1âˆ’2Ïƒ)âˆ’Î“2(1+Ïƒ)Î“2(1âˆ’Ïƒ)]  ","version":"Next","tagName":"h3"},{"title":"t-Studentâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#t-student","content":" Un variable aleatoria TTT tiene distribuciÃ³n ttt-student si su funciÃ³n de densidad estÃ¡ dada por:  fT(t)=Î“[(Î½+1)/2]Ï€Î½â€‰Î“(Î½/2)(1+t2Î½)âˆ’(Î½+1)/2,âˆ’âˆ&lt;t&lt;âˆf_T(t)=\\frac{\\Gamma[(\\nu+1) / 2]}{\\sqrt{\\pi \\nu} \\, \\Gamma(\\nu / 2)}\\left(1+\\frac{t^2}{\\nu}\\right)^{-(\\nu+1) / 2}, \\quad-\\infty&lt;t&lt;\\inftyfTâ€‹(t)=Ï€Î½â€‹Î“(Î½/2)Î“[(Î½+1)/2]â€‹(1+Î½t2â€‹)âˆ’(Î½+1)/2,âˆ’âˆ&lt;t&lt;âˆ  El valor esperado y varianza estÃ¡n dados por:  Î¼T=0\\mu_T=0Î¼Tâ€‹=0, para Î½&gt;1\\nu&gt;1Î½&gt;1.ÏƒT2=Î½Î½âˆ’2\\sigma_T^2=\\dfrac{\\nu}{\\nu-2}ÏƒT2â€‹=Î½âˆ’2Î½â€‹, para Î¼&gt;2\\mu&gt;2Î¼&gt;2.  ","version":"Next","tagName":"h3"},{"title":"Fisherâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#fisher","content":" Si Tâˆ¼Fisherâ¡(Î·,Î½)T \\sim \\operatorname{Fisher}(\\eta, \\nu)Tâˆ¼Fisher(Î·,Î½), se tiene que  fT(t)=Î“(Î·+Î½2)Î“(Î·/2)Î“(Î½/2)(Î·Î½)Î·2tÎ·2âˆ’1(Î·Î½t+1)Î·+Î½2,t&gt;0f_T(t)=\\dfrac{\\Gamma\\left(\\dfrac{\\eta+\\nu}{2}\\right)}{\\Gamma(\\eta / 2) \\Gamma(\\nu / 2)}\\left(\\dfrac{\\eta}{\\nu}\\right)^{\\tfrac{\\eta}{2}} \\dfrac{t^{\\frac{\\eta}{2}-1}}{\\left(\\dfrac{\\eta}{\\nu} t+1\\right)^{\\frac{\\eta+\\nu}{2}}}, \\quad t&gt;0fTâ€‹(t)=Î“(Î·/2)Î“(Î½/2)Î“(2Î·+Î½â€‹)â€‹(Î½Î·â€‹)2Î·â€‹(Î½Î·â€‹t+1)2Î·+Î½â€‹t2Î·â€‹âˆ’1â€‹,t&gt;0  El valor esperado y varianza estÃ¡n dados por:  Î¼T=Î½Î½âˆ’2\\mu_T=\\dfrac{\\nu}{\\nu-2}Î¼Tâ€‹=Î½âˆ’2Î½â€‹, para Î½&gt;2\\nu&gt;2Î½&gt;2.ÏƒT2=2Î½2(Î·+Î½âˆ’2)Î·(Î½âˆ’2)2(Î½âˆ’4)\\sigma_T^2=\\dfrac{2 \\nu^2(\\eta+\\nu-2)}{\\eta(\\nu-2)^2(\\nu-4)}ÏƒT2â€‹=Î·(Î½âˆ’2)2(Î½âˆ’4)2Î½2(Î·+Î½âˆ’2)â€‹, para Î½&gt;4\\nu&gt;4Î½&gt;4.  ","version":"Next","tagName":"h3"},{"title":"MÃºltiples variables aleatoriasâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#mÃºltiples-variables-aleatorias","content":" Para el par de variables aleatorias XXX e YYY se define la funciÃ³n de distribuciÃ³n de probabilidad acumulada como  FX,Y(x,y)=P(Xâ‰¤x,Yâ‰¤y)F_{X, Y}(x, y)=P(X \\leq x, Y \\leq y)FX,Yâ€‹(x,y)=P(Xâ‰¤x,Yâ‰¤y)  La cual satisface la axiomÃ¡tica fundamental de probabilidades:  FX,Y(âˆ’âˆ,âˆ’âˆ)=0.FX,Y(âˆ’âˆ,y)=0.FX,Y(x,âˆ’âˆ)=0.FX,Y(x,+âˆ)=FX(x).FX,Y(+âˆ,y)=FY(y).FX,Y(+âˆ,+âˆ)=1.\\begin{aligned} &amp; F_{X, Y}(-\\infty,-\\infty)=0 . \\\\ &amp; F_{X, Y}(-\\infty, y)=0 . \\\\ &amp; F_{X, Y}(x,-\\infty)=0 . \\\\ &amp; F_{X, Y}(x,+\\infty)=F_X(x) . \\\\ &amp; F_{X, Y}(+\\infty, y)=F_Y(y) . \\\\ &amp; F_{X, Y}(+\\infty,+\\infty)=1 . \\end{aligned}â€‹FX,Yâ€‹(âˆ’âˆ,âˆ’âˆ)=0.FX,Yâ€‹(âˆ’âˆ,y)=0.FX,Yâ€‹(x,âˆ’âˆ)=0.FX,Yâ€‹(x,+âˆ)=FXâ€‹(x).FX,Yâ€‹(+âˆ,y)=FYâ€‹(y).FX,Yâ€‹(+âˆ,+âˆ)=1.â€‹  ","version":"Next","tagName":"h2"},{"title":"DistribuciÃ³n de probabilidad conjuntaâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#distribuciÃ³n-de-probabilidad-conjunta","content":" Si las variables aleatorias XXX e YYY son discretas, la funciÃ³n de distribuciÃ³n de probabilidad conjunta es  pX,Y(x,y)=P(X=x,Y=y)p_{X, Y}(x, y)=P(X=x, Y=y)pX,Yâ€‹(x,y)=P(X=x,Y=y)  siendo su funciÃ³n de distribuciÃ³n de probabilidad acumulada igual a  FX,Y(x,y)=P(Xâ‰¤x,Yâ‰¤y)=âˆ‘xiâ‰¤xâˆ‘yjâ‰¤yP(X=xi,Y=yj)\\begin{aligned} &amp; \\quad F_{X, Y}(x, y)=P(X \\leq x, Y \\leq y)=\\sum_{x_i \\leq x} \\sum_{y_j \\leq y} P\\left(X=x_i, Y=y_j\\right) \\end{aligned}â€‹FX,Yâ€‹(x,y)=P(Xâ‰¤x,Yâ‰¤y)=xiâ€‹â‰¤xâˆ‘â€‹yjâ€‹â‰¤yâˆ‘â€‹P(X=xiâ€‹,Y=yjâ€‹)â€‹  con (xi,yj)âˆˆÎ˜X,Y\\left(x_i, y_j\\right) \\in \\Theta_{X, Y}(xiâ€‹,yjâ€‹)âˆˆÎ˜X,Yâ€‹.  Ahora, si las variables aleatorias XXX e YYY son continuas, la funciÃ³n de de densidad de probabilidad conjunta se define como:  fX,Y(x,y)dxdy=P(x&lt;Xâ‰¤x+dx,y&lt;Yâ‰¤y+dy)f_{X, Y}(x, y) d x d y=P(x&lt;X \\leq x+d x, y&lt;Y \\leq y+d y)fX,Yâ€‹(x,y)dxdy=P(x&lt;Xâ‰¤x+dx,y&lt;Yâ‰¤y+dy)  Entonces,  FX,Y(x,y)=âˆ«âˆ’âˆxâˆ«âˆ’âˆyfX,Y(u,v)dvduF_{X, Y}(x, y)=\\int_{-\\infty}^x \\int_{-\\infty}^y f_{X, Y}(u, v) d v d uFX,Yâ€‹(x,y)=âˆ«âˆ’âˆxâ€‹âˆ«âˆ’âˆyâ€‹fX,Yâ€‹(u,v)dvdu  Si las derivadas parciales existen, entonces  fX,Y(x,y)=âˆ‚2âˆ‚xâˆ‚yFX,Y(x,y)f_{X, Y}(x, y)=\\frac{\\partial^2}{\\partial x \\partial y} F_{X, Y}(x, y)fX,Yâ€‹(x,y)=âˆ‚xâˆ‚yâˆ‚2â€‹FX,Yâ€‹(x,y)  TambiÃ©n, se puede observar que la siguiente probabilidad puede ser obtenida como  P(a&lt;Xâ‰¤b,c&lt;yâ‰¤d)=âˆ«abâˆ«cdfX,Y(u,v)dudvP(a&lt;X \\leq b, c&lt;y \\leq d)=\\int_a^b \\int_c^d f_{X, Y}(u, v) d u d vP(a&lt;Xâ‰¤b,c&lt;yâ‰¤d)=âˆ«abâ€‹âˆ«cdâ€‹fX,Yâ€‹(u,v)dudv  que representa el volumen bajo la superficie fX,Y(x,y)f_{X, Y}(x, y)fX,Yâ€‹(x,y) como se muestra en la figura.  Volumen bajo la superficie  ","version":"Next","tagName":"h3"},{"title":"Distribuciones marginales y condicionalesâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#distribuciones-marginales-y-condicionales","content":" Variables discretasâ€‹  Para variables aleatorias discretas XXX e YYY, la probabilidad de (X=x)(X=x)(X=x) puede depender de los valores que puede tomar YYY (viceversa).  Con base a lo visto en probabilidades, se define la funciÃ³n de distribuciÃ³n de probabilidad condicional como:  pXâˆ£Y=y(x)=P(X=xâˆ£Y=y)=pX,Y(x,y)pY(y),pY(y)&gt;0p_{X \\mid Y=y}(x)=P(X=x \\mid Y=y)=\\frac{p_{X, Y}(x, y)}{p_Y(y)}, \\quad p_Y(y)&gt;0pXâˆ£Y=yâ€‹(x)=P(X=xâˆ£Y=y)=pYâ€‹(y)pX,Yâ€‹(x,y)â€‹,pYâ€‹(y)&gt;0  De manera similar, se tiene que  pYâˆ£X=x(y)=P(Y=yâˆ£X=x)=pX,Y(x,y)pX(x),pX(x)&gt;0p_{Y \\mid X=x}(y)=P(Y=y \\mid X=x)=\\frac{p_{X, Y}(x, y)}{p_X(x)}, \\quad p_X(x)&gt;0pYâˆ£X=xâ€‹(y)=P(Y=yâˆ£X=x)=pXâ€‹(x)pX,Yâ€‹(x,y)â€‹,pXâ€‹(x)&gt;0  La distribuciÃ³n marginal de una variable aleatoria se puede obtener aplicando el teorema de probabilidades totales.  Para determinar la distribuciÃ³n marginal de X,pX(x)X, p_X(x)X,pXâ€‹(x), tenemos que  pX(x)=âˆ‘yâˆˆÎ˜YpXâˆ£Y=y(x)â‹…pY(y)=âˆ‘yâˆˆÎ˜YpX,Y(x,y)\\begin{aligned} p_X(x) &amp; =\\sum_{y \\in \\Theta_Y} p_{X \\mid Y=y}(x) \\cdot p_Y(y) \\\\ &amp; =\\sum_{y \\in \\Theta_Y} p_{X, Y}(x, y) \\end{aligned}pXâ€‹(x)â€‹=yâˆˆÎ˜Yâ€‹âˆ‘â€‹pXâˆ£Y=yâ€‹(x)â‹…pYâ€‹(y)=yâˆˆÎ˜Yâ€‹âˆ‘â€‹pX,Yâ€‹(x,y)â€‹  De la misma forma se tiene que  pY(y)=âˆ‘xâˆˆÎ˜XpX,Y(x,y)p_Y(y)=\\sum_{x \\in \\Theta_X} p_{X, Y}(x, y)pYâ€‹(y)=xâˆˆÎ˜Xâ€‹âˆ‘â€‹pX,Yâ€‹(x,y)  Variables continuasâ€‹  En el caso que ambas sean variables aleatorias continuas se define la funciÃ³n de densidad condicional de XXX dado que Y=yY=yY=y como  fXâˆ£Y=y(x)=fX,Y(x,y)fY(y)fY(y)&gt;0f_{X \\mid Y=y}(x)=\\frac{f_{X, Y}(x, y)}{f_Y(y)} \\quad f_Y(y)&gt;0fXâˆ£Y=yâ€‹(x)=fYâ€‹(y)fX,Yâ€‹(x,y)â€‹fYâ€‹(y)&gt;0  De manera similar se tiene que  fYâˆ£X=x(y)=fX,Y(x,y)fX(x)fX(x)&gt;0f_{Y \\mid X=x}(y)=\\frac{f_{X, Y}(x, y)}{f_X(x)} \\quad f_X(x)&gt;0fYâˆ£X=xâ€‹(y)=fXâ€‹(x)fX,Yâ€‹(x,y)â€‹fXâ€‹(x)&gt;0  Las respectivas marginales se obtienen como sigue:  fX(x)=âˆ«âˆ’âˆâˆfX,Y(x,y)dyfY(y)=âˆ«âˆ’âˆâˆfX,Y(x,y)dx\\begin{aligned} &amp; f_X(x)=\\int_{-\\infty}^{\\infty} f_{X, Y}(x, y) d y \\\\ &amp; f_Y(y)=\\int_{-\\infty}^{\\infty} f_{X, Y}(x, y) d x \\end{aligned}â€‹fXâ€‹(x)=âˆ«âˆ’âˆâˆâ€‹fX,Yâ€‹(x,y)dyfYâ€‹(y)=âˆ«âˆ’âˆâˆâ€‹fX,Yâ€‹(x,y)dxâ€‹  Caso mixtoâ€‹  En el caso mixto, supongamos XXX discreta e YYY continua, el calculo de las respectivas marginales es  pX(x)=âˆ«âˆ’âˆâˆpXâˆ£Y=y(x)â‹…fY(y)dyfY(y)=âˆ‘xâˆˆÎ˜XfYâˆ£X=x(y)â‹…pX(x)\\begin{aligned} &amp; p_X(x)=\\int_{-\\infty}^{\\infty} p_{X \\mid Y=y}(x) \\cdot f_Y(y) d y \\\\ &amp; f_Y(y)=\\sum_{x \\in \\Theta_X} f_{Y \\mid X=x}(y) \\cdot p_X(x) \\end{aligned}â€‹pXâ€‹(x)=âˆ«âˆ’âˆâˆâ€‹pXâˆ£Y=yâ€‹(x)â‹…fYâ€‹(y)dyfYâ€‹(y)=xâˆˆÎ˜Xâ€‹âˆ‘â€‹fYâˆ£X=xâ€‹(y)â‹…pXâ€‹(x)â€‹  Si ambas variables aleatorias son independientes, entonces se tiene que  pX,Y(x,y)=pX(x)â‹…pY(y)fX,Y(x,y)=fX(x)â‹…fY(y)\\begin{aligned} p_{X, Y}(x, y) &amp; =p_X(x) \\cdot p_Y(y) \\\\ f_{X, Y}(x, y) &amp; =f_X(x) \\cdot f_Y(y) \\end{aligned}pX,Yâ€‹(x,y)fX,Yâ€‹(x,y)â€‹=pXâ€‹(x)â‹…pYâ€‹(y)=fXâ€‹(x)â‹…fYâ€‹(y)â€‹  Ejemplo: Determinar distribuciÃ³n de una variable Tenemos que Xâˆ¼Poissonâ¡(Î½)Â yÂ Yâˆ£X=xâˆ¼Binomialâ¡(x,p)X \\sim \\operatorname{Poisson}(\\nu) \\text { y } Y \\mid X=x \\sim \\operatorname{Binomial}(x, p)Xâˆ¼Poisson(Î½)Â yÂ Yâˆ£X=xâˆ¼Binomial(x,p) Luego pX,Y(x,y)=pYâˆ£X=x(y)â‹…pX(x)=(xy)py(1âˆ’p)xâˆ’yâ‹…Î½xeâˆ’Î½x!\\begin{aligned} p_{X, Y}(x, y) &amp; =p_{Y \\mid X=x}(y) \\cdot p_X(x) \\\\ &amp; =\\left(\\begin{array}{c} x \\\\ y \\end{array}\\right) p^y(1-p)^{x-y} \\cdot \\frac{\\nu^x e^{-\\nu}}{x !} \\end{aligned}pX,Yâ€‹(x,y)â€‹=pYâˆ£X=xâ€‹(y)â‹…pXâ€‹(x)=(xyâ€‹)py(1âˆ’p)xâˆ’yâ‹…x!Î½xeâˆ’Î½â€‹â€‹Î˜X,Y={(x,y)âˆ£xâˆˆN0,yâˆˆN0,yâ‰¤x}.Â \\Theta_{X, Y}=\\left\\{(x, y) \\mid x \\in \\mathbb{N}_0, y \\in \\mathbb{N}_0, y \\leq x\\right\\} \\text {. }Î˜X,Yâ€‹={(x,y)âˆ£xâˆˆN0â€‹,yâˆˆN0â€‹,yâ‰¤x}. Por probabilidades totales se tiene que Yâˆ¼Poissonâ¡(Î½p)Y \\sim \\operatorname{Poisson}(\\nu p)Yâˆ¼Poisson(Î½p)  ","version":"Next","tagName":"h3"},{"title":"Covarianza y correlaciÃ³nâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#covarianza-y-correlaciÃ³n","content":" Cuando hay dos variables aleatorias XXX e YYY, puede haber una relaciÃ³n entre ellas.  En particular, la presencia o ausencia de relaciÃ³n estadÃ­stica lineal se determina observando el primer momento conjunto de XXX e YYY definido como  E(XY)=âˆ«âˆ’âˆâˆâˆ«âˆ’âˆâˆxyâ‹…fX,Y(x,y)dxdyE(X Y)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y \\cdot f_{X, Y}(x, y) d x d yE(XY)=âˆ«âˆ’âˆâˆâ€‹âˆ«âˆ’âˆâˆâ€‹xyâ‹…fX,Yâ€‹(x,y)dxdy  Si XXX e YYY son estadÃ­sticamente independientes, entonces  E(XY)=âˆ«âˆ’âˆâˆâˆ«âˆ’âˆâˆxyâ‹…fX(x)â‹…fY(y)dxdy=E(X)â‹…E(Y)E(X Y)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y \\cdot f_X(x) \\cdot f_Y(y) d x d y=E(X) \\cdot E(Y)E(XY)=âˆ«âˆ’âˆâˆâ€‹âˆ«âˆ’âˆâˆâ€‹xyâ‹…fXâ€‹(x)â‹…fYâ€‹(y)dxdy=E(X)â‹…E(Y)  La covarianza corresponde al segundo momento central y se define como:  Covâ¡(X,Y)=E[(Xâˆ’Î¼X)(Yâˆ’Î¼Y)]=E(Xâ‹…Y)âˆ’Î¼Xâ‹…Î¼Y\\operatorname{Cov}(X, Y)=E\\left[\\left(X-\\mu_X\\right)\\left(Y-\\mu_Y\\right)\\right]=E(X \\cdot Y)-\\mu_X \\cdot \\mu_YCov(X,Y)=E[(Xâˆ’Î¼Xâ€‹)(Yâˆ’Î¼Yâ€‹)]=E(Xâ‹…Y)âˆ’Î¼Xâ€‹â‹…Î¼Yâ€‹  Si XXX e YYY son estadÃ­sticamente independientes, entonces  Covâ¡(X,Y)=0\\operatorname{Cov}(X, Y)=0Cov(X,Y)=0  Nota El significado fÃ­sico de la covarianza se puede inferir de la ecuaciÃ³n: Si Covâ¡(X,Y)\\operatorname{Cov}(X, Y)Cov(X,Y) es grande y positiva, los valores de XXX e YYY tienden a ser grandes (o pequeÃ±os) en relaciÃ³n a sus respectivos medias.Si Covâ¡(X,Y)\\operatorname{Cov}(X, Y)Cov(X,Y) es grande y negativo, los valores de XXX tienden a ser grandes con respecto a su media, mientras que los de YYY tienden a ser pequeÃ±os y viceversa.Si Covâ¡(X,Y)\\operatorname{Cov}(X, Y)Cov(X,Y) es pequeÃ±a o cero, la relaciÃ³n (lineal) entre los valores de XXX e YYY es poca o nula, o bien la relaciÃ³n es no lineal.  La covarianza mide el grado de asociaciÃ³n lineal entre dos variables, pero es preferible su normalizaciÃ³n llamada correlaciÃ³n para poder cuantificar la magnitud de la relaciÃ³n.  La correlaciÃ³n esta definida como:  Corâ¡(X,Y)=Covâ¡(X,Y)ÏƒXâ‹…ÏƒY\\operatorname{Cor}(X, Y)=\\frac{\\operatorname{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}Cor(X,Y)=ÏƒXâ€‹â‹…ÏƒYâ€‹Cov(X,Y)â€‹  Este coeficiente toma valores en el intervalo (âˆ’1,1)(-1,1)(âˆ’1,1).  ","version":"Next","tagName":"h3"},{"title":"Esperanza condicionalâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#esperanza-condicional","content":" El valor esperado de una variable aleatoria YYY condicionado a la realizaciÃ³n xxx de una variable aleatoria XXX esta dado por  E(Yâˆ£X=x)={âˆ‘yâˆˆÎ˜Yâˆ£X=xyâ‹…P(Y=yâˆ£X=x),Â casoÂ discretoÂ âˆ«yâˆˆÎ˜Yâˆ£X=xyâ‹…fYâˆ£X=x(y)dy,Â casoÂ continuoÂ \\mathrm{E}(Y \\mid X=x)= \\begin{cases} \\displaystyle\\sum_{y \\in \\Theta_{Y \\mid X=x}} y \\cdot P(Y=y \\mid X=x), &amp; \\text { caso discreto } \\\\[25pt] \\displaystyle\\int_{y \\in \\Theta_{Y \\mid X=x}} y \\cdot f_{Y \\mid X=x}(y) d y, &amp; \\text { caso continuo } \\end{cases}E(Yâˆ£X=x)=â©â¨â§â€‹yâˆˆÎ˜Yâˆ£X=xâ€‹âˆ‘â€‹yâ‹…P(Y=yâˆ£X=x),âˆ«yâˆˆÎ˜Yâˆ£X=xâ€‹â€‹yâ‹…fYâˆ£X=xâ€‹(y)dy,â€‹Â casoÂ discreto casoÂ continuoÂ â€‹  Por otra parte, para una funciÃ³n de YYY, llamemos h(Y)h(Y)h(Y), el valor esperado condicional esta dado por  E[h(Y)âˆ£X=x]={âˆ‘yâˆˆÎ˜Yâˆ£X=xh(y)â‹…P(Y=yâˆ£X=x),Â casoÂ discretoÂ âˆ«yâˆˆÎ˜Yâˆ£X=xh(y)â‹…fYâˆ£X=x(y)dy,Â casoÂ continuoÂ \\mathrm{E}[h(Y) \\mid X=x]= \\begin{cases} \\displaystyle\\sum_{y \\in \\Theta_{Y \\mid X=x}} h(y) \\cdot P(Y=y \\mid X=x), &amp; \\text { caso discreto } \\\\[25pt] \\displaystyle\\int_{y \\in \\Theta_{Y \\mid X=x}} h(y) \\cdot f_{Y \\mid X=x}(y) d y, &amp; \\text { caso continuo }\\end{cases}E[h(Y)âˆ£X=x]=â©â¨â§â€‹yâˆˆÎ˜Yâˆ£X=xâ€‹âˆ‘â€‹h(y)â‹…P(Y=yâˆ£X=x),âˆ«yâˆˆÎ˜Yâˆ£X=xâ€‹â€‹h(y)â‹…fYâˆ£X=xâ€‹(y)dy,â€‹Â casoÂ discreto casoÂ continuoÂ â€‹  ","version":"Next","tagName":"h3"},{"title":"Teorema de probabilidades totales para el valor esperadoâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#teorema-de-probabilidades-totales-para-el-valor-esperado","content":" Preparate (mentalmente) para el teorema de probabilidades totales para el valor esperado condicional:  E(X)={âˆ‘yâˆˆÎ˜Y[âˆ‘xâˆˆÎ˜Xâˆ£Y=yxâ‹…pXâˆ£Y=y(x)]pY(y),Â CasoÂ Discreto-DiscretoÂ âˆ«yâˆˆÎ˜Y[âˆ«xâˆˆÎ˜Xâˆ£Y=yxâ‹…fXâˆ£Y=y(x)dx]fY(y)dy,Â CasoÂ Continuo-ContinuoÂ âˆ«yâˆˆÎ˜Y[âˆ‘xâˆˆÎ˜Xâˆ£Y=yxâ‹…pXâˆ£Y=y(x)]fY(y)dy,Â CasoÂ Discreto-ContinuoÂ âˆ‘yâˆˆÎ˜Y[âˆ«xâˆˆÎ˜Xâˆ£Y=yxâ‹…fXâˆ£Y=y(x)dx]pY(y),Â CasoÂ Continuo-DiscretoÂ E(X)= \\begin{cases} \\displaystyle\\sum_{y \\in \\Theta_Y}\\left[\\sum_{x \\in \\Theta_{X \\mid Y=y}} x \\cdot p_{X \\mid Y=y}(x)\\right] p_Y(y), &amp; \\text { Caso Discreto-Discreto } \\\\[30pt] \\displaystyle\\int_{y \\in \\Theta_Y}\\left[\\int_{x \\in \\Theta_{X \\mid Y=y}} x \\cdot f_{X \\mid Y=y}(x) d x\\right] f_Y(y) d y, &amp; \\text { Caso Continuo-Continuo } \\\\[30pt] \\displaystyle\\int_{y \\in \\Theta_Y}\\left[\\sum_{x \\in \\Theta_{X \\mid Y=y}} x \\cdot p_{X \\mid Y=y}(x)\\right] f_Y(y) d y, &amp; \\text { Caso Discreto-Continuo } \\\\[30pt] \\displaystyle\\sum_{y \\in \\Theta_Y}\\left[\\int_{x \\in \\Theta_{X \\mid Y=y}} x \\cdot f_{X \\mid Y=y}(x) d x\\right] p_Y(y), &amp; \\text { Caso Continuo-Discreto } \\end{cases}E(X)=â©â¨â§â€‹yâˆˆÎ˜Yâ€‹âˆ‘â€‹â€‹xâˆˆÎ˜Xâˆ£Y=yâ€‹âˆ‘â€‹xâ‹…pXâˆ£Y=yâ€‹(x)â€‹pYâ€‹(y),âˆ«yâˆˆÎ˜Yâ€‹â€‹[âˆ«xâˆˆÎ˜Xâˆ£Y=yâ€‹â€‹xâ‹…fXâˆ£Y=yâ€‹(x)dx]fYâ€‹(y)dy,âˆ«yâˆˆÎ˜Yâ€‹â€‹â€‹xâˆˆÎ˜Xâˆ£Y=yâ€‹âˆ‘â€‹xâ‹…pXâˆ£Y=yâ€‹(x)â€‹fYâ€‹(y)dy,yâˆˆÎ˜Yâ€‹âˆ‘â€‹[âˆ«xâˆˆÎ˜Xâˆ£Y=yâ€‹â€‹xâ‹…fXâˆ£Y=yâ€‹(x)dx]pYâ€‹(y),â€‹Â CasoÂ Discreto-Discreto CasoÂ Continuo-Continuo CasoÂ Discreto-Continuo CasoÂ Continuo-DiscretoÂ â€‹  ","version":"Next","tagName":"h3"},{"title":"Teorema de las esperanzas iteradasâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#teorema-de-las-esperanzas-iteradas","content":" El teorema de las esperanzas iteradas es una generalizaciÃ³n del teorema de probabilidades totales para el valor esperado condicional.  E(Y)=E[E(Yâˆ£X)]Varâ¡(Y)=Varâ¡[E(Yâˆ£X)]+E[Varâ¡(Yâˆ£X)]\\begin{aligned} \\mathrm{E}(Y)&amp;=\\mathrm{E}[\\mathrm{E}(Y \\mid X)]\\\\ \\operatorname{Var}(Y)&amp;=\\operatorname{Var}[\\mathrm{E}(Y \\mid X)]+\\mathrm{E}[\\operatorname{Var}(Y \\mid X)] \\end{aligned}E(Y)Var(Y)â€‹=E[E(Yâˆ£X)]=Var[E(Yâˆ£X)]+E[Var(Yâˆ£X)]â€‹  ","version":"Next","tagName":"h3"},{"title":"Mejor predictorâ€‹","type":1,"pageTitle":"Modelos analÃ­ticos de fenÃ³menos aleatorios","url":"/apuntes-fundamentals/docs/MatemÃ¡ticas/Probabilidades y EstadÃ­stica/modelos_analiticos#mejor-predictor","content":" Predecir el valor de una variable aleatoria a partir de otra es un problema comÃºn en estadÃ­stica. Consideremos primero la siguiente situaciÃ³n: &quot;Predecir la realizaciÃ³n de una variable aleatoria YYY&quot;. El &quot;mejor&quot; valor ccc para predecir la realizaciÃ³n de YYY se puede obtener minimizando el error cuadrÃ¡tico medio definido como  ECM=E[(Yâˆ’c)2]\\mathrm{ECM}=\\mathrm{E}\\left[(Y-c)^2\\right]ECM=E[(Yâˆ’c)2]  Nota La constante ccc que minimiza el ECM\\mathrm{ECM}ECM es E(Y)\\mathrm{E}(Y)E(Y).  Si ahora queremos predecir YYY basado en una funciÃ³n de una variable aleatoria XXX, llamemos h(X)h(X)h(X), que minimice el error cuadrÃ¡tico medio definido como  ECM=E{[Yâˆ’h(X)]2}=E(E{[Yâˆ’h(X)]2âˆ£X})\\begin{aligned} \\mathrm{ECM} &amp; =\\mathrm{E}\\left\\{[Y-h(X)]^2\\right\\} \\\\ &amp; =\\mathrm{E}\\left(\\mathrm{E}\\left\\{[Y-h(X)]^2 \\mid X\\right\\}\\right) \\end{aligned}ECMâ€‹=E{[Yâˆ’h(X)]2}=E(E{[Yâˆ’h(X)]2âˆ£X})â€‹  Entonces, la funciÃ³n h(X)h(X)h(X) que minimiza ECM necesariamente debe corresponder a E(Yâˆ£X)\\mathrm{E}(Y \\mid X)E(Yâˆ£X).  Por ejemplo, si XXX e YYY distribuyen conjuntamente segÃºn una Normal bivariada, entonces el mejor predictor YYY basado en XXX es una funciÃ³n lineal dada por  E(Yâˆ£X)=(Î¼Yâˆ’Î¼XÏÏƒYÏƒX)+XÏÏƒYÏƒX\\mathrm{E}(Y \\mid X)=\\left(\\mu_Y-\\mu_X \\frac{\\rho \\sigma_Y}{\\sigma_X}\\right)+X \\frac{\\rho \\sigma_Y}{\\sigma_X}E(Yâˆ£X)=(Î¼Yâ€‹âˆ’Î¼Xâ€‹ÏƒXâ€‹ÏÏƒYâ€‹â€‹)+XÏƒXâ€‹ÏÏƒYâ€‹â€‹ ","version":"Next","tagName":"h3"}],"options":{"languages":["en","es"],"id":"default"}}