import Image from "@site/src/components/Image";
import MDXDetails from "@site/src/components/MDXDetails";

# lgebra lineal

## Ecuaciones lineales en 谩lgebra lineal

### Operaciones elementales de fila

Las operaciones elementales de fila son tres operaciones que se pueden realizar en las filas de una matriz:

1. (Reemplazo) Sustituir una fila por la suma de s铆 misma y un m煤ltiplo de otra fila.
2. (Intercambio) Intercambiar dos filas.
3. (Escalamiento) Multiplicar todos los elementos de una fila por una constante diferente de cero.

### Forma escalonada de una matriz

Una matriz rectangular est谩 en **forma escalonada** (o **forma escalonada por filas**) si tiene las siguientes tres propiedades:

1. Todas las filas diferentes de cero est谩n arriba de las filas que solo contienen ceros.
2. Cada entrada principal de una fila est谩 en una columna a la derecha de la entrada principal de la fila superior.
3. En una columna todas las entradas debajo de la entrada principal son ceros.

Si una matriz de forma escalonada satisface las siguientes condiciones adicionales, entonces est谩 en **forma escalonada reducida** (o forma escalonada reducida por filas):

4. La entrada principal en cada fila diferente de cero es $1$.
5. Cada entrada principal $1$ es la 煤nica entrada distinta de cero en su columna.

Por ejemplo, las matrices

$$
\left[\begin{array}{rrrc}
2 & -3 & 2 & 1 \\
0 & 1 & -4 & 8 \\
0 & 0 & 0 & 5 / 2
\end{array}\right] \text { y }\left[\begin{array}{llll}
1 & 0 & 0 & 29 \\
0 & 1 & 0 & 16 \\
0 & 0 & 1 & 3
\end{array}\right]
$$

est谩n en forma escalonada y forma escalonada reducida, respectivamente.

De forma general, las matrices en forma escalonada pueden tener entradas principales ($\small\blacksquare$) con cualquier valor diferente de cero, pero las entradas con asterisco ($*$) pueden tener cualquier valor, incluyendo cero.

$$
\left[\begin{array}{llll}
\small\blacksquare & * & * & * \\
0 & \small\blacksquare & * & * \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

En cambio, las matrices en forma escalonada reducida tienen entradas principales iguales a $1$ y hay ceros abajo y _arriba_ de cada entrada principal $1$.

$$
\left[\begin{array}{llll}
1 & 0 & * & * \\
0 & 1 & * & * \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right]
$$

### Unicidad de la forma escalonada reducida

:::tip[Teorema]
Cada matriz es equivalente por filas a una, y solo a una, matriz escalonada reducida.
:::

Si una matriz $A$ es equivalente por filas a una matriz escalonada $U$, entonces $U$ se llama una forma escalonada (o una forma escalonada por filas) de $A$; si $U$ est谩 en forma escalonada reducida, entonces $U$ es la forma escalonada reducida de $A$.

### Posiciones pivote

Una posici贸n pivote en una matriz $A$ es una ubicaci贸n en $A$ que corresponde a un $1$ principal en la forma escalonada reducida de $A$. Una columna pivote es una columna de $A$ que contiene una posici贸n pivote.

En los ejemplos anteriores, los cuadrados ($\small\blacksquare$) identifican las posiciones pivote.

### Algoritmo de reducci贸n por filas

El algoritmo que sigue consta de cuatro pasos y produce una matriz en forma escalonada. Un quinto paso da por resultado una matriz en forma escalonada reducida.

1. Se inicia con la columna diferente de cero del extremo izquierdo. Esta es una columna pivote. La posici贸n pivote se ubica en la parte superior.
2. Seleccione como pivote una entrada diferente de cero en la columna pivote. Si es necesario, intercambie filas para mover esta entrada a la posici贸n pivote.
3. Utilice operaciones de remplazo de filas para crear ceros en todas las posiciones ubicadas debajo del pivote.
4. Cubra (o ignore) la fila que contiene la posici贸n pivote y cubra todas las filas, si las hay, por encima de esta. Aplique los pasos 1 a 3 a la submatriz restante. Repita el proceso hasta que no haya filas diferentes de cero por modificar.
5. Empezando con la posici贸n pivote del extremo derecho y trabajando hacia arriba y hacia la izquierda, genere ceros arriba de cada pivote. Si un pivote no es 1 , convi茅rtalo en 1 mediante una operaci贸n de escalamiento.

La combinaci贸n de los pasos 1 a 4 se conoce como **fase progresiva** del algoritmo de reducci贸n por filas. El paso 5, que produce la 煤nica forma escalonada reducida, se conoce como **fase regresiva**.

<MDXDetails>
<summary>Ejemplo: Algoritmo de reducci贸n por filas</summary>

Transforme la siguiente matriz a la forma escalonada, y luego a la forma escalonada reducida:

$$
\left[\begin{array}{rrrrrr}
0 & 3 & -6 & 6 & 4 & -5 \\
3 & -7 & 8 & -5 & 8 & 9 \\
3 & -9 & 12 & -9 & 6 & 15
\end{array}\right]
$$

**Paso 1**: Elegimos el pivote.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_1.png"
  alt="Paso 1"
  width="30%"
/>

**Paso 2**: Intercambiamos las filas 1 y 3. (O bien, tambi茅n se podr铆an intercambiar las filas 1 y 2).

<Image
  src="matematicas/algebra_lineal/ejemplo_1_2.png"
  alt="Paso 2"
  width="30%"
/>

**Paso 3**: Como paso preliminar, se podr铆a dividir la fila superior entre el pivote, $3$. Pero con dos n煤meros $3$ en la columna 1, esto es tan f谩cil como sumar la fila 1 multiplicada por $-1$ a la fila 2.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_3.png"
  alt="Paso 3"
  width="30%"
/>

**Paso 4**: Con la fila 1 cubierta, el paso 1 muestra que la columna 2 es la pr贸xima columna pivote; para el paso 2, seleccione como pivote la entrada "superior" en esa columna.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_4.png"
  alt="Paso 4"
  width="30%"
/>

En el paso 3, se podr铆a insertar un paso adicional de dividir la fila "superior" de la submatriz entre el pivote, $2$. En vez de ello, se suma la fila "superior" multiplicada por $-3 / 2$ a la fila de abajo. Esto produce

<Image
  src="matematicas/algebra_lineal/ejemplo_1_5.png"
  alt="Paso 3"
  width="30%"
/>

Para el paso 4, cuando se cubre la fila que contiene la segunda posici贸n pivote, se obtiene una nueva submatriz con una sola fila:

<Image
  src="matematicas/algebra_lineal/ejemplo_1_6.png"
  alt="Paso 4"
  width="30%"
/>

Los pasos 1 a 3 no necesitan aplicarse para esta submatriz, pues ya se ha alcanzado una forma escalonada para la matriz completa. Si se desea la forma escalonada reducida, se efect煤a un paso m谩s.

**Paso 5**: El pivote del extremo derecho est谩 en la fila 3. Genere ceros sobre 茅l, sumando m煤ltiplos adecuados de la fila 3 a las filas 1 y 2.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_7.png"
  alt="Paso 5"
  width="50%"
/>

El siguiente pivote se encuentra en la fila 2 . Se escala esta fila dividi茅ndola entre el pivote.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_8.png"
  alt="Paso 5"
  width="50%"
/>

Cree un cero en la columna 2 sumando la fila 2 multiplicada por 9 a la fila 1.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_9.png"
  alt="Paso 5"
  width="50%"
/>

Finalmente, escale la fila 1 dividi茅ndola entre el pivote, 3.

<Image
  src="matematicas/algebra_lineal/ejemplo_1_10.png"
  alt="Paso 5"
  width="50%"
/>

Esta es la forma escalonada reducida de la matriz original.

</MDXDetails>

### Soluciones de sistemas lineales

El algoritmo de reducci贸n por filas conduce directamente a una descripci贸n expl铆cita del conjunto soluci贸n de un sistema lineal cuando se aplica a la matriz aumentada del sistema.

Suponga, por ejemplo, que la matriz aumentada de un sistema lineal se transform贸 a la forma escalonada reducida equivalente

$$
\left[\begin{array}{rrrr}1 & 0 & -5 & 1 \\ 0 & 1 & 1 & 4 \\ 0 & 0 & 0 & 0\end{array}\right]
$$

Existen tres variables porque la matriz aumentada tiene cuatro columnas. El sistema de ecuaciones asociado es

$$
\begin{aligned}
x_1-5 x_3 & =1 \\
x_2+x_3 & =4 \\
0 & =0
\end{aligned}
$$

Las variables $x_1$ y $x_2$ correspondientes a las columnas pivote se conocen como **variables b谩sicas**. La otra variable, $x_3$, se denomina **variable libre**.

Siempre que un sistema es consistente, como el anterior, el conjunto soluci贸n se puede describir expl铆citamente al despejar en el sistema de ecuaciones _reducido_ las variables b谩sicas en t茅rminos de las variables libres. Esta operaci贸n es posible porque la forma escalonada reducida coloca a cada variable b谩sica en una y solo una ecuaci贸n. En el sistema anterior, despeje $x_1$ de la primera ecuaci贸n y $x_2$ de la segunda. (Ignore la tercera ecuaci贸n, ya que no ofrece restricciones sobre las variables).

$$
\left\{\begin{array}{l}
x_1=1+5 x_3 \\
x_2=4-x_3 \\
x_3 \text { es libre }
\end{array}\right.
$$

El enunciado "$x_3$ es libre" significa que existe libertad de elegir cualquier valor para $x_3$. Una vez hecho esto, las f贸rmulas anteriores determinan los valores de $x_1$ y $x_2$. Por ejemplo, cuando $x_3=0$, la soluci贸n es $(1,4,0)$; cuando $x_3=1$, la soluci贸n es $(6,3,1)$. Cada asignaci贸n diferente de $x_3$ determina una soluci贸n (distinta) del sistema, y cada soluci贸n del sistema est谩 determinada por una asignaci贸n de $x_3$.

### Preguntas de existencia y unicidad

:::tip[Teorema de existencia y unicidad]
Un sistema lineal es consistente si y solo si la columna m谩s a la derecha de la matriz aumentada no es una columna pivote, es decir, si y solo si una forma escalonada de la matriz aumentada no tiene filas del tipo

$$
\left[\begin{array}{llll}0 & \cdots & 0 & b\end{array}\right] \quad  \text{con } b \text{ diferente de cero}
$$

Si un sistema lineal es consistente, entonces el conjunto soluci贸n contiene: i. una 煤nica soluci贸n, cuando no existen variables libres, o ii. una infinidad de soluciones, cuando hay al menos una variable libre.
:::

El siguiente procedimiento indica c贸mo encontrar y describir todas las soluciones de un sistema lineal.

1. Escriba la matriz aumentada del sistema.
2. Emplee el algoritmo de reducci贸n por filas para obtener una matriz aumentada equivalente en forma escalonada. Determine si el sistema es consistente o no. Si no existe soluci贸n, det茅ngase; en caso contrario, contin煤e con el siguiente paso.
3. Prosiga con la reducci贸n por filas para obtener la forma escalonada reducida.
4. Escriba el sistema de ecuaciones correspondiente a la matriz obtenida en el paso 3.
5. Rescriba cada ecuaci贸n no nula del paso 4 de manera que su 煤nica variable b谩sica se exprese en t茅rminos de cualquiera de las variables libres que aparecen en la ecuaci贸n.

### Combinaciones lineales

Dados los vectores $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$ en $\mathbb{R}^n$ y dados los escalares $c_1, c_2, \ldots, c_p$, el vector $\mathbf{y}$ definido por

$$
\mathbf{y}=c_1 \mathbf{v}_1+\cdots+c_p \mathbf{v}_p
$$

se llama combinaci贸n lineal de $\mathbf{v}_1, \ldots, \mathbf{v}_p$ con pesos $c_1, \ldots, c_p$.

La siguiente figura identifica combinaciones lineales seleccionadas de $\mathbf{v}_1=\left[\begin{array}{r}-1 \\ 1\end{array}\right]$
y $\mathbf{v}_2=\left[\begin{array}{l}2 \\ 1\end{array}\right]$.

<Image
  src="matematicas/algebra_lineal/combinaciones_lineales.png"
  alt="Combinaci贸n lineal"
  caption="Ejemplo de combinaci贸n lineal"
  width="50%"
/>

### Espacio generado

Si $\mathbf{v}_1, \ldots, \mathbf{v}_p$ est谩n en $\mathbb{R}^n$, entonces el conjunto de todas las combinaciones lineales de $\mathbf{v}_1, \ldots, \mathbf{v}_p$ se denota como $\text{Gen}\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ y se llama el subconjunto de $\mathbb{R}^n$ extendido o generado por $\mathbf{v}_1, \ldots, \mathbf{v}_p$. Es decir, $\text{Gen}\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ es el conjunto de todos los vectores que se pueden escribir en la forma

$$
c_1 \mathbf{v}_1+c_2 \mathbf{v}_2+\cdots+c_p \mathbf{v}_p
$$

con escalares $c_1, \ldots, c_p$.

Sea $\mathbf{v}$ un vector diferente de cero en $\mathbb{R}^3$. Entonces $\text{Gen}\{\mathbf{v}\}$ es el conjunto de todos los m煤ltiplos escalares de $\mathbf{v}$, que es el conjunto de puntos sobre la recta en $\mathbb{R}^3$ que pasa por $\mathbf{v}$ y $\mathbf{0}$.

Si $\mathbf{u}$ y $\mathbf{v}$ son vectores diferentes de cero en $\mathbb{R}^3$, y $\mathbf{v}$ no es un m煤ltiplo de $\mathbf{u}$, entonces Gen $\{\mathbf{u}, \mathbf{v}\}$ es el plano en $\mathbb{R}^3$ que contiene a $\mathbf{u}, \mathbf{v}$ y $\mathbf{0}$. En particular, Gen $\{\mathbf{u}, \mathbf{v}\}$ contiene la recta en $\mathbb{R}^3$ que pasa por $\mathbf{u}$ y $\mathbf{0}$, y la recta que pasa por $\mathbf{v}$ y $\mathbf{0}$.

<Image
  src="matematicas/algebra_lineal/gen.png"
  alt="Espacio generado"
  caption="Espacio generado"
  width="50%"
/>

### Ecuaci贸n matricial

Si $A$ es una matriz de $m \times n$, con columnas $\mathbf{a}_1, \ldots, \mathbf{a}_n$, y si $\mathbf{x}$ est谩 en $\mathbb{R}^n$, entonces el producto de $A$ y $\mathbf{x}$, denotado como $A \mathbf{x}$, es la combinaci贸n lineal de las columnas de $A$ utilizando como pesos las entradas correspondientes en $\mathbf{x}$; es decir,

$$
A \mathbf{x}=\left[\begin{array}{llll}
\mathbf{a}_1 & \mathbf{a}_2 & \cdots & \mathbf{a}_n
\end{array}\right]\left[\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}\right]=x_1 \mathbf{a}_1+x_2 \mathbf{a}_2+\cdots+x_n \mathbf{a}_n
$$

:::tip[Teorema]
Si $A$ es una matriz de $m \times n$, con columnas $\mathbf{a}_1, \ldots, \mathbf{a}_n$, y si $\mathbf{b}$ est谩 en $\mathbb{R}^m$, la ecuaci贸n matricial

$$
A \mathbf{x}=\mathbf{b}
$$

tiene el mismo conjunto soluci贸n que la ecuaci贸n vectorial

$$
x_1 \mathbf{a}_1+x_2 \mathbf{a}_2+\cdots+x_n \mathbf{a}_n=\mathbf{b}
$$

la cual, a la vez, tiene el mismo conjunto soluci贸n que el sistema de ecuaciones lineales cuya matriz aumentada es

$$
\left[\begin{array}{lllll}
\mathbf{a}_1 & \mathbf{a}_2 & \cdots & \mathbf{a}_n & \mathbf{b}
\end{array}\right]
$$

:::

La definici贸n de $A \mathbf{x}$ conduce directamente al siguiente hecho que resulta 煤til:

:::tip[Existencia de soluci贸n]
La ecuaci贸n $A \mathbf{x}=\mathbf{b}$ tiene una soluci贸n si y solo si b es una combinaci贸n lineal de las columnas de $A$.
:::

Dado lo anterior, podemos enunciar el siguiente teorema:

:::tip[Teorema]
Sea $A$ una matriz de $m \times n$. Entonces, los siguientes enunciados son l贸gicamente equivalentes. Es decir, para una $A$ particular, todos los enunciados son verdaderos o todos son falsos.

- Para cada $\mathbf{b}$ en $\mathbb{R}^m$, la ecuaci贸n $A \mathbf{x}=\mathbf{b}$ tiene una soluci贸n.
- Cada $\mathbf{b}$ en $\mathbb{R}^m$ es una combinaci贸n lineal de las columnas de $A$.
- Las columnas de $A$ generan $\mathbb{R}^m$.
- $A$ tiene una posici贸n pivote en cada fila.
  :::

### Sistemas lineales homog茅neos

Se dice que un sistema de ecuaciones lineales es homog茅neo si se puede escribir en la forma $A \mathbf{x}=\mathbf{0}$, donde $A$ es una matriz de $m \times n$, y $\mathbf{0}$ es el vector cero en $\mathbb{R}^m$. Tal sistema $A \mathbf{x}=\mathbf{0}$ siempre tiene al menos una soluci贸n, a saber, $\mathbf{x}=\mathbf{0}$ (el vector cero en $\mathbb{R}^n$ ). Esta soluci贸n cero generalmente se conoce como soluci贸n trivial. Para una ecuaci贸n dada $A \mathbf{x}=\mathbf{0}$, la pregunta importante es si existe una soluci贸n no trivial, es decir, un vector $\mathbf{x}$ diferente de cero que satisfaga $A \mathbf{x}=\mathbf{0}$. El teorema de existencia y unicidad conduce de inmediato al siguiente resultado.

:::tip[Resultado]
La ecuaci贸n homog茅nea $A \mathbf{x}=\mathbf{0}$ tiene una soluci贸n no trivial si y solo si la ecuaci贸n tiene al menos una variable libre.
:::

### Independencia lineal

Las ecuaciones homog茅neas se pueden estudiar desde una perspectiva diferente si las escribimos como ecuaciones vectoriales. De esta manera, la atenci贸n se transfiere de las soluciones desconocidas de $A \mathbf{x}=\mathbf{0}$ a los vectores que aparecen en las ecuaciones vectoriales.

Por ejemplo, considere la ecuaci贸n

$$
x_1\left[\begin{array}{l}
1 \\
2 \\
3
\end{array}\right]+x_2\left[\begin{array}{l}
4 \\
5 \\
6
\end{array}\right]+x_3\left[\begin{array}{l}
2 \\
1 \\
0
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0
\end{array}\right]
$$

Esta ecuaci贸n tiene una soluci贸n trivial, desde luego, donde $x_1=x_2=x_3=0$. Al igual que hemos discutido antes, el asunto principal es si la soluci贸n trivial es la 煤nica.

:::tip[Definici贸n]
Se dice que un conjunto indexado de vectores $\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ en $\mathbb{R}^n$ es **linealmente independiente** si la ecuaci贸n vectorial

$$
x_1 \mathbf{v}_1+x_2 \mathbf{v}_2+\cdots+x_p \mathbf{v}_p=\mathbf{0}
$$

solo tiene la soluci贸n trivial.

Se dice que el conjunto $\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ es **linealmente dependiente** si existen pesos $c_1, \ldots, c_p$, no todos cero, tales que

$$
c_1 \mathbf{v}_1+c_2 \mathbf{v}_2+\cdots+c_p \mathbf{v}_p=\mathbf{0}
$$

:::

La ecuaci贸n que define el concepto de "linealmente dependiente" se llama **relaci贸n de dependencia lineal** entre $\mathbf{v}_1, \ldots, \mathbf{v}_p$ cuando no todos los pesos son cero. Un conjunto indexado es linealmente dependiente si y solo si no es linealmente independiente. Por brevedad, puede decirse que $\mathbf{v}_1, \ldots, \mathbf{v}_p$ son linealmente dependientes cuando queremos decir que $\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ es un conjunto linealmente dependiente. Se utiliza una terminolog铆a semejante para los conjuntos linealmente independientes.

### Independencia lineal en columnas de una matriz

Suponga que, en vez de utilizar un conjunto de vectores, se inicia con una matriz $A=\left[\begin{array}{lll}\mathbf{a}_1 & \cdots & \mathbf{a}_n\end{array}\right]$. En tal caso, la ecuaci贸n matricial $A \mathbf{x}=\mathbf{0}$ se puede escribir como

$$
x_1 \mathbf{a}_1+x_2 \mathbf{a}_2+\cdots+x_n \mathbf{a}_n=\mathbf{0}
$$

Cada relaci贸n de dependencia lineal entre las columnas de A corresponde a una soluci贸n no trivial de $A \mathbf{x}=\mathbf{0}$. As铆, tenemos el siguiente resultado importante.

:::tip[Teorema]
Las columnas de una matriz $A$ son linealmente independientes si y solo si la ecuaci贸n $A \mathbf{x}=\mathbf{0}$ tiene solo la soluci贸n trivial.
:::

### Geometr铆a de la dependencia lineal

Siempre es posible determinar por _inspecci贸n_ cu谩ndo un conjunto de dos vectores es linealmente dependiente. Las operaciones de fila son innecesarias. Basta con comprobar si al menos uno de los vectores es un escalar multiplicado por el otro. (La prueba solo se aplica a conjuntos de dos vectores).

:::tip[Dependencia lineal entre dos vectores]
Un conjunto de dos vectores $\left\{\mathbf{v}_1, \mathbf{v}_2\right\}$ es linealmente dependiente si al menos uno de los vectores es un m煤ltiplo del otro. El conjunto es linealmente independiente si y solo si ninguno de los vectores es un m煤ltiplo del otro.
:::

<Image
  src="matematicas/algebra_lineal/dep_lineal.png"
  alt="Dependencia lineal"
  caption="Dependencia lineal"
  width="25%"
/>

En t茅rminos geom茅tricos, dos vectores son linealmente dependientes si y solo si ambos est谩n sobre la misma recta que pasa por el origen.

Para dos o m谩s vectores, tenemos el siguiente resultado:

:::tip[Caracterizaci贸n de conjuntos linealmente dependientes]
Un conjunto indexado $S=\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ de dos o m谩s vectores es linealmente dependiente si y solo si al menos uno de los vectores en $S$ es una combinaci贸n lineal de los otros. De hecho, si $S$ es linealmente dependiente y $\mathbf{v}_1 \neq \mathbf{0}$, entonces alguna $\mathbf{v}_j(\operatorname{con} j>1$ ) es una combinaci贸n lineal de los vectores precedentes, $\mathbf{v}_1, \ldots, \mathbf{v}_{j-1}$.
:::

Los siguientes dos teoremas describen casos especiales en los cuales la dependencia lineal de un conjunto es autom谩tica.

:::tip[Teorema]
Si un conjunto contiene m谩s vectores que entradas en cada vector, entonces el conjunto es linealmente dependiente. Es decir, cualquier conjunto $\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ en $\mathbb{R}^n$ es linealmente dependiente si $p>n$.
:::

:::tip[Teorema]
Si un conjunto $S=\left\{\mathbf{v}_1, \ldots, \mathbf{v}_p\right\}$ en $\mathbb{R}^n$ contiene al vector cero, entonces el conjunto es linealmente dependiente.
:::

### Transformaciones lineales

La correspondencia de $\mathbf{x}$ a $A\mathbf{x}$ es una funci贸n de un conjunto de vectores a otro. Este concepto generaliza la noci贸n com煤n de una funci贸n como una regla que transforma un n煤mero real en otro.

Una **transformaci贸n** (o funci贸n o mapeo) $T$ de $\mathbb{R}^n$ a $\mathbb{R}^m$ es una regla que asigna a cada vector $\mathbf{x}$ en $\mathbb{R}^n$ un vector $T(\mathbf{x})$ en $\mathbb{R}^m$. El conjunto $\mathbb{R}^n$ se llama el dominio de $T$, y $\mathbb{R}^m$ se llama el codominio de $T$. La notaci贸n $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ indica que el dominio de $T$ es $\mathbb{R}^n$ y que el codominio es $\mathbb{R}^m$. Para $\mathbf{x}$ en $\mathbb{R}^n$, el vector $T(\mathbf{x})$ en $\mathbb{R}^m$ es la imagen de $\mathbf{x}$ (bajo la acci贸n de $T$ ). El conjunto de todas las im谩genes $T(\mathbf{x})$ es el rango de $T$.

<Image
  src="matematicas/algebra_lineal/transformacion_lineal.png"
  alt="Transformaci贸n lineal"
  width="40%"
/>

Para cada $\mathbf{x}$ en $\mathbb{R}^n, T(\mathbf{x})$ se calcula como $A \mathbf{x}$, donde $A$ es una matriz de $m \times n$. Para simplificar, algunas veces esta transformaci贸n matricial se denota como $\mathbf{x} \mapsto A \mathbf{x}$. Observe que el dominio de $T$ es $\mathbb{R}^n$ cuando $A$ tiene $n$ columnas, y el codominio de $T$ es $\mathbb{R}^m$ cuando cada columna de $A$ tiene $m$ entradas. El rango de $T$ es el conjunto de todas las combinaciones lineales de las columnas de $A$, porque cada imagen $T(\mathbf{x})$ es de la forma $A \mathbf{x}$.

:::tip[Definici贸n]
Una transformaci贸n (o mapeo) $T$ es lineal si:

- $T(\mathbf{u}+\mathbf{v})=T(\mathbf{u})+T(\mathbf{v})$ para todas las $\mathbf{u}$, $\mathbf{v}$ en el dominio de $T$;
- $T(c \mathbf{u})=c T(\mathbf{u})$ para todos los escalares $c$ y para todas las $\mathbf{u}$ en el dominio de $T$.
  :::

## lgebra de matrices

### Operaciones b谩sicas

Sean $A, B$ y $C$ matrices del mismo tama帽o, y sean $r$ y $s$ escalares.

- $A+B=B+A$
- $r(A+B)=r A+r B$
- $(A+B)+C=A+(B+C)$
- $(r+s) A=r A+s A$
- $A+0=A$
- $r(s A)=(r s) A$
- $A^k=\underbrace{A \cdots A}_k$

### Multiplicaci贸n de matrices

Si $A$ es una matriz de $m \times n$, y si $B$ es una matriz de $n \times p$ con columnas $\mathbf{b}_1, \ldots, \mathbf{b}_p$ entonces el producto $A B$ es la matriz de $m \times p$ cuyas columnas son $A \mathbf{b}_1, \ldots, A \mathbf{b}_p$. Es decir,

$$
A B=A\left[\begin{array}{llll}
\mathbf{b}_1 & \mathbf{b}_2 & \cdots & \mathbf{b}_p
\end{array}\right]=\left[\begin{array}{llll}
A \mathbf{b}_1 & A \mathbf{b}_2 & \cdots & A \mathbf{b}_p
\end{array}\right]
$$

Es decir, cada columna de $A B$ es una combinaci贸n lineal de las columnas de $A$ usando pesos de la columna correspondiente de $B$.

:::tip[Regla fila-columna para calcular AB]
Si el producto $A B$ est谩 definido, entonces la entrada en la fila $i$ y la columna $j$ de $A B$ es la suma de los productos de las entradas correspondientes de la fila $i$ de $A$ y la columna $j$ de $B$. Si $(A B)_{i j}$ denota la entrada $(i, j)$ en $A B$, y si $A$ es una matriz de $m \times n$, entonces

$$
(A B)_{i j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i n} b_{n j}
$$

:::

:::tip[Propiedades de la multiplicaci贸n de matrices]
Sea $A$ una matriz de $m \times n$, y sean $B$ y $C$ matrices con tama帽os para los que las sumas y los productos indicados est谩n definidos.

- $A(B C)=(A B) C$
- $A(B+C)=A B+A C$
- $(B+C) A=B A+C A$
- $r(A B)=(r A) B=A(r B)$ para cualquier escalar $r$
- $I_m A=A=A I_n$
  :::

:::danger[Advertencias]

1. En general, $A B \neq B A$.
2. Las leyes de la cancelaci贸n no se aplican en la multiplicaci贸n de matrices. Es decir, si $A B=A C$, en general no es cierto que $B=C$.
3. Si un producto $A B$ es la matriz cero, en general, no se puede concluir que $A=0$ o $B=0$.
   :::

### La transpuesta de una matriz

Dada una matriz $A$ de $m \times n$, la **transpuesta** de $A$ es la matriz de $n \times m$, que se denota con $A^T$, cuyas columnas se forman a partir de las filas correspondientes de $A$.

:::tip[Teorema]
Sean $A$ y $B$ matrices cuyos tama帽os son adecuados para las siguientes sumas y productos.

- $\left(A^T\right)^T=A$
- $(A+B)^T=A^T+B^T$
- Para cualquier escalar $r,(r A)^T=r A^T$
- $(A B)^T=B^T A^T$
  :::

### Inversa de una matriz

Se dice que una matriz $A$ de $n \times n$ es **invertible** si existe otra matriz $A^{-1}$ de $n \times n$ tal que

$$
A^{-1} A=I \quad \text { y } \quad A A^{-1}=I
$$

donde $I=I_n$, la matriz identidad de $n \times n$. En este caso, $A^{-1}$ es una inversa de $A$.

:::tip[Teorema]
Sea $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$. Si $a d-b c \neq 0$, entonces $A$ es invertible y

$$
A^{-1}=\frac{1}{\text{det}(A)}\left[\begin{array}{rr}
d & -b \\
-c & a
\end{array}\right]
$$

Si $\text{det}(A)=a d-b c=0$, entonces $A$ no es invertible.
:::

La definici贸n de matriz inversa nos entrega el siguiente teorema:

:::tip[Teorema]
Si $A$ es una matriz invertible de $n \times n$, entonces, para cada $\mathbf{b}$ en $\mathbb{R}^n$, la ecuaci贸n $A \mathbf{x}=\boldsymbol{b}$ tiene la soluci贸n 煤nica $\mathbf{x}=A^{-1} \mathbf{b}$.
:::

Adem谩s, podemos definir las siguientes propiedades para matrices invertibles:

- Si $A$ es una matriz invertible, entonces $A^{-1}$ es invertible y
  $$
  \left(A^{-1}\right)^{-1}=A
  $$
- Si $A$ y $B$ son matrices invertibles de $n \times n$, entonces tambi茅n lo es $A B$, y la inversa de $A B$ es el producto de las inversas de $A$ y $B$ en el orden opuesto. Es decir,
  $$
  (A B)^{-1}=B^{-1} A^{-1}
  $$
- Si $A$ es una matriz invertible, tambi茅n lo es $A^T$, y la inversa de $A^T$ es la transpuesta de $A^{-1}$. Es decir,
  $$
  \left(A^T\right)^{-1}=\left(A^{-1}\right)^T
  $$

### Matrices elementales

Una **matriz elemental** es aquella que se obtiene al realizar una 煤nica operaci贸n elemental de fila sobre una matriz identidad. El siguiente ejemplo ilustra los tres tipos de matrices elementales.

Si se realiza una operaci贸n elemental de fila con una matriz $A$ de $m \times n$, la matriz resultante se puede escribir como $E A$, donde la matriz $E$ de $m \times m$ se crea al realizar la misma operaci贸n de fila sobre $I_m$.

Por ejemplo, si

<Image
  src="matematicas/algebra_lineal/matriz_elemental_1.png"
  alt="Matriz elemental"
  width="55%"
/>

entonces

<Image
  src="matematicas/algebra_lineal/matriz_elemental_2.png"
  alt="Matriz elemental"
  width="55%"
/>

Al sumar a la fila 3 la fila 1 de $A$ multiplicada por $-4$ , se obtiene $E_1 A$. (Esta es una operaci贸n de remplazo de filas). Con un intercambio de las filas 1 y 2 de $A$ se obtiene $E_2 A$, y multiplicando la fila 3 de $A$ por $5$ se obtiene $E_3 A$.

:::tip[Teorema]
Toda matriz elemental $E$ es invertible. La inversa de $E$ es la matriz elemental del mismo tipo que transforma a $E$ de nuevo en $I$.

Una matriz $A$ de $n \times n$ es invertible si y solo si $A$ es equivalente por filas a $I_n$, y, en este caso, cualquier secuencia de operaciones elementales de fila que reduzca $A$ a $I_n$ tambi茅n transforma a $I_n$ en $A^{-1}$.
:::

El teorema anterior nos da el siguiente algoritmo para determinar la inversa de una matriz $A$ de $n \times n$:

:::tip[Algoritmo para determinar la inversa de una matriz]
Reduzca por filas la matriz aumentada $\left[\begin{array}{ll}A & I\end{array}\right]$. Si $A$ es equivalente por filas a $I$, entonces $\left[\begin{array}{ll}A & I\end{array}\right]$ es equivalente por filas a $\left[\begin{array}{ll}I & A^{-1}\end{array}\right]$. De otra manera, $A$ no tiene inversa.
:::

### El teorema de la matriz invertible

El resultado principal de todo el estudio anterior es el siguiente teorema.

:::tip[Teorema]
Sea $A$ una matriz cuadrada de $n \times n$. Entonces, los siguientes enunciados son equivalentes. Es decir, para una $A$ dada, los enunciados son todos ciertos o todos falsos.

- $A$ es una matriz invertible.
- $A$ es equivalente por filas a la matriz identidad de $n \times n$.
- $A$ tiene $n$ posiciones pivote.
- La ecuaci贸n $A \mathbf{x}=\mathbf{0}$ tiene solamente la soluci贸n trivial.
- Las columnas de $A$ forman un conjunto linealmente independiente.
- La transformaci贸n lineal $\mathbf{x} \mapsto A \mathbf{x}$ es uno a uno.
- La ecuaci贸n $A \mathbf{x}=\mathbf{b}$ tiene al menos una soluci贸n para toda $\mathbf{b}$ en $\mathbb{R}^n$.
- Las columnas de $A$ generan $\mathbb{R}^n$.
- La transformaci贸n lineal $\mathbf{x} \mapsto A \mathbf{x}$ mapea $\mathbb{R}^n$ sobre $\mathbb{R}^n$.
- Existe una matriz $C$ de $n \times n$ tal que $C A=I$,
- Existe una matriz $D$ de $n \times n$ tal que $A D=I$.
- $A^T$ es una matriz invertible.
  :::

### Transformaci贸n lineal invertible

Se dice que una transformaci贸n lineal $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ es **invertible** si existe una funci贸n $S: \mathbb{R}^n \rightarrow \mathbb{R}^n$ tal que

$$
\begin{array}{ll}
S(T(\mathbf{x}))=\mathbf{x} & \text { para toda } \mathbf{x} \text { en } \mathbb{R}^n \\
T(S(\mathbf{x}))=\mathbf{x} & \text { para toda } \mathbf{x} \text { en } \mathbb{R}^n
\end{array}
$$

El siguiente teorema establece que si dicha $S$ existe, es 煤nica y debe ser una transformaci贸n lineal. Se dice que $S$ es la inversa de $T$ y se escribe como $T^{-1}$.

:::tip[Teorema]
Sea $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ una transformaci贸n lineal y sea $A$ la matriz est谩ndar para $T$. As铆, $T$ es invertible si y solo si $A$ es una matriz invertible. En tal caso, la transformaci贸n lineal $S$ dada por $S(\mathbf{x})=A^{-1} \mathbf{x}$ es la 煤nica funci贸n que satisface las ecuaciones mostradas.
:::

### Subespacios

Un subespacio de $\mathbb{R}^n$ es cualquier conjunto $H$ en $\mathbb{R}^n$ que tenga tres propiedades:

- El vector cero est谩 en $H$.
- Para cada $\mathbf{u}$ y $\mathbf{v}$ en $H$, la suma $\mathbf{u}+\mathbf{v}$ est谩 en $H$.
- Para cada $\mathbf{u}$ en $H$ y cada escalar $c$, el vector $c \mathbf{u}$ est谩 en $H$.

Dicho con palabras, un subespacio es cerrado bajo la suma y la multiplicaci贸n escalar.

<Image
  src="matematicas/algebra_lineal/subespacio.png"
  alt="Subespacio"
  width="25%"
/>

### Espacio de columnas

Los subespacios de $\mathbb{R}^n$ generalmente se presentan en aplicaciones y en la teor铆a en una de dos formas. En ambos casos, es posible relacionar el subespacio con una matriz.

:::tip[Definici贸n]
El espacio de columnas de una matriz $A$ es el conjunto $\mathrm{Col} A$ de todas las combinaciones de las columnas de $A$.
:::

Si $A=\left[\begin{array}{lll}\mathbf{a}_1 & \cdots & \mathbf{a}_n\end{array}\right]$, con las columnas en $\mathbb{R}^m$, entonces $\text{Col} A$ es lo mismo que $\text{Gen}\left\{\mathbf{a}_1, \ldots, \mathbf{a}_n\right\}$. De hecho, el espacio columna de una matriz de $\boldsymbol{m} \times \boldsymbol{n}$ es un subespacio de $\mathbb{R}^m$. Observe que $\text{Col} A$ es igual a $\mathbb{R}^m$ solo cuando las columnas de $A$ generan a $\mathbb{R}^m$. Si no la generan, $\text{Col}A$ es solo una parte de $\mathbb{R}^m$.

Cuando un sistema de ecuaciones lineales est谩 escrito en la forma $A \mathbf{x}=\mathbf{b}$, el espacio columna de $A$ es el conjunto de todas las $\mathbf{b}$ para las que el sistema tiene una soluci贸n.

### Espacio nulo

El **espacio nulo** de una matriz $A$ es el conjunto $\mathrm{Nul} A$ de todas las soluciones posibles para la ecuaci贸n homog茅nea $A \mathbf{x}=\mathbf{0}$.

Cuando $A$ tiene $n$ columnas, las soluciones de $A \mathbf{x}=\mathbf{0}$ pertenecen a $\mathbb{R}^n$, y el espacio nulo de $A$ es un subconjunto de $\mathbb{R}^n$. De hecho, Nul $A$ tiene las propiedades de un subespacio de matrices de $\mathbb{R}^n$.

:::tip[Teorema]
El espacio nulo de una matriz $A$ de $m \times n$ es un subespacio de $\mathbb{R}^n$. De manera equivalente, el conjunto de todas las soluciones posibles para un sistema $A \mathbf{x}=\mathbf{0}$ de $m$ ecuaciones lineales homog茅neas con $n$ inc贸gnitas es un subespacio de $\mathbb{R}^n$.
:::

### Base para un subespacio

Como, por lo general, un subespacio contiene un n煤mero infinito de vectores, algunos problemas relacionados con subespacios se manejan mejor trabajando con un conjunto finito y peque帽o de vectores que genere el subespacio. Cuanto menor sea el conjunto, ser谩 mejor. Es factible demostrar que el conjunto generador m谩s peque帽o posible debe ser linealmente independiente.

:::tip[Definici贸n]
Una base de un subespacio $H$ de $\mathbb{R}^n$ es un conjunto linealmente independiente en $H$, que genera a $H$.
:::

<Image
  src="matematicas/algebra_lineal/base.png"
  alt="Base"
  caption="La base est谩ndar para todo el espacio"
  width="25%"
/>

:::warning
Solo son 4 preguntas de Lineal, despu茅s me sigo pegando el show terminando esto 
:::
